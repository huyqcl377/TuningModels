{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Installs and Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time # For timing epochs\n",
    "from py_vncorenlp import VnCoreNLP\n",
    "\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model output path: ./ViCLIP_landmark\n",
      "Image base path (for resolving paths in JSON): /root/TuningModels/data/LANDMARK-IN-VIETNAM\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration Class (CFG)\n",
    "# import torch\n",
    "# import os\n",
    "\n",
    "# ==============================================================================\n",
    "# Configuration Class (CFG)\n",
    "# ==============================================================================\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    # /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000000000.png\n",
    "    # Base directory where your train.json, dev.json, test.json are located\n",
    "    data_path = \"./json_data//\"\n",
    "    image_path = \"./data/LANDMARK-IN-VIETNAM/\"\n",
    "\n",
    "    # Output directory for saved models\n",
    "    model_path = \"./ViCLIP_landmark\"\n",
    "\n",
    "    # --- Available Models ---\n",
    "    text_models = {\n",
    "        \"PhoBERT-base\": \"vinai/phobert-base\",\n",
    "        \"PhoBERT-large\": \"vinai/phobert-large\",\n",
    "        \"ViT5-base\": \"VietAI/vit5-base\",\n",
    "        \"ViT5-large\": \"VietAI/vit5-large\"\n",
    "    }\n",
    "    image_models = {\n",
    "        \"ViT-S\": \"vit_small_patch16_224\", \"ViT-B\": \"vit_base_patch16_224\",\n",
    "        \"ViT-L\": \"vit_large_patch16_224\", \"ViT-H\": \"vit_huge_patch16_224\",\n",
    "        \"ResNet50\": \"resnet50\"\n",
    "    }\n",
    "\n",
    "    # --- User Selections ---\n",
    "    selected_text_model = \"PhoBERT-base\"\n",
    "    selected_image_model = \"ResNet50\"\n",
    "\n",
    "    # --- Model parameters based on selection (Properties) ---\n",
    "    @property\n",
    "    def model_name(self): return self.image_models[self.selected_image_model]\n",
    "    @property\n",
    "    def text_encoder_model(self): return self.text_models[self.selected_text_model]\n",
    "    @property\n",
    "    def text_tokenizer(self): return self.text_models[self.selected_text_model]\n",
    "\n",
    "    @property\n",
    "    def text_embedding(self): # Encoder output dim before projection\n",
    "        model_key = self.text_models[self.selected_text_model]\n",
    "        if \"large\" in model_key: return 1024\n",
    "        elif \"base\" in model_key: return 768\n",
    "        else: print(f\"Warning: Unknown text embedding size for {model_key}, defaulting to 768.\"); return 768\n",
    "\n",
    "    @property\n",
    "    def image_embedding(self): # Encoder output dim before projection\n",
    "        if self.selected_image_model == \"ResNet50\": return 2048\n",
    "        elif self.selected_image_model == \"ViT-S\": return 384\n",
    "        elif self.selected_image_model == \"ViT-B\": return 768\n",
    "        elif self.selected_image_model == \"ViT-L\": return 1024\n",
    "        elif self.selected_image_model == \"ViT-H\": return 1280\n",
    "        else: print(f\"Warning: Unknown image embedding size for {self.model_name}, defaulting to 768.\"); return 768\n",
    "    \n",
    "    # --- Fixed parameters ---\n",
    "    projection_dim = 256 # Shared latent space dimension\n",
    "\n",
    "    # --- Training parameters ---\n",
    "    seed = 42\n",
    "    batch_size = 64\n",
    "    num_workers = 20  \n",
    "    head_lr = 5e-4\n",
    "    image_encoder_lr = 2e-4\n",
    "    text_encoder_lr = 2e-5\n",
    "    weight_decay = 5e-3\n",
    "    patience = 7\n",
    "    factor = 0.8\n",
    "    epochs = 32\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- Image/Text parameters ---\n",
    "    size = 224 # Input image size\n",
    "    max_length = 200 # Max text sequence length\n",
    "\n",
    "    # --- Loss/Saving parameters ---\n",
    "    temperature = 0.07\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"avg_acc\"\n",
    "    mode = \"max\" if metric_to_track != \"loss\" else \"min\"\n",
    "\n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Seeding for Reproducibility\n",
    "\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # For multi-GPU\n",
    "        # These can slow down training, use cautiously if performance is critical\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Metric Calculation Utilities\n",
    "\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        # Ensure val is a scalar number before adding to sum\n",
    "        if torch.is_tensor(val):\n",
    "             val = val.item() # Convert tensor to Python number\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "        # else:\n",
    "            # Optionally print a warning if the value is not usable\n",
    "            # print(f\"Warning: Cannot update AvgMeter '{self.name}' with value type {type(val)}\")\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    n = similarity_matrix.shape[1-dim]\n",
    "    correct_count = 0\n",
    "    top_k_indices = torch.topk(similarity_matrix, k, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "\n",
    "    if dim == 0: # I2T\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]:\n",
    "                correct_count += 1\n",
    "    elif dim == 1: # T2I\n",
    "        for txt_idx in range(n):\n",
    "             if ground_truth[txt_idx] in top_k_indices[txt_idx, :]:\n",
    "                correct_count += 1\n",
    "    else: raise ValueError(\"dim must be 0 or 1\")\n",
    "    return correct_count / n if n > 0 else 0.0\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    sim_matrix = text_embeddings @ image_embeddings.T\n",
    "    sim_matrix = sim_matrix.float() # Ensure float for calculations\n",
    "    n = sim_matrix.shape[0]\n",
    "    if n == 0:\n",
    "        # Return default zero metrics for empty batch\n",
    "        return {\n",
    "            \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "            \"avg_cosine_sim\": 0.0,\n",
    "            \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "        }\n",
    "\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "    i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "    t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "    t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "    avg_acc = (i2t_acc + t2i_acc) / 2\n",
    "    avg_cosine_sim = torch.diagonal(sim_matrix).mean().item()\n",
    "\n",
    "    i2t_recall = {}\n",
    "    t2i_recall = {}\n",
    "    recall_k_values = [k for k in [1, 5, 10] if k <= n]\n",
    "    for k in recall_k_values:\n",
    "        i2t_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "        t2i_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "\n",
    "    # Ensure all keys R@1, R@5, R@10 exist even if k>n\n",
    "    for k in [1, 5, 10]:\n",
    "        k_str = f\"R@{k}\"\n",
    "        if k_str not in i2t_recall: i2t_recall[k_str] = 0.0\n",
    "        if k_str not in t2i_recall: t2i_recall[k_str] = 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "        \"avg_cosine_sim\": avg_cosine_sim,\n",
    "        \"i2t_recall\": i2t_recall, \"t2i_recall\": t2i_recall\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageCaptionDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Dataset Class Definition\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, tokenizer, transforms, max_length):\n",
    "        super().__init__()\n",
    "        print(f\"Attempting to load data from: {os.path.abspath(json_path)}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                self.data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: JSON file not found at {json_path}\")\n",
    "            print(f\"       Please ensure '{json_path}' exists relative to your notebook or provide the full path.\")\n",
    "            self.data = []\n",
    "            # Optionally raise error: raise FileNotFoundError(f\"JSON file not found at {json_path}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Could not decode JSON from {json_path}\")\n",
    "            self.data = []\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred loading {json_path}: {e}\")\n",
    "            self.data = []\n",
    "\n",
    "        print(f\"Found {len(self.data)} samples in {os.path.basename(json_path)}.\")\n",
    "        self.image_base_path = image_base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transforms = transforms\n",
    "        self.max_length = max_length\n",
    "        # Optional: Check if the image base path exists\n",
    "        if not os.path.isdir(self.image_base_path):\n",
    "             print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "             print(f\"         Ensure 'image_path' in CFG points to the correct directory relative to your notebook.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data):\n",
    "             raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path', None) # Use .get for safety\n",
    "        caption = item.get('caption', '') # Use .get for safety\n",
    "\n",
    "        if relative_image_path is None:\n",
    "            print(f\"Warning: Missing 'image_path' for item at index {idx}. Returning dummy data.\")\n",
    "            image = torch.zeros((3, config.size, config.size))\n",
    "        else:\n",
    "            image_path = os.path.normpath(os.path.join(self.image_base_path, relative_image_path))\n",
    "            # /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000000000.png\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image = self.transforms(image)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Img not found: {image_path}. Base: {self.image_base_path}, Rel: {relative_image_path}\")\n",
    "                image = torch.zeros((3, config.size, config.size))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading image {image_path}: {e}\")\n",
    "                image = torch.zeros((3, config.size, config.size))\n",
    "\n",
    "        # Process text with consistent dimensions\n",
    "        text_inputs = self.tokenizer(\n",
    "            caption, padding='max_length', truncation=True,\n",
    "            max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Always get first dimension (which should be batch dim of 1) and ensure 1D tensor\n",
    "        input_ids = text_inputs['input_ids'][0]\n",
    "        attention_mask = text_inputs['attention_mask'][0]\n",
    "        \n",
    "        # Verify tensors are 1D\n",
    "        if input_ids.dim() > 1:\n",
    "            # If somehow we get 2D tensor, flatten to 1D\n",
    "            input_ids = input_ids.view(-1)\n",
    "            \n",
    "        if attention_mask.dim() > 1:\n",
    "            attention_mask = attention_mask.view(-1)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "print(\"ImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model components and loss function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Model Component Definitions (Encoders, CLIP Model, Loss)\n",
    "\n",
    "# --- Image Encoder ---\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, config, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        print(f\"Initializing Image Encoder: {config.selected_image_model}\")\n",
    "        if config.selected_image_model == \"ResNet50\":\n",
    "            weights = models.ResNet50_Weights.DEFAULT if pretrained else None\n",
    "            self.model = models.resnet50(weights=weights)\n",
    "            self.input_features = config.image_embedding\n",
    "            self.model.fc = nn.Identity()\n",
    "            print(f\"  Loaded ResNet50 from torchvision. Input features: {self.input_features}\")\n",
    "        # --- Add ViT logic here if needed ---\n",
    "        # elif \"ViT\" in config.selected_image_model:\n",
    "        #     try:\n",
    "        #         import timm\n",
    "        #     except ImportError:\n",
    "        #         print(\"Please install timm library ('pip install timm') to use ViT models.\")\n",
    "        #         raise\n",
    "        #     print(f\"  Loading {config.model_name} using timm.\")\n",
    "        #     self.model = timm.create_model(config.model_name, pretrained=pretrained)\n",
    "        #     self.input_features = config.image_embedding # Should match timm model output before head\n",
    "        #     # Verify input_features matches actual output if possible, e.g., self.model.embed_dim\n",
    "        #     # print(f\"    ViT embed_dim: {self.model.embed_dim}\") # Example check\n",
    "        #     self.model.head = nn.Identity() # Remove classification head\n",
    "        #     print(f\"    Removed head. Input features to projection: {self.input_features}\")\n",
    "        # --- End ViT logic ---\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image model type in config: {config.selected_image_model}\")\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, self.config.projection_dim)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {self.config.projection_dim}\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        # ViT models might output class token + patch tokens, check model architecture\n",
    "        # If using ViT, might need features = features[:, 0] to get class token\n",
    "        features = features.view(features.size(0), -1) # Flatten if needed\n",
    "        projected_features = self.projection(features)\n",
    "        projected_features = F.normalize(projected_features, p=2, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "# --- Text Encoder ---\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, config, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.input_features = config.text_embedding # Dim before projection\n",
    "        print(f\"Initializing Text Encoder: {config.text_encoder_model}\")\n",
    "        print(f\"  Expected input features: {self.input_features}\")\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config.text_encoder_model)\n",
    "        else:\n",
    "            model_config = AutoConfig.from_pretrained(config.text_encoder_model)\n",
    "            self.model = AutoModel.from_config(model_config)\n",
    "\n",
    "        # Check if actual model hidden size matches config\n",
    "        actual_hidden_size = self.model.config.hidden_size\n",
    "        if actual_hidden_size != self.input_features:\n",
    "             print(f\"WARNING: Configured text_embedding ({self.input_features}) does not match actual model hidden size ({actual_hidden_size}). Using actual size for projection.\")\n",
    "             self.input_features = actual_hidden_size # Use actual size\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, self.config.projection_dim)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {self.config.projection_dim}\")\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        if input_ids.dim() == 1: input_ids = input_ids.unsqueeze(0)\n",
    "        if attention_mask.dim() == 1: attention_mask = attention_mask.unsqueeze(0)\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Using [CLS] token embedding\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        projected_features = self.projection(cls_embedding)\n",
    "        projected_features = F.normalize(projected_features, p=2, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "# --- CLIP Model ---\n",
    "class CLIPViModel(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, config):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.temperature = config.temperature\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.image_encoder(image)\n",
    "        text_features = self.text_encoder(input_ids, attention_mask)\n",
    "        logit_scale = 1 / self.temperature\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        return logits_per_image, logits_per_text, image_features, text_features\n",
    "\n",
    "# --- Loss Function ---\n",
    "def contrastive_loss(logits_per_image, logits_per_text):\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    if batch_size == 0: return torch.tensor(0.0, device=logits_per_image.device, requires_grad=True) # Handle empty batch\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "    loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "    total_loss = (loss_img + loss_txt) / 2.0\n",
    "    return total_loss\n",
    "\n",
    "print(\"Model components and loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation epoch functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Training and Validation Epoch Functions\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch_num):\n",
    "    model.train()\n",
    "    loss_meter = AvgMeter(f\"Train Loss E{epoch_num}\")\n",
    "    # Using tqdm.notebook for progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_size = images.size(0)\n",
    "        if batch_size == 0: continue # Skip empty batches\n",
    "\n",
    "        logits_per_image, logits_per_text, _, _ = model(images, input_ids, attention_mask)\n",
    "        loss = contrastive_loss(logits_per_image, logits_per_text)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_meter.update(loss.item(), batch_size)\n",
    "        progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    return loss_meter.avg\n",
    "\n",
    "def validate_epoch(model, dataloader, device, epoch_num):\n",
    "    model.eval()\n",
    "    # Initialize meters\n",
    "    loss_meter = AvgMeter(f\"Val Loss E{epoch_num}\")\n",
    "    acc_meter = AvgMeter(f\"Val Acc E{epoch_num}\")\n",
    "    cos_sim_meter = AvgMeter(f\"Val CosSim E{epoch_num}\")\n",
    "    # Initialize all potential recall meters\n",
    "    recall_meters = {\n",
    "        \"i2t_R@1\": AvgMeter(f\"Val I2T R@1 E{epoch_num}\"), \"i2t_R@5\": AvgMeter(f\"Val I2T R@5 E{epoch_num}\"), \"i2t_R@10\": AvgMeter(f\"Val I2T R@10 E{epoch_num}\"),\n",
    "        \"t2i_R@1\": AvgMeter(f\"Val T2I R@1 E{epoch_num}\"), \"t2i_R@5\": AvgMeter(f\"Val T2I R@5 E{epoch_num}\"), \"t2i_R@10\": AvgMeter(f\"Val T2I R@10 E{epoch_num}\"),\n",
    "    }\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = images.size(0)\n",
    "            if batch_size == 0: continue # Skip empty batches\n",
    "\n",
    "            logits_per_image, logits_per_text, image_features, text_features = model(images, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(logits_per_image, logits_per_text)\n",
    "            metrics = compute_metrics(image_features, text_features)\n",
    "\n",
    "            # Update meters\n",
    "            loss_meter.update(loss.item(), batch_size)\n",
    "            acc_meter.update(metrics[\"avg_acc\"], batch_size)\n",
    "            cos_sim_meter.update(metrics[\"avg_cosine_sim\"], batch_size)\n",
    "            for k_val, meter in recall_meters.items():\n",
    "                 recall_type, recall_key = k_val.split('_') # 'i2t'/'t2i', 'R@k'\n",
    "                 if recall_key in metrics[f\"{recall_type}_recall\"]:\n",
    "                    meter.update(metrics[f\"{recall_type}_recall\"][recall_key], batch_size)\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\", acc=f\"{acc_meter.avg:.3f}\")\n",
    "\n",
    "    # Collect results\n",
    "    validation_results = {\"loss\": loss_meter.avg, \"avg_acc\": acc_meter.avg, \"avg_cosine_sim\": cos_sim_meter.avg}\n",
    "    for k_val, meter in recall_meters.items():\n",
    "         validation_results[k_val.replace(\"_\", \" \")] = meter.avg # Format key nicely\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "print(\"Training and Validation epoch functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer: vinai/phobert-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce47219e1f84d5f80d46a87b4c792c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771482e980a24167932cf625c5c522cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7361f5fd9104e418498783f023ab887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bcd29dfb504576ae51b461bb43fa9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n",
      "Image transforms defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Setup - Tokenizer and Transforms\n",
    "\n",
    "# 1. Load Tokenizer\n",
    "print(f\"Loading Tokenizer: {config.text_tokenizer}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer)\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading tokenizer '{config.text_tokenizer}': {e}\")\n",
    "    print(\"Please ensure the model name is correct and you have internet access or the model is cached.\")\n",
    "    # Optionally raise the error to stop execution\n",
    "    # raise e\n",
    "\n",
    "# 2. Define Image Transforms\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(config.size),\n",
    "    transforms.CenterCrop(config.size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # ImageNet stats\n",
    "])\n",
    "print(\"Image transforms defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Attempting to load data from: /root/TuningModels/json_data/train.json\n",
      "Found 19844 samples in train.json.\n",
      "Attempting to load data from: /root/TuningModels/json_data/dev.json\n",
      "Found 5667 samples in dev.json.\n",
      "\n",
      "Creating dataloaders...\n",
      "Using 20 workers for DataLoaders.\n",
      "Train loader created with 311 batches.\n",
      "Validation loader created with 89 batches.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Setup - Datasets and DataLoaders\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "# Construct JSON paths using config.data_path\n",
    "# /Users/quanghuypham/Desktop/CP_Glimpse/Tuning-CLIP/data/UIT-OpenViIC-dataset/test.json\n",
    "train_json = f\"{os.path.abspath(config.data_path)}/train.json\"\n",
    "dev_json = f\"{os.path.abspath(config.data_path)}/dev.json\"\n",
    "test_json = f\"{os.path.abspath(config.data_path)}/test.json\"\n",
    "\n",
    "# Make sure tokenizer is loaded before creating datasets\n",
    "if 'tokenizer' not in globals():\n",
    "     print(\"ERROR: Tokenizer not loaded. Please run the previous cell.\")\n",
    "else:\n",
    "    train_dataset = ImageCaptionDataset(\n",
    "        json_path=train_json, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, transforms=image_transforms, max_length=config.max_length\n",
    "    )\n",
    "    dev_dataset = ImageCaptionDataset(\n",
    "        json_path=dev_json, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, transforms=image_transforms, max_length=config.max_length\n",
    "    )\n",
    "\n",
    "    # Basic checks after loading\n",
    "    if not train_dataset.data:\n",
    "        print(\"\\nERROR: Failed to load training data. Check 'train_json' path and format.\")\n",
    "        # Optionally raise an error: raise ValueError(\"Training data failed to load\")\n",
    "    if not dev_dataset.data:\n",
    "         print(\"\\nWARNING: Failed to load validation data. Validation steps will be skipped.\")\n",
    "\n",
    "    print(\"\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    train_loader = None\n",
    "    if train_dataset.data:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False # Keep last incomplete batch for training\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "\n",
    "    dev_loader = None\n",
    "    if dev_dataset.data:\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False # Keep last incomplete batch for validation\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "\n",
    "    if not train_loader:\n",
    "         print(\"\\nERROR: Train loader could not be created. Cannot proceed.\")\n",
    "         # raise ValueError(\"Train loader creation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model components...\n",
      "Initializing Image Encoder: ResNet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 58.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded ResNet50 from torchvision. Input features: 2048\n",
      "  Added projection head: 2048 -> 256\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Expected input features: 768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2416e45689b49178bb2039fc25d2bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added projection head: 768 -> 256\n",
      "\n",
      "CLIPViModel initialized successfully on cuda.\n",
      "Trainable parameters: 159.23 M\n",
      "\n",
      "Setting up optimizer...\n",
      "  Param counts: ImgBase=159, ImgHead=2, TxtBase=199, TxtHead=2\n",
      "Optimizer AdamW initialized.\n",
      "LR Scheduler ReduceLROnPlateau initialized (mode='max', factor=0.8, patience=7)\n",
      "Early stopping initialized (patience=5, min_delta=0.001)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Setup - Model, Optimizer, Scheduler\n",
    "\n",
    "print(\"\\nInitializing model components...\")\n",
    "try:\n",
    "    image_encoder = ImageEncoder(config).to(config.device)\n",
    "    text_encoder = TextEncoder(config).to(config.device)\n",
    "    model = CLIPViModel(image_encoder, text_encoder, config).to(config.device)\n",
    "    print(f\"\\nCLIPViModel initialized successfully on {config.device}.\")\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {num_params / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing models: {e}\")\n",
    "    print(\"Check model names in CFG, internet connection, and available memory.\")\n",
    "    # raise e # Optionally stop execution\n",
    "\n",
    "# --- Optimizer Setup ---\n",
    "if 'model' in globals() : # Check if model was created\n",
    "    print(\"\\nSetting up optimizer...\")\n",
    "    image_encoder_params = list(model.image_encoder.model.parameters())\n",
    "    image_head_params = list(model.image_encoder.projection.parameters())\n",
    "    text_encoder_params = list(model.text_encoder.model.parameters())\n",
    "    text_head_params = list(model.text_encoder.projection.parameters())\n",
    "\n",
    "    print(f\"  Param counts: ImgBase={len(image_encoder_params)}, ImgHead={len(image_head_params)}, TxtBase={len(text_encoder_params)}, TxtHead={len(text_head_params)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for p in image_encoder_params if p.requires_grad], \"lr\": config.image_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": [p for p in image_head_params if p.requires_grad], \"lr\": config.head_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": [p for p in text_encoder_params if p.requires_grad], \"lr\": config.text_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": [p for p in text_head_params if p.requires_grad], \"lr\": config.head_lr, \"weight_decay\": config.weight_decay},\n",
    "    ]\n",
    "\n",
    "    # Filter out groups with no parameters (can happen if parts are frozen)\n",
    "    optimizer_grouped_parameters = [g for g in optimizer_grouped_parameters if g['params']]\n",
    "\n",
    "    if not optimizer_grouped_parameters:\n",
    "         print(\"ERROR: No parameters found for the optimizer. Check model structure and requires_grad flags.\")\n",
    "         # raise ValueError(\"Optimizer has no parameters\")\n",
    "    else:\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "        print(f\"Optimizer AdamW initialized.\")\n",
    "\n",
    "        # --- LR Scheduler Setup ---\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode=config.mode, # Use mode from config ('min' for loss, 'max' for acc/recall)\n",
    "            factor=config.factor, patience=config.patience\n",
    "        )\n",
    "        print(f\"LR Scheduler ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.factor}, patience={config.patience})\")\n",
    "        \n",
    "        # --- Early Stopping Setup ---\n",
    "        early_stopping_patience = getattr(config, 'early_stopping_patience', 5) \n",
    "        early_stopping_min_delta = getattr(config, 'early_stopping_min_delta', 0.001)\n",
    "        early_stopping_counter = 0\n",
    "        print(f\"Early stopping initialized (patience={early_stopping_patience}, min_delta={early_stopping_min_delta})\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not initialized. Skipping optimizer/scheduler setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 32 epochs...\n",
      "Tracking metric: 'avg_acc' (mode: max)\n",
      "\n",
      "--- Epoch 1/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc02b604890443685055e5c599a7372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E1:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686c174e33124ad0bc5b84b574b2d330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.2186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5712b1984cc4de685f450745606704f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E1:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.8516 | avg_acc: 0.4439 | avg_cosine_sim: 0.5309 | i2t R@1: 0.4463 | i2t R@5: 0.8198 | i2t R@10: 0.9229 | t2i R@1: 0.4415 | t2i R@5: 0.8202 | t2i R@10: 0.9174\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "  Saved Best Model (Epoch 1, avg_acc=0.4439) to ./ViCLIP_landmark/clip_vi_best.pt\n",
      "--- Epoch 1 Time: 210.68 seconds ---\n",
      "\n",
      "--- Epoch 2/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebae96412f54752a043a1857c63ce14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E2:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 1.4219\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f2c186d74f4efb9382c10ebe67e8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E2:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.7302 | avg_acc: 0.4769 | avg_cosine_sim: 0.5019 | i2t R@1: 0.4756 | i2t R@5: 0.8456 | i2t R@10: 0.9370 | t2i R@1: 0.4782 | t2i R@5: 0.8422 | t2i R@10: 0.9321\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "  Saved Best Model (Epoch 2, avg_acc=0.4769) to ./ViCLIP_landmark/clip_vi_best.pt\n",
      "--- Epoch 2 Time: 247.73 seconds ---\n",
      "\n",
      "--- Epoch 3/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54d13a105b84cdbb78c44a88b278ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E3:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 1.1034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f8d0d4c3e747faafb39ca7c2199815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E3:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6849 | avg_acc: 0.4951 | avg_cosine_sim: 0.4945 | i2t R@1: 0.5008 | i2t R@5: 0.8560 | i2t R@10: 0.9405 | t2i R@1: 0.4895 | t2i R@5: 0.8525 | t2i R@10: 0.9345\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "  Saved Best Model (Epoch 3, avg_acc=0.4951) to ./ViCLIP_landmark/clip_vi_best.pt\n",
      "--- Epoch 3 Time: 246.74 seconds ---\n",
      "\n",
      "--- Epoch 4/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a398fe16234b440c909df0f902b21b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E4:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.8830\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2896eacc848240b68458e0d6134d372f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E4:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6728 | avg_acc: 0.4993 | avg_cosine_sim: 0.4755 | i2t R@1: 0.5059 | i2t R@5: 0.8574 | i2t R@10: 0.9359 | t2i R@1: 0.4927 | t2i R@5: 0.8525 | t2i R@10: 0.9333\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "  Saved Best Model (Epoch 4, avg_acc=0.4993) to ./ViCLIP_landmark/clip_vi_best.pt\n",
      "--- Epoch 4 Time: 246.75 seconds ---\n",
      "\n",
      "--- Epoch 5/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e030c55af484cd8ab18f400c19bb3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E5:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.7158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f530ef54f0c4466cba85e1d22ef1ae70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E5:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6259 | avg_acc: 0.5110 | avg_cosine_sim: 0.4726 | i2t R@1: 0.5064 | i2t R@5: 0.8654 | i2t R@10: 0.9446 | t2i R@1: 0.5156 | t2i R@5: 0.8657 | t2i R@10: 0.9441\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "  Saved Best Model (Epoch 5, avg_acc=0.5110) to ./ViCLIP_landmark/clip_vi_best.pt\n",
      "--- Epoch 5 Time: 243.22 seconds ---\n",
      "\n",
      "--- Epoch 6/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b59f2d09fe4acba7cae2a42b1267a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E6:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.5991\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a5a9aed3744d0eb7f8afa241b10944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E6:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6404 | avg_acc: 0.5206 | avg_cosine_sim: 0.4685 | i2t R@1: 0.5146 | i2t R@5: 0.8546 | i2t R@10: 0.9381 | t2i R@1: 0.5267 | t2i R@5: 0.8579 | t2i R@10: 0.9368\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "  Saved Best Model (Epoch 6, avg_acc=0.5206) to ./ViCLIP_landmark/clip_vi_best.pt\n",
      "--- Epoch 6 Time: 241.60 seconds ---\n",
      "\n",
      "--- Epoch 7/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592bd37030fe4d85b23d3bcde79feabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E7:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.5217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb989ce1e12d4ae8825e4bb03ee29f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E7:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6092 | avg_acc: 0.5251 | avg_cosine_sim: 0.4676 | i2t R@1: 0.5239 | i2t R@5: 0.8664 | i2t R@10: 0.9418 | t2i R@1: 0.5262 | t2i R@5: 0.8620 | t2i R@10: 0.9427\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "  Saved Best Model (Epoch 7, avg_acc=0.5251) to ./ViCLIP_landmark/clip_vi_best.pt\n",
      "--- Epoch 7 Time: 243.40 seconds ---\n",
      "\n",
      "--- Epoch 8/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73cf9aceb864321bca401426c0cdf48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E8:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.4666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5f773172a345c1b94472867393539b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E8:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6092 | avg_acc: 0.5279 | avg_cosine_sim: 0.4694 | i2t R@1: 0.5280 | i2t R@5: 0.8675 | i2t R@10: 0.9397 | t2i R@1: 0.5278 | t2i R@5: 0.8627 | t2i R@10: 0.9421\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "  Saved Best Model (Epoch 8, avg_acc=0.5279) to ./ViCLIP_landmark/clip_vi_best.pt\n",
      "--- Epoch 8 Time: 246.29 seconds ---\n",
      "\n",
      "--- Epoch 9/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a247c7fce0492db3221e7cb7ec60f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E9:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.4090\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72476fd93ca24f9b8264ce14b58dfb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E9:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6477 | avg_acc: 0.5203 | avg_cosine_sim: 0.4546 | i2t R@1: 0.5163 | i2t R@5: 0.8627 | i2t R@10: 0.9368 | t2i R@1: 0.5243 | t2i R@5: 0.8581 | t2i R@10: 0.9319\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "--- Epoch 9 Time: 243.49 seconds ---\n",
      "\n",
      "--- Epoch 10/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d34d1761aa419a9b35503cd0715246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E10:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.3605\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e537258a658a4c4492bb27b825f71999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E10:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6299 | avg_acc: 0.5280 | avg_cosine_sim: 0.4567 | i2t R@1: 0.5244 | i2t R@5: 0.8659 | i2t R@10: 0.9374 | t2i R@1: 0.5315 | t2i R@5: 0.8654 | t2i R@10: 0.9361\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "  Saved Best Model (Epoch 10, avg_acc=0.5280) to ./ViCLIP_landmark/clip_vi_best.pt\n",
      "--- Epoch 10 Time: 240.42 seconds ---\n",
      "\n",
      "--- Epoch 11/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b21f838543943f8b2d5a32285a8b864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E11:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 0.3256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1df44d8a25f4ac6aa56a065e56143f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E11:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6179 | avg_acc: 0.5258 | avg_cosine_sim: 0.4551 | i2t R@1: 0.5255 | i2t R@5: 0.8609 | i2t R@10: 0.9412 | t2i R@1: 0.5260 | t2i R@5: 0.8673 | t2i R@10: 0.9381\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "--- Epoch 11 Time: 243.58 seconds ---\n",
      "\n",
      "--- Epoch 12/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8188c4b3c6764a988f519d4b974270ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E12:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 0.2926\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47036eced6a4d2bac9f6f164b2e2c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E12:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6132 | avg_acc: 0.5369 | avg_cosine_sim: 0.4574 | i2t R@1: 0.5317 | i2t R@5: 0.8594 | i2t R@10: 0.9367 | t2i R@1: 0.5421 | t2i R@5: 0.8622 | t2i R@10: 0.9382\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "  Saved Best Model (Epoch 12, avg_acc=0.5369) to ./ViCLIP_landmark/clip_vi_best.pt\n",
      "--- Epoch 12 Time: 242.69 seconds ---\n",
      "\n",
      "--- Epoch 13/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72ade79eeae4c3aa621964fd7655765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E13:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 0.2637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aadc01ec103495793f7f19b23224c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E13:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6537 | avg_acc: 0.5245 | avg_cosine_sim: 0.4499 | i2t R@1: 0.5188 | i2t R@5: 0.8609 | i2t R@10: 0.9382 | t2i R@1: 0.5303 | t2i R@5: 0.8578 | t2i R@10: 0.9349\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "--- Epoch 13 Time: 249.17 seconds ---\n",
      "\n",
      "--- Epoch 14/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f3d4f666d142ffb7974f85e1b244b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E14:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 0.2427\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceeefc948da24b00ad61b81159eb4d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E14:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6239 | avg_acc: 0.5336 | avg_cosine_sim: 0.4519 | i2t R@1: 0.5317 | i2t R@5: 0.8615 | i2t R@10: 0.9352 | t2i R@1: 0.5356 | t2i R@5: 0.8597 | t2i R@10: 0.9333\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "--- Epoch 14 Time: 245.34 seconds ---\n",
      "\n",
      "--- Epoch 15/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50daf732f3324b37b055111446295ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E15:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 0.2297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10619d7b97ee4c53914b1540d7062593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E15:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6305 | avg_acc: 0.5327 | avg_cosine_sim: 0.4432 | i2t R@1: 0.5297 | i2t R@5: 0.8595 | i2t R@10: 0.9361 | t2i R@1: 0.5357 | t2i R@5: 0.8629 | t2i R@10: 0.9312\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "--- Epoch 15 Time: 244.41 seconds ---\n",
      "\n",
      "--- Epoch 16/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87173786a3e4c0ba26750fa14857558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E16:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss = 0.2173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63074aa4fe734a98aafd7b411085f535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E16:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 1.6272 | avg_acc: 0.5321 | avg_cosine_sim: 0.4456 | i2t R@1: 0.5260 | i2t R@5: 0.8682 | i2t R@10: 0.9340 | t2i R@1: 0.5382 | t2i R@5: 0.8592 | t2i R@10: 0.9331\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "--- Epoch 16 Time: 242.97 seconds ---\n",
      "\n",
      "--- Epoch 17/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33a91337f394c63bd74b0c03eb6e8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E17:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 11: Training Loop\n",
    "\n",
    "if 'model' in globals() and 'train_loader' in globals() and 'optimizer' in globals(): # Check prerequisites\n",
    "    print(f\"\\nStarting training for {config.epochs} epochs...\")\n",
    "    print(f\"Tracking metric: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    history = {'train_loss': [], 'validation_results': []}\n",
    "    start_train_time = time.time()\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "        # --- Training ---\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, config.device, epoch+1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_results = {\"loss\": float('inf'), \"avg_acc\": 0.0} # Default if no validation\n",
    "        if dev_loader:\n",
    "            val_results = validate_epoch(model, dev_loader, config.device, epoch+1)\n",
    "            history['validation_results'].append(val_results)\n",
    "            # Print validation metrics\n",
    "            print(\"  Validation Metrics:\")\n",
    "            metric_log_str = \"  \"\n",
    "            for name, value in val_results.items():\n",
    "                metric_log_str += f\"{name}: {value:.4f} | \"\n",
    "            print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "            # Step the LR scheduler based on the tracked metric\n",
    "            current_val_metric_for_scheduler = val_results.get(config.metric_to_track, None)\n",
    "            if current_val_metric_for_scheduler is not None:\n",
    "                 lr_scheduler.step(current_val_metric_for_scheduler)\n",
    "                 current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "                 # Simplified LR print: print first + last group LR assuming Head LRs are same\n",
    "                 print(f\"  Current LRs: ImgEnc={current_lrs[0]:.2e}, Head={current_lrs[1]:.2e}, TxtEnc={current_lrs[2]:.2e}\")\n",
    "            else:\n",
    "                 print(f\"  Warning: Metric '{config.metric_to_track}' not found in validation results. Scheduler not stepped.\")\n",
    "\n",
    "        else:\n",
    "             print(\"  Validation skipped.\")\n",
    "             history['validation_results'].append(None) # Append None if no validation\n",
    "\n",
    "        # --- Save Checkpoint ---\n",
    "        current_val_metric = val_results.get(config.metric_to_track, -float('inf') if config.mode == \"max\" else float('inf'))\n",
    "\n",
    "        is_best = False\n",
    "        if dev_loader: # Only compare if validation was done\n",
    "            if config.mode == \"max\" and current_val_metric > best_val_metric:\n",
    "                is_best = True\n",
    "                best_val_metric = current_val_metric\n",
    "            elif config.mode == \"min\" and current_val_metric < best_val_metric:\n",
    "                is_best = True\n",
    "                best_val_metric = current_val_metric\n",
    "\n",
    "        # Prepare save dictionary\n",
    "        save_dict = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'validation_results': val_results,\n",
    "            'best_val_metric': best_val_metric, # Store the best metric value seen so far\n",
    "            'metric_tracked': config.metric_to_track, # Store which metric was tracked\n",
    "        }\n",
    "\n",
    "        # Save logic\n",
    "        if config.save_best_only and dev_loader:\n",
    "            if is_best:\n",
    "                best_checkpoint_path = os.path.join(config.model_path, \"clip_vi_best.pt\")\n",
    "                torch.save(save_dict, best_checkpoint_path)\n",
    "                print(f\"  Saved Best Model (Epoch {epoch+1}, {config.metric_to_track}={current_val_metric:.4f}) to {best_checkpoint_path}\")\n",
    "        else: # Save every epoch if not save_best_only\n",
    "            epoch_checkpoint_path = os.path.join(config.model_path, f\"clip_vi_epoch_{epoch+1}.pt\")\n",
    "            torch.save(save_dict, epoch_checkpoint_path)\n",
    "            print(f\"  Saved Epoch {epoch+1} Checkpoint to {epoch_checkpoint_path}\")\n",
    "            if is_best and dev_loader: # Also save a copy as best if it's the best so far\n",
    "                 best_checkpoint_path = os.path.join(config.model_path, \"clip_vi_best.pt\")\n",
    "                 torch.save(save_dict, best_checkpoint_path)\n",
    "                 print(f\"  (Also saved as best model)\")\n",
    "\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {epoch_end_time - epoch_start_time:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "    # --- End of Training ---\n",
    "    end_train_time = time.time()\n",
    "    total_train_time = end_train_time - start_train_time\n",
    "    print(f\"\\n=============== Training Finished ===============\")\n",
    "    print(f\"Total Training Time: {total_train_time:.2f} seconds ({total_train_time/60:.2f} minutes)\")\n",
    "    final_model_path = os.path.join(config.model_path, 'clip_vi_final_epoch.pt')\n",
    "    torch.save(save_dict, final_model_path) # Save the final epoch state regardless\n",
    "    print(f\"Final epoch model state saved to {final_model_path}\")\n",
    "    if config.save_best_only and dev_loader and os.path.exists(os.path.join(config.model_path, \"clip_vi_best.pt\")):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) saved to: {os.path.join(config.model_path, 'clip_vi_best.pt')}\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training (model, dataloader, optimizer) not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Starting Test Set Evaluation ===============\n",
      "Loading test data from: ./data/LANDMARK-IN-VIETNAM/test.json\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/data/LANDMARK-IN-VIETNAM/test.json\n",
      "Found 2845 samples in test.json.\n",
      "Test loader created with 45 batches.\n",
      "Initializing Image Encoder: ResNet50\n",
      "  Loaded ResNet50 from torchvision. Input features: 2048\n",
      "  Added projection head: 2048 -> 256\n",
      "ERROR creating model structure for testing: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "================= Evaluation Finished =================\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Final Evaluation on Test Set\n",
    "\n",
    "print(\"\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "\n",
    "if os.path.exists(test_json_path) and 'tokenizer' in globals():\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    test_dataset = ImageCaptionDataset(\n",
    "        json_path=test_json_path, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, transforms=image_transforms, max_length=config.max_length\n",
    "    )\n",
    "\n",
    "    if test_dataset.data:\n",
    "        num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False\n",
    "        )\n",
    "        print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "\n",
    "        # --- Load Model for Testing ---\n",
    "        # Create a fresh model instance\n",
    "        try:\n",
    "            test_image_encoder = ImageEncoder(config).to(config.device)\n",
    "            test_text_encoder = TextEncoder(config).to(config.device)\n",
    "            model_to_test = CLIPViModel(test_image_encoder, test_text_encoder, config).to(config.device)\n",
    "            print(\"Model structure for testing created.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR creating model structure for testing: {e}\")\n",
    "            model_to_test = None # Prevent loading if structure fails\n",
    "\n",
    "        if model_to_test:\n",
    "            # Determine which model weights to load (best or final)\n",
    "            best_model_path = os.path.join(config.model_path, \"clip_vi_best.pt\")\n",
    "            final_model_path = os.path.join(config.model_path, \"clip_vi_final_epoch.pt\") # Use final epoch saved name\n",
    "\n",
    "            load_path = None\n",
    "            if os.path.exists(best_model_path):\n",
    "                load_path = best_model_path\n",
    "                print(f\"Attempting to load best model weights from: {load_path}\")\n",
    "            elif os.path.exists(final_model_path):\n",
    "                load_path = final_model_path\n",
    "                print(f\"Best model not found. Attempting to load final epoch weights from: {load_path}\")\n",
    "            else:\n",
    "                print(\"WARNING: No saved model checkpoints ('best' or 'final') found in output directory.\")\n",
    "                print(\"         Evaluation will not be performed.\")\n",
    "\n",
    "            if load_path:\n",
    "                try:\n",
    "                    checkpoint = torch.load(load_path, map_location=config.device)\n",
    "                    state_dict = checkpoint['model_state_dict']\n",
    "                    # Handle potential 'module.' prefix if saved with DataParallel or DDP\n",
    "                    if next(iter(state_dict)).startswith('module.'):\n",
    "                        print(\"Detected 'module.' prefix, removing for loading.\")\n",
    "                        from collections import OrderedDict\n",
    "                        new_state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "                        model_to_test.load_state_dict(new_state_dict)\n",
    "                    else:\n",
    "                        model_to_test.load_state_dict(state_dict)\n",
    "                    print(f\"Model weights loaded successfully from {load_path}\")\n",
    "\n",
    "                    # --- Run Evaluation ---\n",
    "                    print(\"\\nRunning evaluation on test set...\")\n",
    "                    test_results = validate_epoch(model_to_test, test_loader, config.device, epoch_num=\"Test\") # Use validate func\n",
    "\n",
    "                    print(\"\\n--- Test Set Results ---\")\n",
    "                    metric_log_str = \"\"\n",
    "                    for name, value in test_results.items():\n",
    "                        metric_log_str += f\"  {name}: {value:.4f}\\n\"\n",
    "                    print(metric_log_str.strip())\n",
    "                    print(\"------------------------\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nERROR loading model weights or running evaluation from {load_path}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc() # Print detailed traceback\n",
    "            # --- End Model Loading/Eval ---\n",
    "        # --- End Model Structure Check ---\n",
    "    else:\n",
    "         print(\"Could not load test data. Skipping test evaluation.\")\n",
    "else:\n",
    "    if not os.path.exists(test_json_path):\n",
    "         print(f\"\\nTest JSON path not found ({test_json_path}). Skipping test evaluation.\")\n",
    "    if 'tokenizer' not in globals():\n",
    "         print(\"\\nTokenizer not available. Skipping test evaluation.\")\n",
    "\n",
    "print(\"\\n================= Evaluation Finished =================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Training Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def plot_training_metrics(history):\n",
    "    if not history['train_loss'] or not history['validation_results']:\n",
    "        print(\"No training history available to plot.\")\n",
    "        return\n",
    "        \n",
    "    # Create figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16, y=1.02)\n",
    "    \n",
    "    # Plot Training Loss\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    val_loss = [res['loss'] for res in history['validation_results'] if res]\n",
    "    \n",
    "    axes[0,0].plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    if val_loss:\n",
    "        axes[0,0].plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "    axes[0,0].set_title('Loss over Epochs')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True)\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    if history['validation_results'] and 'avg_acc' in history['validation_results'][0]:\n",
    "        val_acc = [res['avg_acc'] for res in history['validation_results'] if res]\n",
    "        axes[0,1].plot(epochs, val_acc, 'g-', label='Average Accuracy')\n",
    "        axes[0,1].set_title('Validation Accuracy over Epochs')\n",
    "        axes[0,1].set_xlabel('Epoch')\n",
    "        axes[0,1].set_ylabel('Accuracy')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True)\n",
    "    \n",
    "    # Plot Recall Metrics\n",
    "    if history['validation_results'] and 'i2t R@1' in history['validation_results'][0]:\n",
    "        i2t_recall = {\n",
    "            'R@1': [res['i2t R@1'] for res in history['validation_results'] if res],\n",
    "            'R@5': [res['i2t R@5'] for res in history['validation_results'] if res],\n",
    "            'R@10': [res['i2t R@10'] for res in history['validation_results'] if res]\n",
    "        }\n",
    "        \n",
    "        for k, values in i2t_recall.items():\n",
    "            axes[1,0].plot(epochs, values, label=f'I2T {k}')\n",
    "        axes[1,0].set_title('Image-to-Text Recall over Epochs')\n",
    "        axes[1,0].set_xlabel('Epoch')\n",
    "        axes[1,0].set_ylabel('Recall')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True)\n",
    "        \n",
    "        t2i_recall = {\n",
    "            'R@1': [res['t2i R@1'] for res in history['validation_results'] if res],\n",
    "            'R@5': [res['t2i R@5'] for res in history['validation_results'] if res],\n",
    "            'R@10': [res['t2i R@10'] for res in history['validation_results'] if res]\n",
    "        }\n",
    "        \n",
    "        for k, values in t2i_recall.items():\n",
    "            axes[1,1].plot(epochs, values, label=f'T2I {k}')\n",
    "        axes[1,1].set_title('Text-to-Image Recall over Epochs')\n",
    "        axes[1,1].set_xlabel('Epoch')\n",
    "        axes[1,1].set_ylabel('Recall')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot if history exists\n",
    "if 'history' in globals():\n",
    "    plot_training_metrics(history)\n",
    "else:\n",
    "    print(\"No training history found. Run training first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
