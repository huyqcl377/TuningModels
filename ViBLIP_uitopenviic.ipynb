{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "Transformers Version: 4.50.0\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: Installs and Imports ===\n",
    "# !pip install -q transformers torch torchvision torchaudio Pillow tqdm sentencepiece\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Import BLIP specific components\n",
    "from transformers import BlipProcessor, BlipVisionModel, BlipTextModel, BlipConfig\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "# Use standard tqdm if running as a script\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time # For timing epochs\n",
    "import transformers\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model output path: ./ViCLIP_uitopenviic\n",
      "Selected BLIP Model: Salesforce/blip-image-captioning-large\n",
      "Image base path (for resolving paths in JSON): /home/researcher/huypq69/TuningModels/data/UIT-OpenViIC-dataset\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Configuration Class (CFG) ===\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    data_path = \"./json_data//\"\n",
    "    image_path = \"./data/UIT-OpenViIC-dataset/\"\n",
    "\n",
    "    # Output directory for saved models\n",
    "    model_path = \"./ViCLIP_uitopenviic\"\n",
    "\n",
    "    # --- BLIP Model Selection (Original BLIP) ---\n",
    "    # Options: Salesforce/blip-image-captioning-large, Salesforce/blip-vqa-large, etc.\n",
    "    selected_blip_model = \"Salesforce/blip-image-captioning-large\"\n",
    "\n",
    "    # --- Model parameters ---\n",
    "    blip_processor_name = selected_blip_model\n",
    "    blip_vision_model_name = selected_blip_model\n",
    "    blip_text_model_name = selected_blip_model\n",
    "\n",
    "    @property\n",
    "    def text_embedding(self): # Output dim of BlipTextModel base\n",
    "        # Typically 768 for BERT-base like models used in BLIP\n",
    "        # Verify with config if needed: BlipConfig.from_pretrained(self.selected_blip_model).text_config.hidden_size\n",
    "        return 768\n",
    "    @property\n",
    "    def image_embedding(self): # Output dim of BlipVisionModel large\n",
    "        # Typically 1024 for ViT-L used in BLIP-large\n",
    "        # Verify with config: BlipConfig.from_pretrained(self.selected_blip_model).vision_config.hidden_size\n",
    "        return 1024\n",
    "\n",
    "    projection_dim = 256 # Shared latent space dimension\n",
    "\n",
    "    # --- Training parameters ---\n",
    "    seed = 42\n",
    "    # BLIP-large is smaller than BLIP2, can use larger batch size\n",
    "    batch_size = 8  # Adjust based on GPU memory (e.g., 32, 64)\n",
    "    num_workers = 32  # Adjust based on system\n",
    "    # Learning rates (adjust based on experiments)\n",
    "    projection_lr = 1e-4\n",
    "    vision_encoder_lr = 1e-5\n",
    "    text_encoder_lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    patience = 3 # Scheduler patience\n",
    "    factor = 0.8 # Scheduler reduction factor\n",
    "    epochs = 1 # Adjust as needed\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_amp = False # Disabled for simplicity with BLIP original, can be re-enabled\n",
    "\n",
    "    # --- Image/Text parameters ---\n",
    "    # Image size is determined by the BlipProcessor\n",
    "    max_length = 64 # Max text sequence length for tokenizer\n",
    "\n",
    "    # --- Loss/Saving parameters ---\n",
    "    temperature = 0.07\n",
    "    learnable_temperature = True\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"avg_acc\" # Changed default to Recall@1 average\n",
    "    mode = \"max\" # Mode for scheduler/saving\n",
    "    early_stopping_patience = 5\n",
    "    early_stopping_min_delta = 0.001\n",
    "    accumulation_steps = 1 # Set > 1 if needed for lower batch sizes\n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Selected BLIP Model: {config.selected_blip_model}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: Seeding for Reproducibility ===\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # torch.backends.cudnn.deterministic = True\n",
    "        # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Metric Calculation Utilities ===\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"): self.name = name; self.reset()\n",
    "    def reset(self): self.sum = 0; self.count = 0; self.avg = 0\n",
    "    def update(self, val, count=1):\n",
    "        if torch.is_tensor(val): val = val.item()\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count; self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "    def __repr__(self): return f\"{self.name}: {self.avg:.4f}\"\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    n = similarity_matrix.shape[1-dim]\n",
    "    k_eff = min(k, similarity_matrix.shape[dim])\n",
    "    if k_eff == 0 or n == 0: return 0.0\n",
    "    top_k_indices = torch.topk(similarity_matrix, k_eff, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "    correct_count = 0\n",
    "    if dim == 0: # I2T\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]: correct_count += 1\n",
    "    elif dim == 1: # T2I\n",
    "        for txt_idx in range(n):\n",
    "             if ground_truth[txt_idx] in top_k_indices[txt_idx, :]: correct_count += 1\n",
    "    else: raise ValueError(\"dim must be 0 or 1\")\n",
    "    return correct_count / n if n > 0 else 0.0\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    if image_embeddings.device != text_embeddings.device:\n",
    "        text_embeddings = text_embeddings.to(image_embeddings.device)\n",
    "    sim_matrix = text_embeddings @ image_embeddings.T\n",
    "    sim_matrix = sim_matrix.float()\n",
    "    n = sim_matrix.shape[0]\n",
    "    if n == 0:\n",
    "        return { \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0, \"avg_cosine_sim\": 0.0,\n",
    "                 \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "                 \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "                 \"avg_R@1\": 0.0, \"avg_R@5\": 0.0, \"avg_R@10\": 0.0 }\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "    t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "    t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "    avg_acc = (i2t_acc + t2i_acc) / 2\n",
    "    avg_cosine_sim = torch.diagonal(sim_matrix).mean().item()\n",
    "    i2t_recall, t2i_recall, avg_recall = {}, {}, {}\n",
    "    recall_k_values = [1, 5, 10]\n",
    "    for k in recall_k_values:\n",
    "        k_str = f\"R@{k}\"\n",
    "        i2t_recall[k_str] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "        t2i_recall[k_str] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "        avg_recall[f\"avg_{k_str}\"] = (i2t_recall[k_str] + t2i_recall[k_str]) / 2\n",
    "    metrics = { \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "                \"avg_cosine_sim\": avg_cosine_sim, \"i2t_recall\": i2t_recall,\n",
    "                \"t2i_recall\": t2i_recall, **avg_recall }\n",
    "    return metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlipImageCaptionDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Dataset Class Definition (for BLIP original) ===\n",
    "class BlipImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, processor, max_length):\n",
    "        super().__init__()\n",
    "        print(f\"Attempting to load data from: {os.path.abspath(json_path)}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f: self.data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading JSON {json_path}: {e}\")\n",
    "            self.data = []\n",
    "\n",
    "        print(f\"Found {len(self.data)} samples in {os.path.basename(json_path)}.\")\n",
    "        self.image_base_path = image_base_path\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        try: # Get image size from processor\n",
    "             self.img_size = processor.image_processor.size['height']\n",
    "        except: self.img_size = 224 # Fallback\n",
    "        print(f\"Using image size: {self.img_size}x{self.img_size}\")\n",
    "        if not os.path.isdir(self.image_base_path): print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data): raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path')\n",
    "        captions = item.get('caption', [])\n",
    "        caption = captions[0] if captions else \"\"\n",
    "\n",
    "        # Load Image\n",
    "        image = None\n",
    "        pixel_values = torch.zeros((3, self.img_size, self.img_size)) # Dummy tensor\n",
    "        if relative_image_path:\n",
    "            image_path = os.path.normpath(os.path.join(self.image_base_path, relative_image_path))\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "            except FileNotFoundError: print(f\"Warning: Img not found: {image_path}. Using dummy for idx {idx}.\")\n",
    "            except Exception as e: print(f\"Warning: Error loading image {image_path}: {e}. Using dummy for idx {idx}.\")\n",
    "        else: print(f\"Warning: Missing 'image_path' for idx {idx}. Using dummy.\")\n",
    "\n",
    "        if image is None: # If loading failed or path missing\n",
    "            image = Image.new('RGB', (self.img_size, self.img_size)) # Use dummy PIL image\n",
    "\n",
    "        # Process image and text together using BlipProcessor\n",
    "        try:\n",
    "            inputs = self.processor(images=image, text=caption, padding=\"max_length\", truncation=True,\n",
    "                                    max_length=self.max_length, return_tensors=\"pt\")\n",
    "            # Squeeze processor outputs before returning\n",
    "            pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "            input_ids = inputs['input_ids'].squeeze(0)\n",
    "            attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing idx {idx} (caption: '{caption}'): {e}\")\n",
    "            # Return dummy tensors if processing fails\n",
    "            input_ids = torch.zeros(self.max_length, dtype=torch.long)\n",
    "            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n",
    "            # pixel_values remains the dummy tensor initialized earlier\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "print(\"BlipImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP Original Retrieval Model components and loss function defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Model Definition (BLIP Original Retrieval Model, Loss) ===\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Encodes images using BLIP's Vision Model.\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        print(f\"Initializing BLIP Vision Encoder: {config_train.blip_vision_model_name}\")\n",
    "        self.vision_model = BlipVisionModel.from_pretrained(config_train.blip_vision_model_name)\n",
    "        # Freeze if needed (optional for BLIP original)\n",
    "        # for param in self.vision_model.parameters(): param.requires_grad = False\n",
    "        self.projection = nn.Linear(config_train.image_embedding, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {config_train.image_embedding} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # Get image features from vision model\n",
    "        vision_outputs = self.vision_model(pixel_values=pixel_values)\n",
    "        # Use pooled output (e.g., CLS token) for projection\n",
    "        image_features = vision_outputs.pooler_output # Shape: [batch_size, image_embedding_dim]\n",
    "        projected_features = self.projection(image_features)\n",
    "        projected_features = F.normalize(projected_features, p=2, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Encodes text using BLIP's Text Model.\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        print(f\"Initializing BLIP Text Encoder: {config_train.blip_text_model_name}\")\n",
    "        self.text_model = BlipTextModel.from_pretrained(config_train.blip_text_model_name)\n",
    "        # Freeze if needed (optional)\n",
    "        # for param in self.text_model.parameters(): param.requires_grad = False\n",
    "        self.projection = nn.Linear(config_train.text_embedding, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {config_train.text_embedding} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get text features\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use pooled output (e.g., CLS token)\n",
    "        text_features = text_outputs.pooler_output # Shape: [batch_size, text_embedding_dim]\n",
    "        # Alternative: text_features = text_outputs.last_hidden_state[:, 0, :] # CLS token state\n",
    "        projected_features = self.projection(text_features)\n",
    "        projected_features = F.normalize(projected_features, p=2, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "class BlipRetrievalModel(nn.Module):\n",
    "    \"\"\"Combines BLIP encoders for contrastive retrieval.\"\"\"\n",
    "    def __init__(self, image_encoder, text_encoder, config_train):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.config_train = config_train\n",
    "\n",
    "        if config_train.learnable_temperature:\n",
    "            self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / config_train.temperature))\n",
    "            print(f\"Using learnable temperature, initialized to {self.logit_scale.exp().item():.4f}\")\n",
    "        else:\n",
    "            self.register_buffer('logit_scale', torch.tensor(np.log(1 / config_train.temperature)))\n",
    "            print(f\"Using fixed temperature: {config_train.temperature}\")\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        image_features = self.image_encoder(pixel_values)\n",
    "        text_features = self.text_encoder(input_ids, attention_mask)\n",
    "\n",
    "        # Cosine similarity scaling\n",
    "        if isinstance(self.logit_scale, nn.Parameter):\n",
    "             current_logit_scale = self.logit_scale.exp()\n",
    "        else:\n",
    "             current_logit_scale = self.logit_scale.exp().to(image_features.device) # Move buffer if needed\n",
    "\n",
    "        # Ensure FP32 for stability if inputs are FP16/BF16\n",
    "        logits_per_image = current_logit_scale.float() * image_features.float() @ text_features.float().t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text, image_features, text_features\n",
    "\n",
    "# --- Loss Function (Contrastive Loss) ---\n",
    "def contrastive_loss(logits_per_image, logits_per_text):\n",
    "    # Ensure logits are FP32 for cross_entropy\n",
    "    logits_per_image = logits_per_image.float()\n",
    "    logits_per_text = logits_per_text.float()\n",
    "\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    if batch_size == 0: return torch.tensor(0.0, device=logits_per_image.device, requires_grad=True)\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "    loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "    total_loss = (loss_img + loss_txt) / 2.0\n",
    "    return total_loss\n",
    "\n",
    "print(\"BLIP Original Retrieval Model components and loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation epoch functions defined (No AMP).\n"
     ]
    }
   ],
   "source": [
    "# === Cell 7: Training and Validation Epoch Functions (No AMP/Scaler) ===\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch_num):\n",
    "    model.train()\n",
    "    loss_meter = AvgMeter(f\"Train Loss E{epoch_num}\")\n",
    "    try: from tqdm.notebook import tqdm as pbar\n",
    "    except ImportError: from tqdm import tqdm as pbar\n",
    "    progress_bar = pbar(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_size = pixel_values.size(0)\n",
    "        if batch_size == 0: continue\n",
    "\n",
    "        logits_per_image, logits_per_text, _, _ = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "        # Ensure logits are float32 for loss\n",
    "        loss = contrastive_loss(logits_per_image.float(), logits_per_text.float())\n",
    "        loss = loss / config.accumulation_steps # Normalize loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % config.accumulation_steps == 0 or (i + 1) == len(dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_meter.update(loss.item() * config.accumulation_steps, batch_size) # Log un-normalized loss\n",
    "        progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    optimizer.zero_grad() # Clean up at end\n",
    "    return loss_meter.avg\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, device, epoch_num):\n",
    "    model.eval()\n",
    "    loss_meter = AvgMeter(f\"Val Loss E{epoch_num}\")\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    try: from tqdm.notebook import tqdm as pbar\n",
    "    except ImportError: from tqdm import tqdm as pbar\n",
    "    progress_bar = pbar(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = pixel_values.size(0)\n",
    "            if batch_size == 0: continue\n",
    "\n",
    "            logits_per_image, logits_per_text, image_embeds, text_embeds = model(pixel_values, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(logits_per_image.float(), logits_per_text.float()) # Ensure FP32\n",
    "\n",
    "            loss_meter.update(loss.item(), batch_size)\n",
    "            all_image_embeddings.append(image_embeds.cpu())\n",
    "            all_text_embeddings.append(text_embeds.cpu())\n",
    "            progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    if not all_image_embeddings or not all_text_embeddings:\n",
    "         print(\"Warning: No embeddings collected during validation.\")\n",
    "         zero_metrics = { \"loss\": loss_meter.avg, \"avg acc\": 0.0, \"avg cosine sim\": 0.0,\n",
    "                           \"i2t recall R@1\": 0.0, \"i2t recall R@5\": 0.0, \"i2t recall R@10\": 0.0,\n",
    "                           \"t2i recall R@1\": 0.0, \"t2i recall R@5\": 0.0, \"t2i recall R@10\": 0.0,\n",
    "                           \"avg R@1\": 0.0, \"avg R@5\": 0.0, \"avg R@10\": 0.0 }\n",
    "         return zero_metrics\n",
    "\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "\n",
    "    print(f\"\\nComputing metrics over {all_image_embeddings.shape[0]} validation samples...\")\n",
    "    validation_metrics = compute_metrics(all_image_embeddings.to(device), all_text_embeddings.to(device))\n",
    "\n",
    "    final_results = {\"loss\": loss_meter.avg}\n",
    "    for k, v in validation_metrics.items():\n",
    "        if isinstance(v, dict): # Handle recall dicts\n",
    "            for recall_k, recall_v in v.items(): final_results[f\"{k.replace('_', ' ')} {recall_k}\"] = recall_v\n",
    "        else: final_results[k.replace('_', ' ')] = v\n",
    "    return final_results\n",
    "\n",
    "print(\"Training and Validation epoch functions defined (No AMP).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP Processor: Salesforce/blip-image-captioning-large\n",
      "Processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8: Setup - BLIP Processor ===\n",
    "print(f\"Loading BLIP Processor: {config.blip_processor_name}\")\n",
    "try:\n",
    "    processor = BlipProcessor.from_pretrained(config.blip_processor_name)\n",
    "    print(\"Processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading processor '{config.blip_processor_name}': {e}\")\n",
    "    processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/json_data/train.json\n",
      "Found 41236 samples in train.json.\n",
      "Using image size: 384x384\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/json_data/dev.json\n",
      "Found 10002 samples in dev.json.\n",
      "Using image size: 384x384\n",
      "\n",
      "Creating dataloaders...\n",
      "Using 32 workers for DataLoaders.\n",
      "Train loader created with 5154 batches.\n",
      "Validation loader created with 1251 batches.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 9: Setup - Datasets and DataLoaders ===\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "\n",
    "if processor:\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_json = os.path.join(config.data_path, \"train.json\")\n",
    "    dev_json = os.path.join(config.data_path, \"dev.json\")\n",
    "\n",
    "    train_dataset = BlipImageCaptionDataset(\n",
    "        json_path=train_json, image_base_path=config.image_path,\n",
    "        processor=processor, max_length=config.max_length\n",
    "    )\n",
    "    dev_dataset = BlipImageCaptionDataset(\n",
    "        json_path=dev_json, image_base_path=config.image_path,\n",
    "        processor=processor, max_length=config.max_length\n",
    "    )\n",
    "\n",
    "    if not train_dataset.data: print(\"\\nERROR: Failed to load training data.\")\n",
    "    if not dev_dataset.data: print(\"\\nWARNING: Failed to load validation data.\")\n",
    "\n",
    "    print(\"\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    if train_dataset.data:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=True\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "    else: print(\"Skipping train loader creation.\")\n",
    "\n",
    "    if dev_dataset.data:\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "    else: print(\"Skipping validation loader creation.\")\n",
    "\n",
    "    if not train_loader: print(\"\\nERROR: Train loader could not be created.\")\n",
    "else:\n",
    "     print(\"ERROR: BLIP Processor not loaded. Skipping dataset/loader creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model components...\n",
      "Initializing BLIP Vision Encoder: Salesforce/blip-image-captioning-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipVisionModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-large and are newly initialized: ['embeddings.class_embedding', 'embeddings.patch_embedding.bias', 'embeddings.patch_embedding.weight', 'embeddings.position_embedding', 'encoder.layers.0.layer_norm1.bias', 'encoder.layers.0.layer_norm1.weight', 'encoder.layers.0.layer_norm2.bias', 'encoder.layers.0.layer_norm2.weight', 'encoder.layers.0.mlp.fc1.bias', 'encoder.layers.0.mlp.fc1.weight', 'encoder.layers.0.mlp.fc2.bias', 'encoder.layers.0.mlp.fc2.weight', 'encoder.layers.0.self_attn.projection.bias', 'encoder.layers.0.self_attn.projection.weight', 'encoder.layers.0.self_attn.qkv.bias', 'encoder.layers.0.self_attn.qkv.weight', 'encoder.layers.1.layer_norm1.bias', 'encoder.layers.1.layer_norm1.weight', 'encoder.layers.1.layer_norm2.bias', 'encoder.layers.1.layer_norm2.weight', 'encoder.layers.1.mlp.fc1.bias', 'encoder.layers.1.mlp.fc1.weight', 'encoder.layers.1.mlp.fc2.bias', 'encoder.layers.1.mlp.fc2.weight', 'encoder.layers.1.self_attn.projection.bias', 'encoder.layers.1.self_attn.projection.weight', 'encoder.layers.1.self_attn.qkv.bias', 'encoder.layers.1.self_attn.qkv.weight', 'encoder.layers.10.layer_norm1.bias', 'encoder.layers.10.layer_norm1.weight', 'encoder.layers.10.layer_norm2.bias', 'encoder.layers.10.layer_norm2.weight', 'encoder.layers.10.mlp.fc1.bias', 'encoder.layers.10.mlp.fc1.weight', 'encoder.layers.10.mlp.fc2.bias', 'encoder.layers.10.mlp.fc2.weight', 'encoder.layers.10.self_attn.projection.bias', 'encoder.layers.10.self_attn.projection.weight', 'encoder.layers.10.self_attn.qkv.bias', 'encoder.layers.10.self_attn.qkv.weight', 'encoder.layers.11.layer_norm1.bias', 'encoder.layers.11.layer_norm1.weight', 'encoder.layers.11.layer_norm2.bias', 'encoder.layers.11.layer_norm2.weight', 'encoder.layers.11.mlp.fc1.bias', 'encoder.layers.11.mlp.fc1.weight', 'encoder.layers.11.mlp.fc2.bias', 'encoder.layers.11.mlp.fc2.weight', 'encoder.layers.11.self_attn.projection.bias', 'encoder.layers.11.self_attn.projection.weight', 'encoder.layers.11.self_attn.qkv.bias', 'encoder.layers.11.self_attn.qkv.weight', 'encoder.layers.12.layer_norm1.bias', 'encoder.layers.12.layer_norm1.weight', 'encoder.layers.12.layer_norm2.bias', 'encoder.layers.12.layer_norm2.weight', 'encoder.layers.12.mlp.fc1.bias', 'encoder.layers.12.mlp.fc1.weight', 'encoder.layers.12.mlp.fc2.bias', 'encoder.layers.12.mlp.fc2.weight', 'encoder.layers.12.self_attn.projection.bias', 'encoder.layers.12.self_attn.projection.weight', 'encoder.layers.12.self_attn.qkv.bias', 'encoder.layers.12.self_attn.qkv.weight', 'encoder.layers.13.layer_norm1.bias', 'encoder.layers.13.layer_norm1.weight', 'encoder.layers.13.layer_norm2.bias', 'encoder.layers.13.layer_norm2.weight', 'encoder.layers.13.mlp.fc1.bias', 'encoder.layers.13.mlp.fc1.weight', 'encoder.layers.13.mlp.fc2.bias', 'encoder.layers.13.mlp.fc2.weight', 'encoder.layers.13.self_attn.projection.bias', 'encoder.layers.13.self_attn.projection.weight', 'encoder.layers.13.self_attn.qkv.bias', 'encoder.layers.13.self_attn.qkv.weight', 'encoder.layers.14.layer_norm1.bias', 'encoder.layers.14.layer_norm1.weight', 'encoder.layers.14.layer_norm2.bias', 'encoder.layers.14.layer_norm2.weight', 'encoder.layers.14.mlp.fc1.bias', 'encoder.layers.14.mlp.fc1.weight', 'encoder.layers.14.mlp.fc2.bias', 'encoder.layers.14.mlp.fc2.weight', 'encoder.layers.14.self_attn.projection.bias', 'encoder.layers.14.self_attn.projection.weight', 'encoder.layers.14.self_attn.qkv.bias', 'encoder.layers.14.self_attn.qkv.weight', 'encoder.layers.15.layer_norm1.bias', 'encoder.layers.15.layer_norm1.weight', 'encoder.layers.15.layer_norm2.bias', 'encoder.layers.15.layer_norm2.weight', 'encoder.layers.15.mlp.fc1.bias', 'encoder.layers.15.mlp.fc1.weight', 'encoder.layers.15.mlp.fc2.bias', 'encoder.layers.15.mlp.fc2.weight', 'encoder.layers.15.self_attn.projection.bias', 'encoder.layers.15.self_attn.projection.weight', 'encoder.layers.15.self_attn.qkv.bias', 'encoder.layers.15.self_attn.qkv.weight', 'encoder.layers.16.layer_norm1.bias', 'encoder.layers.16.layer_norm1.weight', 'encoder.layers.16.layer_norm2.bias', 'encoder.layers.16.layer_norm2.weight', 'encoder.layers.16.mlp.fc1.bias', 'encoder.layers.16.mlp.fc1.weight', 'encoder.layers.16.mlp.fc2.bias', 'encoder.layers.16.mlp.fc2.weight', 'encoder.layers.16.self_attn.projection.bias', 'encoder.layers.16.self_attn.projection.weight', 'encoder.layers.16.self_attn.qkv.bias', 'encoder.layers.16.self_attn.qkv.weight', 'encoder.layers.17.layer_norm1.bias', 'encoder.layers.17.layer_norm1.weight', 'encoder.layers.17.layer_norm2.bias', 'encoder.layers.17.layer_norm2.weight', 'encoder.layers.17.mlp.fc1.bias', 'encoder.layers.17.mlp.fc1.weight', 'encoder.layers.17.mlp.fc2.bias', 'encoder.layers.17.mlp.fc2.weight', 'encoder.layers.17.self_attn.projection.bias', 'encoder.layers.17.self_attn.projection.weight', 'encoder.layers.17.self_attn.qkv.bias', 'encoder.layers.17.self_attn.qkv.weight', 'encoder.layers.18.layer_norm1.bias', 'encoder.layers.18.layer_norm1.weight', 'encoder.layers.18.layer_norm2.bias', 'encoder.layers.18.layer_norm2.weight', 'encoder.layers.18.mlp.fc1.bias', 'encoder.layers.18.mlp.fc1.weight', 'encoder.layers.18.mlp.fc2.bias', 'encoder.layers.18.mlp.fc2.weight', 'encoder.layers.18.self_attn.projection.bias', 'encoder.layers.18.self_attn.projection.weight', 'encoder.layers.18.self_attn.qkv.bias', 'encoder.layers.18.self_attn.qkv.weight', 'encoder.layers.19.layer_norm1.bias', 'encoder.layers.19.layer_norm1.weight', 'encoder.layers.19.layer_norm2.bias', 'encoder.layers.19.layer_norm2.weight', 'encoder.layers.19.mlp.fc1.bias', 'encoder.layers.19.mlp.fc1.weight', 'encoder.layers.19.mlp.fc2.bias', 'encoder.layers.19.mlp.fc2.weight', 'encoder.layers.19.self_attn.projection.bias', 'encoder.layers.19.self_attn.projection.weight', 'encoder.layers.19.self_attn.qkv.bias', 'encoder.layers.19.self_attn.qkv.weight', 'encoder.layers.2.layer_norm1.bias', 'encoder.layers.2.layer_norm1.weight', 'encoder.layers.2.layer_norm2.bias', 'encoder.layers.2.layer_norm2.weight', 'encoder.layers.2.mlp.fc1.bias', 'encoder.layers.2.mlp.fc1.weight', 'encoder.layers.2.mlp.fc2.bias', 'encoder.layers.2.mlp.fc2.weight', 'encoder.layers.2.self_attn.projection.bias', 'encoder.layers.2.self_attn.projection.weight', 'encoder.layers.2.self_attn.qkv.bias', 'encoder.layers.2.self_attn.qkv.weight', 'encoder.layers.20.layer_norm1.bias', 'encoder.layers.20.layer_norm1.weight', 'encoder.layers.20.layer_norm2.bias', 'encoder.layers.20.layer_norm2.weight', 'encoder.layers.20.mlp.fc1.bias', 'encoder.layers.20.mlp.fc1.weight', 'encoder.layers.20.mlp.fc2.bias', 'encoder.layers.20.mlp.fc2.weight', 'encoder.layers.20.self_attn.projection.bias', 'encoder.layers.20.self_attn.projection.weight', 'encoder.layers.20.self_attn.qkv.bias', 'encoder.layers.20.self_attn.qkv.weight', 'encoder.layers.21.layer_norm1.bias', 'encoder.layers.21.layer_norm1.weight', 'encoder.layers.21.layer_norm2.bias', 'encoder.layers.21.layer_norm2.weight', 'encoder.layers.21.mlp.fc1.bias', 'encoder.layers.21.mlp.fc1.weight', 'encoder.layers.21.mlp.fc2.bias', 'encoder.layers.21.mlp.fc2.weight', 'encoder.layers.21.self_attn.projection.bias', 'encoder.layers.21.self_attn.projection.weight', 'encoder.layers.21.self_attn.qkv.bias', 'encoder.layers.21.self_attn.qkv.weight', 'encoder.layers.22.layer_norm1.bias', 'encoder.layers.22.layer_norm1.weight', 'encoder.layers.22.layer_norm2.bias', 'encoder.layers.22.layer_norm2.weight', 'encoder.layers.22.mlp.fc1.bias', 'encoder.layers.22.mlp.fc1.weight', 'encoder.layers.22.mlp.fc2.bias', 'encoder.layers.22.mlp.fc2.weight', 'encoder.layers.22.self_attn.projection.bias', 'encoder.layers.22.self_attn.projection.weight', 'encoder.layers.22.self_attn.qkv.bias', 'encoder.layers.22.self_attn.qkv.weight', 'encoder.layers.23.layer_norm1.bias', 'encoder.layers.23.layer_norm1.weight', 'encoder.layers.23.layer_norm2.bias', 'encoder.layers.23.layer_norm2.weight', 'encoder.layers.23.mlp.fc1.bias', 'encoder.layers.23.mlp.fc1.weight', 'encoder.layers.23.mlp.fc2.bias', 'encoder.layers.23.mlp.fc2.weight', 'encoder.layers.23.self_attn.projection.bias', 'encoder.layers.23.self_attn.projection.weight', 'encoder.layers.23.self_attn.qkv.bias', 'encoder.layers.23.self_attn.qkv.weight', 'encoder.layers.3.layer_norm1.bias', 'encoder.layers.3.layer_norm1.weight', 'encoder.layers.3.layer_norm2.bias', 'encoder.layers.3.layer_norm2.weight', 'encoder.layers.3.mlp.fc1.bias', 'encoder.layers.3.mlp.fc1.weight', 'encoder.layers.3.mlp.fc2.bias', 'encoder.layers.3.mlp.fc2.weight', 'encoder.layers.3.self_attn.projection.bias', 'encoder.layers.3.self_attn.projection.weight', 'encoder.layers.3.self_attn.qkv.bias', 'encoder.layers.3.self_attn.qkv.weight', 'encoder.layers.4.layer_norm1.bias', 'encoder.layers.4.layer_norm1.weight', 'encoder.layers.4.layer_norm2.bias', 'encoder.layers.4.layer_norm2.weight', 'encoder.layers.4.mlp.fc1.bias', 'encoder.layers.4.mlp.fc1.weight', 'encoder.layers.4.mlp.fc2.bias', 'encoder.layers.4.mlp.fc2.weight', 'encoder.layers.4.self_attn.projection.bias', 'encoder.layers.4.self_attn.projection.weight', 'encoder.layers.4.self_attn.qkv.bias', 'encoder.layers.4.self_attn.qkv.weight', 'encoder.layers.5.layer_norm1.bias', 'encoder.layers.5.layer_norm1.weight', 'encoder.layers.5.layer_norm2.bias', 'encoder.layers.5.layer_norm2.weight', 'encoder.layers.5.mlp.fc1.bias', 'encoder.layers.5.mlp.fc1.weight', 'encoder.layers.5.mlp.fc2.bias', 'encoder.layers.5.mlp.fc2.weight', 'encoder.layers.5.self_attn.projection.bias', 'encoder.layers.5.self_attn.projection.weight', 'encoder.layers.5.self_attn.qkv.bias', 'encoder.layers.5.self_attn.qkv.weight', 'encoder.layers.6.layer_norm1.bias', 'encoder.layers.6.layer_norm1.weight', 'encoder.layers.6.layer_norm2.bias', 'encoder.layers.6.layer_norm2.weight', 'encoder.layers.6.mlp.fc1.bias', 'encoder.layers.6.mlp.fc1.weight', 'encoder.layers.6.mlp.fc2.bias', 'encoder.layers.6.mlp.fc2.weight', 'encoder.layers.6.self_attn.projection.bias', 'encoder.layers.6.self_attn.projection.weight', 'encoder.layers.6.self_attn.qkv.bias', 'encoder.layers.6.self_attn.qkv.weight', 'encoder.layers.7.layer_norm1.bias', 'encoder.layers.7.layer_norm1.weight', 'encoder.layers.7.layer_norm2.bias', 'encoder.layers.7.layer_norm2.weight', 'encoder.layers.7.mlp.fc1.bias', 'encoder.layers.7.mlp.fc1.weight', 'encoder.layers.7.mlp.fc2.bias', 'encoder.layers.7.mlp.fc2.weight', 'encoder.layers.7.self_attn.projection.bias', 'encoder.layers.7.self_attn.projection.weight', 'encoder.layers.7.self_attn.qkv.bias', 'encoder.layers.7.self_attn.qkv.weight', 'encoder.layers.8.layer_norm1.bias', 'encoder.layers.8.layer_norm1.weight', 'encoder.layers.8.layer_norm2.bias', 'encoder.layers.8.layer_norm2.weight', 'encoder.layers.8.mlp.fc1.bias', 'encoder.layers.8.mlp.fc1.weight', 'encoder.layers.8.mlp.fc2.bias', 'encoder.layers.8.mlp.fc2.weight', 'encoder.layers.8.self_attn.projection.bias', 'encoder.layers.8.self_attn.projection.weight', 'encoder.layers.8.self_attn.qkv.bias', 'encoder.layers.8.self_attn.qkv.weight', 'encoder.layers.9.layer_norm1.bias', 'encoder.layers.9.layer_norm1.weight', 'encoder.layers.9.layer_norm2.bias', 'encoder.layers.9.layer_norm2.weight', 'encoder.layers.9.mlp.fc1.bias', 'encoder.layers.9.mlp.fc1.weight', 'encoder.layers.9.mlp.fc2.bias', 'encoder.layers.9.mlp.fc2.weight', 'encoder.layers.9.self_attn.projection.bias', 'encoder.layers.9.self_attn.projection.weight', 'encoder.layers.9.self_attn.qkv.bias', 'encoder.layers.9.self_attn.qkv.weight', 'post_layernorm.bias', 'post_layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added projection head: 1024 -> 256\n",
      "Initializing BLIP Text Encoder: Salesforce/blip-image-captioning-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipTextModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-large and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added projection head: 768 -> 256\n",
      "Using learnable temperature, initialized to 14.2857\n",
      "\n",
      "BlipRetrievalModel initialized successfully on cuda.\n",
      "Total parameters: 446.72 M\n",
      "Trainable parameters: 446.72 M\n",
      "\n",
      "Setting up optimizer...\n",
      "  Param counts (Trainable): VisionBase=294, VisionHead=1, TextBase=318, TextHead=1, LogitScale=1\n",
      "Optimizer AdamW initialized.\n",
      "LR Scheduler ReduceLROnPlateau initialized (mode='max', factor=0.8, patience=3)\n",
      "Early stopping initialized (patience=5, min_delta=0.001)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10: Setup - Model, Optimizer, Scheduler ===\n",
    "model = None\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "\n",
    "print(\"\\nInitializing model components...\")\n",
    "try:\n",
    "    image_encoder = ImageEncoder(config).to(config.device)\n",
    "    text_encoder = TextEncoder(config).to(config.device)\n",
    "    model = BlipRetrievalModel(image_encoder, text_encoder, config).to(config.device)\n",
    "    print(f\"\\nBlipRetrievalModel initialized successfully on {config.device}.\")\n",
    "    num_params_total = sum(p.numel() for p in model.parameters())\n",
    "    num_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {num_params_total / 1e6:.2f} M\")\n",
    "    print(f\"Trainable parameters: {num_params_trainable / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing BLIP model components: {e}\")\n",
    "    model = None\n",
    "\n",
    "if model:\n",
    "    print(\"\\nSetting up optimizer...\")\n",
    "    image_encoder_params = [p for p in model.image_encoder.vision_model.parameters() if p.requires_grad]\n",
    "    image_head_params = [p for p in model.image_encoder.projection.parameters() if p.requires_grad]\n",
    "    text_encoder_params = [p for p in model.text_encoder.text_model.parameters() if p.requires_grad]\n",
    "    text_head_params = [p for p in model.text_encoder.projection.parameters() if p.requires_grad]\n",
    "    logit_scale_param = [model.logit_scale] if isinstance(model.logit_scale, nn.Parameter) else []\n",
    "\n",
    "    print(f\"  Param counts (Trainable): VisionBase={len(image_encoder_params)}, VisionHead={len(image_head_params)}, TextBase={len(text_encoder_params)}, TextHead={len(text_head_params)}, LogitScale={len(logit_scale_param)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": image_encoder_params, \"lr\": config.vision_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": image_head_params, \"lr\": config.projection_lr, \"weight_decay\": config.weight_decay}, # Use projection LR for heads\n",
    "        {\"params\": text_encoder_params, \"lr\": config.text_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": text_head_params, \"lr\": config.projection_lr, \"weight_decay\": config.weight_decay},  # Use projection LR for heads\n",
    "        {\"params\": logit_scale_param, \"lr\": config.projection_lr, \"weight_decay\": 0} # Often use head LR, no decay for scale\n",
    "    ]\n",
    "\n",
    "    optimizer_grouped_parameters = [g for g in optimizer_grouped_parameters if g['params']]\n",
    "\n",
    "    if not optimizer_grouped_parameters:\n",
    "         print(\"ERROR: No trainable parameters found for the optimizer.\")\n",
    "    else:\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "        print(f\"Optimizer AdamW initialized.\")\n",
    "\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode=config.mode, factor=config.factor, patience=config.patience\n",
    "        )\n",
    "        print(f\"LR Scheduler ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.factor}, patience={config.patience})\")\n",
    "\n",
    "        early_stopping_counter = 0\n",
    "        print(f\"Early stopping initialized (patience={config.early_stopping_patience}, min_delta={config.early_stopping_min_delta})\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not initialized. Skipping optimizer/scheduler setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 1 epochs...\n",
      "Tracking metric: 'avg_acc' (mode: max)\n",
      "\n",
      "--- Epoch 1/1 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442a055a4c8a40898086cf46c76953ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E1:   0%|          | 0/5154 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/researcher/huypq69/TuningModels/.venv/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 2215.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/researcher/huypq69/TuningModels/.venv/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# --- Training ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_loss)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device, epoch_num)\u001b[39m\n\u001b[32m     21\u001b[39m loss = contrastive_loss(logits_per_image.float(), logits_per_text.float())\n\u001b[32m     22\u001b[39m loss = loss / config.accumulation_steps \u001b[38;5;66;03m# Normalize loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % config.accumulation_steps == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (i + \u001b[32m1\u001b[39m) == \u001b[38;5;28mlen\u001b[39m(dataloader):\n\u001b[32m     27\u001b[39m     optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Cell 11: Training Loop (No AMP) ===\n",
    "if model and train_loader and optimizer and lr_scheduler:\n",
    "    print(f\"\\nStarting training for {config.epochs} epochs...\")\n",
    "    print(f\"Tracking metric: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    history = {'train_loss': [], 'validation_results': []}\n",
    "    start_train_time = time.time()\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "\n",
    "        # --- Training ---\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, config.device, epoch+1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_results = {\"loss\": float('inf'), config.metric_to_track.replace('_', ' '): (-float('inf') if config.mode == 'max' else float('inf'))}\n",
    "        if dev_loader:\n",
    "            val_results = validate_epoch(model, dev_loader, config.device, epoch+1)\n",
    "            history['validation_results'].append(val_results)\n",
    "            print(\"  Validation Metrics:\")\n",
    "            metric_log_str = \"  \"\n",
    "            sorted_keys = sorted(val_results.keys(), key=lambda x: (x.split()[0], int(x.split('@')[-1]) if '@' in x else -1))\n",
    "            for name in sorted_keys: metric_log_str += f\"{name}: {val_results[name]:.4f} | \"\n",
    "            print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "            # --- Scheduler Step ---\n",
    "            current_val_metric_for_scheduler = val_results.get(config.metric_to_track.replace('_', ' '), None)\n",
    "            if current_val_metric_for_scheduler is not None:\n",
    "                lr_scheduler.step(current_val_metric_for_scheduler)\n",
    "                current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "                # Updated LR mapping for BLIP original structure\n",
    "                lr_str = f\"  Current LRs: VisionBase={current_lrs[0]:.2e}, VisionHead={current_lrs[1]:.2e}, TextBase={current_lrs[2]:.2e}, TextHead={current_lrs[3]:.2e}\"\n",
    "                if len(current_lrs) > 4: lr_str += f\", LogitScale={current_lrs[4]:.2e}\"\n",
    "                print(lr_str)\n",
    "            else: print(f\"  Warning: Metric '{config.metric_to_track}' not found. Scheduler not stepped.\")\n",
    "        else:\n",
    "            print(\"  Validation skipped (no dev_loader).\")\n",
    "            history['validation_results'].append(None)\n",
    "\n",
    "        # --- Save Checkpoint & Early Stopping Logic ---\n",
    "        current_val_metric = val_results.get(config.metric_to_track.replace('_', ' '), -float('inf') if config.mode == \"max\" else float('inf'))\n",
    "        is_best = False\n",
    "        if dev_loader:\n",
    "            if config.mode == \"max\":\n",
    "                if current_val_metric > best_val_metric + config.early_stopping_min_delta: is_best = True\n",
    "            else: # min mode\n",
    "                if current_val_metric < best_val_metric - config.early_stopping_min_delta: is_best = True\n",
    "\n",
    "            if is_best:\n",
    "                print(f\"  Metric '{config.metric_to_track}' improved from {best_val_metric:.4f} to {current_val_metric:.4f}\")\n",
    "                best_val_metric = current_val_metric; no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                print(f\"  Metric '{config.metric_to_track}' did not improve. Best: {best_val_metric:.4f}. Counter: {no_improve_epochs}/{config.early_stopping_patience}\")\n",
    "\n",
    "        save_dict = {\n",
    "            'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'train_loss': train_loss, 'validation_results': val_results,\n",
    "            'best_val_metric': best_val_metric, 'metric_tracked': config.metric_to_track,\n",
    "            # Saving BLIP config might require splitting vision/text config if needed for reload\n",
    "            'blip_vision_config_dict': model.image_encoder.vision_model.config.to_dict(),\n",
    "            'blip_text_config_dict': model.text_encoder.text_model.config.to_dict()\n",
    "        }\n",
    "\n",
    "        best_checkpoint_path = os.path.join(config.model_path, \"blip_retrieval_best.pt\")\n",
    "        final_epoch_path = os.path.join(config.model_path, f\"blip_retrieval_epoch_{epoch+1}.pt\")\n",
    "\n",
    "        if config.save_best_only and dev_loader:\n",
    "            if is_best:\n",
    "                torch.save(save_dict, best_checkpoint_path)\n",
    "                print(f\"  Saved Best Model (Epoch {epoch+1}) to {best_checkpoint_path}\")\n",
    "        else:\n",
    "            torch.save(save_dict, final_epoch_path)\n",
    "            print(f\"  Saved Epoch {epoch+1} Checkpoint to {final_epoch_path}\")\n",
    "            if is_best and dev_loader:\n",
    "                 torch.save(save_dict, best_checkpoint_path)\n",
    "                 print(f\"  (Also saved as best model)\")\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {epoch_end_time - epoch_start_time:.2f} seconds ---\")\n",
    "\n",
    "        if dev_loader and no_improve_epochs >= config.early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {config.early_stopping_patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    end_train_time = time.time()\n",
    "    total_train_time = end_train_time - start_train_time\n",
    "    print(f\"\\n=============== Training Finished ================\")\n",
    "    print(f\"Total Training Time: {total_train_time:.2f} seconds ({total_train_time/60:.2f} minutes)\")\n",
    "    final_model_path = os.path.join(config.model_path, 'blip_retrieval_final_epoch.pt')\n",
    "    torch.save(save_dict, final_model_path)\n",
    "    print(f\"Final epoch model state saved to {final_model_path}\")\n",
    "    best_model_file = os.path.join(config.model_path, \"blip_retrieval_best.pt\")\n",
    "    if dev_loader and os.path.exists(best_model_file):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) is saved at: {best_model_file}\")\n",
    "    elif dev_loader: print(\"Best model checkpoint file not found.\")\n",
    "    print(f\"=================================================\")\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training (model, dataloader, optimizer, scheduler) not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Starting Test Set Evaluation ===============\n",
      "Loading test data from: ./json_data/test.json\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/json_data/test.json\n",
      "Found 2176 samples in test.json.\n",
      "Using image size: 384x384\n",
      "Test loader created with 272 batches.\n",
      "\n",
      "Loading best model: ./ViCLIP_vivqa/blip_retrieval_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_187460/152439419.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(load_path, map_location=config.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-creating model structure for testing...\n",
      "Initializing BLIP Vision Encoder: Salesforce/blip-image-captioning-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipVisionModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-large and are newly initialized: ['embeddings.class_embedding', 'embeddings.patch_embedding.bias', 'embeddings.patch_embedding.weight', 'embeddings.position_embedding', 'encoder.layers.0.layer_norm1.bias', 'encoder.layers.0.layer_norm1.weight', 'encoder.layers.0.layer_norm2.bias', 'encoder.layers.0.layer_norm2.weight', 'encoder.layers.0.mlp.fc1.bias', 'encoder.layers.0.mlp.fc1.weight', 'encoder.layers.0.mlp.fc2.bias', 'encoder.layers.0.mlp.fc2.weight', 'encoder.layers.0.self_attn.projection.bias', 'encoder.layers.0.self_attn.projection.weight', 'encoder.layers.0.self_attn.qkv.bias', 'encoder.layers.0.self_attn.qkv.weight', 'encoder.layers.1.layer_norm1.bias', 'encoder.layers.1.layer_norm1.weight', 'encoder.layers.1.layer_norm2.bias', 'encoder.layers.1.layer_norm2.weight', 'encoder.layers.1.mlp.fc1.bias', 'encoder.layers.1.mlp.fc1.weight', 'encoder.layers.1.mlp.fc2.bias', 'encoder.layers.1.mlp.fc2.weight', 'encoder.layers.1.self_attn.projection.bias', 'encoder.layers.1.self_attn.projection.weight', 'encoder.layers.1.self_attn.qkv.bias', 'encoder.layers.1.self_attn.qkv.weight', 'encoder.layers.10.layer_norm1.bias', 'encoder.layers.10.layer_norm1.weight', 'encoder.layers.10.layer_norm2.bias', 'encoder.layers.10.layer_norm2.weight', 'encoder.layers.10.mlp.fc1.bias', 'encoder.layers.10.mlp.fc1.weight', 'encoder.layers.10.mlp.fc2.bias', 'encoder.layers.10.mlp.fc2.weight', 'encoder.layers.10.self_attn.projection.bias', 'encoder.layers.10.self_attn.projection.weight', 'encoder.layers.10.self_attn.qkv.bias', 'encoder.layers.10.self_attn.qkv.weight', 'encoder.layers.11.layer_norm1.bias', 'encoder.layers.11.layer_norm1.weight', 'encoder.layers.11.layer_norm2.bias', 'encoder.layers.11.layer_norm2.weight', 'encoder.layers.11.mlp.fc1.bias', 'encoder.layers.11.mlp.fc1.weight', 'encoder.layers.11.mlp.fc2.bias', 'encoder.layers.11.mlp.fc2.weight', 'encoder.layers.11.self_attn.projection.bias', 'encoder.layers.11.self_attn.projection.weight', 'encoder.layers.11.self_attn.qkv.bias', 'encoder.layers.11.self_attn.qkv.weight', 'encoder.layers.12.layer_norm1.bias', 'encoder.layers.12.layer_norm1.weight', 'encoder.layers.12.layer_norm2.bias', 'encoder.layers.12.layer_norm2.weight', 'encoder.layers.12.mlp.fc1.bias', 'encoder.layers.12.mlp.fc1.weight', 'encoder.layers.12.mlp.fc2.bias', 'encoder.layers.12.mlp.fc2.weight', 'encoder.layers.12.self_attn.projection.bias', 'encoder.layers.12.self_attn.projection.weight', 'encoder.layers.12.self_attn.qkv.bias', 'encoder.layers.12.self_attn.qkv.weight', 'encoder.layers.13.layer_norm1.bias', 'encoder.layers.13.layer_norm1.weight', 'encoder.layers.13.layer_norm2.bias', 'encoder.layers.13.layer_norm2.weight', 'encoder.layers.13.mlp.fc1.bias', 'encoder.layers.13.mlp.fc1.weight', 'encoder.layers.13.mlp.fc2.bias', 'encoder.layers.13.mlp.fc2.weight', 'encoder.layers.13.self_attn.projection.bias', 'encoder.layers.13.self_attn.projection.weight', 'encoder.layers.13.self_attn.qkv.bias', 'encoder.layers.13.self_attn.qkv.weight', 'encoder.layers.14.layer_norm1.bias', 'encoder.layers.14.layer_norm1.weight', 'encoder.layers.14.layer_norm2.bias', 'encoder.layers.14.layer_norm2.weight', 'encoder.layers.14.mlp.fc1.bias', 'encoder.layers.14.mlp.fc1.weight', 'encoder.layers.14.mlp.fc2.bias', 'encoder.layers.14.mlp.fc2.weight', 'encoder.layers.14.self_attn.projection.bias', 'encoder.layers.14.self_attn.projection.weight', 'encoder.layers.14.self_attn.qkv.bias', 'encoder.layers.14.self_attn.qkv.weight', 'encoder.layers.15.layer_norm1.bias', 'encoder.layers.15.layer_norm1.weight', 'encoder.layers.15.layer_norm2.bias', 'encoder.layers.15.layer_norm2.weight', 'encoder.layers.15.mlp.fc1.bias', 'encoder.layers.15.mlp.fc1.weight', 'encoder.layers.15.mlp.fc2.bias', 'encoder.layers.15.mlp.fc2.weight', 'encoder.layers.15.self_attn.projection.bias', 'encoder.layers.15.self_attn.projection.weight', 'encoder.layers.15.self_attn.qkv.bias', 'encoder.layers.15.self_attn.qkv.weight', 'encoder.layers.16.layer_norm1.bias', 'encoder.layers.16.layer_norm1.weight', 'encoder.layers.16.layer_norm2.bias', 'encoder.layers.16.layer_norm2.weight', 'encoder.layers.16.mlp.fc1.bias', 'encoder.layers.16.mlp.fc1.weight', 'encoder.layers.16.mlp.fc2.bias', 'encoder.layers.16.mlp.fc2.weight', 'encoder.layers.16.self_attn.projection.bias', 'encoder.layers.16.self_attn.projection.weight', 'encoder.layers.16.self_attn.qkv.bias', 'encoder.layers.16.self_attn.qkv.weight', 'encoder.layers.17.layer_norm1.bias', 'encoder.layers.17.layer_norm1.weight', 'encoder.layers.17.layer_norm2.bias', 'encoder.layers.17.layer_norm2.weight', 'encoder.layers.17.mlp.fc1.bias', 'encoder.layers.17.mlp.fc1.weight', 'encoder.layers.17.mlp.fc2.bias', 'encoder.layers.17.mlp.fc2.weight', 'encoder.layers.17.self_attn.projection.bias', 'encoder.layers.17.self_attn.projection.weight', 'encoder.layers.17.self_attn.qkv.bias', 'encoder.layers.17.self_attn.qkv.weight', 'encoder.layers.18.layer_norm1.bias', 'encoder.layers.18.layer_norm1.weight', 'encoder.layers.18.layer_norm2.bias', 'encoder.layers.18.layer_norm2.weight', 'encoder.layers.18.mlp.fc1.bias', 'encoder.layers.18.mlp.fc1.weight', 'encoder.layers.18.mlp.fc2.bias', 'encoder.layers.18.mlp.fc2.weight', 'encoder.layers.18.self_attn.projection.bias', 'encoder.layers.18.self_attn.projection.weight', 'encoder.layers.18.self_attn.qkv.bias', 'encoder.layers.18.self_attn.qkv.weight', 'encoder.layers.19.layer_norm1.bias', 'encoder.layers.19.layer_norm1.weight', 'encoder.layers.19.layer_norm2.bias', 'encoder.layers.19.layer_norm2.weight', 'encoder.layers.19.mlp.fc1.bias', 'encoder.layers.19.mlp.fc1.weight', 'encoder.layers.19.mlp.fc2.bias', 'encoder.layers.19.mlp.fc2.weight', 'encoder.layers.19.self_attn.projection.bias', 'encoder.layers.19.self_attn.projection.weight', 'encoder.layers.19.self_attn.qkv.bias', 'encoder.layers.19.self_attn.qkv.weight', 'encoder.layers.2.layer_norm1.bias', 'encoder.layers.2.layer_norm1.weight', 'encoder.layers.2.layer_norm2.bias', 'encoder.layers.2.layer_norm2.weight', 'encoder.layers.2.mlp.fc1.bias', 'encoder.layers.2.mlp.fc1.weight', 'encoder.layers.2.mlp.fc2.bias', 'encoder.layers.2.mlp.fc2.weight', 'encoder.layers.2.self_attn.projection.bias', 'encoder.layers.2.self_attn.projection.weight', 'encoder.layers.2.self_attn.qkv.bias', 'encoder.layers.2.self_attn.qkv.weight', 'encoder.layers.20.layer_norm1.bias', 'encoder.layers.20.layer_norm1.weight', 'encoder.layers.20.layer_norm2.bias', 'encoder.layers.20.layer_norm2.weight', 'encoder.layers.20.mlp.fc1.bias', 'encoder.layers.20.mlp.fc1.weight', 'encoder.layers.20.mlp.fc2.bias', 'encoder.layers.20.mlp.fc2.weight', 'encoder.layers.20.self_attn.projection.bias', 'encoder.layers.20.self_attn.projection.weight', 'encoder.layers.20.self_attn.qkv.bias', 'encoder.layers.20.self_attn.qkv.weight', 'encoder.layers.21.layer_norm1.bias', 'encoder.layers.21.layer_norm1.weight', 'encoder.layers.21.layer_norm2.bias', 'encoder.layers.21.layer_norm2.weight', 'encoder.layers.21.mlp.fc1.bias', 'encoder.layers.21.mlp.fc1.weight', 'encoder.layers.21.mlp.fc2.bias', 'encoder.layers.21.mlp.fc2.weight', 'encoder.layers.21.self_attn.projection.bias', 'encoder.layers.21.self_attn.projection.weight', 'encoder.layers.21.self_attn.qkv.bias', 'encoder.layers.21.self_attn.qkv.weight', 'encoder.layers.22.layer_norm1.bias', 'encoder.layers.22.layer_norm1.weight', 'encoder.layers.22.layer_norm2.bias', 'encoder.layers.22.layer_norm2.weight', 'encoder.layers.22.mlp.fc1.bias', 'encoder.layers.22.mlp.fc1.weight', 'encoder.layers.22.mlp.fc2.bias', 'encoder.layers.22.mlp.fc2.weight', 'encoder.layers.22.self_attn.projection.bias', 'encoder.layers.22.self_attn.projection.weight', 'encoder.layers.22.self_attn.qkv.bias', 'encoder.layers.22.self_attn.qkv.weight', 'encoder.layers.23.layer_norm1.bias', 'encoder.layers.23.layer_norm1.weight', 'encoder.layers.23.layer_norm2.bias', 'encoder.layers.23.layer_norm2.weight', 'encoder.layers.23.mlp.fc1.bias', 'encoder.layers.23.mlp.fc1.weight', 'encoder.layers.23.mlp.fc2.bias', 'encoder.layers.23.mlp.fc2.weight', 'encoder.layers.23.self_attn.projection.bias', 'encoder.layers.23.self_attn.projection.weight', 'encoder.layers.23.self_attn.qkv.bias', 'encoder.layers.23.self_attn.qkv.weight', 'encoder.layers.3.layer_norm1.bias', 'encoder.layers.3.layer_norm1.weight', 'encoder.layers.3.layer_norm2.bias', 'encoder.layers.3.layer_norm2.weight', 'encoder.layers.3.mlp.fc1.bias', 'encoder.layers.3.mlp.fc1.weight', 'encoder.layers.3.mlp.fc2.bias', 'encoder.layers.3.mlp.fc2.weight', 'encoder.layers.3.self_attn.projection.bias', 'encoder.layers.3.self_attn.projection.weight', 'encoder.layers.3.self_attn.qkv.bias', 'encoder.layers.3.self_attn.qkv.weight', 'encoder.layers.4.layer_norm1.bias', 'encoder.layers.4.layer_norm1.weight', 'encoder.layers.4.layer_norm2.bias', 'encoder.layers.4.layer_norm2.weight', 'encoder.layers.4.mlp.fc1.bias', 'encoder.layers.4.mlp.fc1.weight', 'encoder.layers.4.mlp.fc2.bias', 'encoder.layers.4.mlp.fc2.weight', 'encoder.layers.4.self_attn.projection.bias', 'encoder.layers.4.self_attn.projection.weight', 'encoder.layers.4.self_attn.qkv.bias', 'encoder.layers.4.self_attn.qkv.weight', 'encoder.layers.5.layer_norm1.bias', 'encoder.layers.5.layer_norm1.weight', 'encoder.layers.5.layer_norm2.bias', 'encoder.layers.5.layer_norm2.weight', 'encoder.layers.5.mlp.fc1.bias', 'encoder.layers.5.mlp.fc1.weight', 'encoder.layers.5.mlp.fc2.bias', 'encoder.layers.5.mlp.fc2.weight', 'encoder.layers.5.self_attn.projection.bias', 'encoder.layers.5.self_attn.projection.weight', 'encoder.layers.5.self_attn.qkv.bias', 'encoder.layers.5.self_attn.qkv.weight', 'encoder.layers.6.layer_norm1.bias', 'encoder.layers.6.layer_norm1.weight', 'encoder.layers.6.layer_norm2.bias', 'encoder.layers.6.layer_norm2.weight', 'encoder.layers.6.mlp.fc1.bias', 'encoder.layers.6.mlp.fc1.weight', 'encoder.layers.6.mlp.fc2.bias', 'encoder.layers.6.mlp.fc2.weight', 'encoder.layers.6.self_attn.projection.bias', 'encoder.layers.6.self_attn.projection.weight', 'encoder.layers.6.self_attn.qkv.bias', 'encoder.layers.6.self_attn.qkv.weight', 'encoder.layers.7.layer_norm1.bias', 'encoder.layers.7.layer_norm1.weight', 'encoder.layers.7.layer_norm2.bias', 'encoder.layers.7.layer_norm2.weight', 'encoder.layers.7.mlp.fc1.bias', 'encoder.layers.7.mlp.fc1.weight', 'encoder.layers.7.mlp.fc2.bias', 'encoder.layers.7.mlp.fc2.weight', 'encoder.layers.7.self_attn.projection.bias', 'encoder.layers.7.self_attn.projection.weight', 'encoder.layers.7.self_attn.qkv.bias', 'encoder.layers.7.self_attn.qkv.weight', 'encoder.layers.8.layer_norm1.bias', 'encoder.layers.8.layer_norm1.weight', 'encoder.layers.8.layer_norm2.bias', 'encoder.layers.8.layer_norm2.weight', 'encoder.layers.8.mlp.fc1.bias', 'encoder.layers.8.mlp.fc1.weight', 'encoder.layers.8.mlp.fc2.bias', 'encoder.layers.8.mlp.fc2.weight', 'encoder.layers.8.self_attn.projection.bias', 'encoder.layers.8.self_attn.projection.weight', 'encoder.layers.8.self_attn.qkv.bias', 'encoder.layers.8.self_attn.qkv.weight', 'encoder.layers.9.layer_norm1.bias', 'encoder.layers.9.layer_norm1.weight', 'encoder.layers.9.layer_norm2.bias', 'encoder.layers.9.layer_norm2.weight', 'encoder.layers.9.mlp.fc1.bias', 'encoder.layers.9.mlp.fc1.weight', 'encoder.layers.9.mlp.fc2.bias', 'encoder.layers.9.mlp.fc2.weight', 'encoder.layers.9.self_attn.projection.bias', 'encoder.layers.9.self_attn.projection.weight', 'encoder.layers.9.self_attn.qkv.bias', 'encoder.layers.9.self_attn.qkv.weight', 'post_layernorm.bias', 'post_layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added projection head: 1024 -> 256\n",
      "Initializing BLIP Text Encoder: Salesforce/blip-image-captioning-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipTextModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-large and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added projection head: 768 -> 256\n",
      "Using learnable temperature, initialized to 14.2857\n",
      "  State dict loading result: <All keys matched successfully>\n",
      "Model weights loaded successfully.\n",
      "\n",
      "Running evaluation on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd02d89d78243929429a09a3af76fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation ETest:   0%|          | 0/272 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2176 validation samples...\n",
      "\n",
      "--- Test Set Results ---\n",
      "avg acc: 0.0011\n",
      "  avg cosine sim: 0.4518\n",
      "  avg R@1: 0.0011\n",
      "  avg R@5: 0.0090\n",
      "  avg R@10: 0.0198\n",
      "  i2t acc: 0.0014\n",
      "  i2t recall R@1: 0.0014\n",
      "  i2t recall R@5: 0.0078\n",
      "  i2t recall R@10: 0.0198\n",
      "  loss: 1.7825\n",
      "  t2i acc: 0.0009\n",
      "  t2i recall R@1: 0.0009\n",
      "  t2i recall R@5: 0.0101\n",
      "  t2i recall R@10: 0.0198\n",
      "------------------------\n",
      "\n",
      "================= Evaluation Finished =================\n",
      "Plot directory ensured at: /home/researcher/huypq69/TuningModels/train_plot\n",
      "Saved loss plot to: train_plot/training_loss.png\n",
      "Saved accuracy plot to: train_plot/training_accuracy.png\n",
      "Saved i2t_recall plot to: train_plot/training_i2t_recall.png\n",
      "Saved t2i_recall plot to: train_plot/training_t2i_recall.png\n",
      "Saved combined plot to: train_plot/training_metrics_combined.png\n"
     ]
    }
   ],
   "source": [
    "# === Cell 12: Final Evaluation on Test Set ===\n",
    "print(\"\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_loader = None\n",
    "model_to_test = None\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "\n",
    "if os.path.exists(test_json_path) and 'processor' in globals() and processor:\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    try:\n",
    "        test_dataset = BlipImageCaptionDataset(\n",
    "            json_path=test_json_path, image_base_path=config.image_path,\n",
    "            processor=processor, max_length=config.max_length\n",
    "        )\n",
    "        if test_dataset.data:\n",
    "            num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "                num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "        else: print(\"Test dataset loaded but is empty.\")\n",
    "    except Exception as e: print(f\"Error creating test dataset/loader: {e}\")\n",
    "else: print(\"Skipping test evaluation: Test JSON or Processor not found/loaded.\")\n",
    "\n",
    "if test_loader:\n",
    "    try:\n",
    "        best_model_path = os.path.join(config.model_path, \"blip_retrieval_best.pt\")\n",
    "        final_model_path = os.path.join(config.model_path, \"blip_retrieval_final_epoch.pt\")\n",
    "        load_path = None\n",
    "        if os.path.exists(best_model_path): load_path = best_model_path; print(f\"\\nLoading best model: {load_path}\")\n",
    "        elif os.path.exists(final_model_path): load_path = final_model_path; print(f\"\\nLoading final model: {load_path}\")\n",
    "        else: print(f\"\\nWARNING: No checkpoints found in {config.model_path}.\")\n",
    "\n",
    "        if load_path:\n",
    "            checkpoint = torch.load(load_path, map_location=config.device)\n",
    "            print(\"Re-creating model structure for testing...\")\n",
    "            # Use current config - assumes architecture hasn't changed drastically from training config\n",
    "            test_image_encoder = ImageEncoder(config).to(config.device)\n",
    "            test_text_encoder = TextEncoder(config).to(config.device)\n",
    "            model_to_test = BlipRetrievalModel(test_image_encoder, test_text_encoder, config).to(config.device)\n",
    "\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "                print(\"Detected 'module.' prefix, removing.\")\n",
    "                from collections import OrderedDict\n",
    "                state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "\n",
    "            load_result = model_to_test.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"  State dict loading result: {load_result}\")\n",
    "            if load_result.missing_keys: print(f\"  Warning: Missing keys: {load_result.missing_keys}\")\n",
    "            if load_result.unexpected_keys: print(f\"  Warning: Unexpected keys: {load_result.unexpected_keys}\")\n",
    "            print(f\"Model weights loaded successfully.\")\n",
    "\n",
    "            print(\"\\nRunning evaluation on test set...\")\n",
    "            test_results = validate_epoch(model_to_test, test_loader, config.device, epoch_num=\"Test\")\n",
    "\n",
    "            print(\"\\n--- Test Set Results ---\")\n",
    "            metric_log_str = \"\"\n",
    "            sorted_keys = sorted(test_results.keys(), key=lambda x: (x.split()[0], int(x.split('@')[-1]) if '@' in x else -1))\n",
    "            for name in sorted_keys: metric_log_str += f\"  {name}: {test_results[name]:.4f}\\n\"\n",
    "            print(metric_log_str.strip())\n",
    "            print(\"------------------------\")\n",
    "        else: print(\"Evaluation skipped (no weights found).\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR during test setup/evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n================= Evaluation Finished =================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 13: Training Visualization (Adapted) ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plot_dir = \"train_plot/ViBLIP_uitopenviic\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "print(f\"Plot directory ensured at: {os.path.abspath(plot_dir)}\")\n",
    "\n",
    "def save_subplot_as_figure(subplot, save_path):\n",
    "    fig_new = plt.figure(figsize=(8, 6))\n",
    "    ax_new = fig_new.add_subplot(111)\n",
    "    lines = subplot.get_lines()\n",
    "    if not lines: print(f\"Warning: No lines in subplot for {save_path}\"); plt.close(fig_new); return\n",
    "    labels = [line.get_label() for line in lines]\n",
    "    for line in lines:\n",
    "        ax_new.plot(line.get_xdata(), line.get_ydata(), color=line.get_color(),\n",
    "                    linestyle=line.get_linestyle(), marker=line.get_marker(), label=line.get_label())\n",
    "    ax_new.set_title(subplot.get_title()); ax_new.set_xlabel(subplot.get_xlabel()); ax_new.set_ylabel(subplot.get_ylabel())\n",
    "    ax_new.grid(True)\n",
    "    if any(label and not label.startswith('_') for label in labels): ax_new.legend()\n",
    "    plt.tight_layout()\n",
    "    fig_new.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig_new)\n",
    "\n",
    "def plot_training_metrics(history):\n",
    "    if not history or not history.get('train_loss') or not history.get('validation_results'):\n",
    "        print(\"No/incomplete training history available.\"); return\n",
    "    valid_results = [res for res in history['validation_results'] if res is not None]\n",
    "    if not valid_results:\n",
    "        print(\"No valid validation results. Plotting only training loss.\")\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "        ax.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')\n",
    "        ax.set_title('Training Loss over Epochs'); ax.set_xlabel('Epoch'); ax.set_ylabel('Loss'); ax.legend(); ax.grid(True)\n",
    "        save_path = os.path.join(plot_dir, 'training_loss.png')\n",
    "        fig.savefig(save_path, bbox_inches='tight', dpi=300); print(f\"Saved loss plot to: {save_path}\"); plt.close(fig)\n",
    "        return\n",
    "\n",
    "    num_epochs_trained = len(history['train_loss']); num_epochs_validated = len(valid_results)\n",
    "    epochs_train = range(1, num_epochs_trained + 1); epochs_val = range(1, num_epochs_validated + 1)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 13))\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16, y=1.02)\n",
    "\n",
    "    val_loss = [res.get('loss', float('nan')) for res in valid_results]\n",
    "    axes[0, 0].plot(epochs_train, history['train_loss'], 'b-o', label='Training Loss')\n",
    "    axes[0, 0].plot(epochs_val, val_loss, 'r-s', label='Validation Loss')\n",
    "    axes[0, 0].set_title('Loss'); axes[0, 0].set_xlabel('Epoch'); axes[0, 0].set_ylabel('Loss'); axes[0, 0].legend(); axes[0, 0].grid(True)\n",
    "\n",
    "    metric_key_acc = 'avg acc'\n",
    "    if metric_key_acc in valid_results[0]:\n",
    "        val_acc = [res[metric_key_acc] for res in valid_results]\n",
    "        axes[0, 1].plot(epochs_val, val_acc, 'g-^', label='Avg Accuracy (Val)')\n",
    "        axes[0, 1].set_title('Avg Accuracy'); axes[0, 1].set_xlabel('Epoch'); axes[0, 1].set_ylabel('Accuracy'); axes[0, 1].legend(); axes[0, 1].grid(True)\n",
    "    else: axes[0, 1].set_title(f'{metric_key_acc} (Not Found)')\n",
    "\n",
    "    has_recall = 'i2t recall R@1' in valid_results[0]\n",
    "    if has_recall:\n",
    "        for k in [1, 5, 10]:\n",
    "            axes[1, 0].plot(epochs_val, [res.get(f'i2t recall R@{k}', float('nan')) for res in valid_results], marker='o', label=f'I2T R@{k}')\n",
    "        axes[1, 0].set_title('I2T Recall (Val)'); axes[1, 0].set_xlabel('Epoch'); axes[1, 0].set_ylabel('Recall'); axes[1, 0].legend(); axes[1, 0].grid(True)\n",
    "        for k in [1, 5, 10]:\n",
    "            axes[1, 1].plot(epochs_val, [res.get(f't2i recall R@{k}', float('nan')) for res in valid_results], marker='s', label=f'T2I R@{k}')\n",
    "        axes[1, 1].set_title('T2I Recall (Val)'); axes[1, 1].set_xlabel('Epoch'); axes[1, 1].set_ylabel('Recall'); axes[1, 1].legend(); axes[1, 1].grid(True)\n",
    "    else: axes[1, 0].set_title('I2T Recall (Not Found)'); axes[1, 1].set_title('T2I Recall (Not Found)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plot_names = ['loss', 'accuracy', 'i2t_recall', 't2i_recall']\n",
    "    for idx, name in enumerate(plot_names):\n",
    "        i, j = divmod(idx, 2); save_path = os.path.join(plot_dir, f'training_{name}.png')\n",
    "        if axes[i, j].has_data(): save_subplot_as_figure(axes[i, j], save_path); print(f\"Saved {name} plot to: {save_path}\")\n",
    "        else: print(f\"Skipping save for {name} plot (no data).\")\n",
    "    combined_save_path = os.path.join(plot_dir, 'training_metrics_combined.png')\n",
    "    fig.savefig(combined_save_path, bbox_inches='tight', dpi=300); print(f\"Saved combined plot to: {combined_save_path}\")\n",
    "    # plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "if 'history' in locals() and isinstance(history, dict) and history.get('train_loss'):\n",
    "    plot_training_metrics(history)\n",
    "else:\n",
    "    print(\"No training history found. Run training first.\")\n",
    "\n",
    "# --- END OF SCRIPT ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
