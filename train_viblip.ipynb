{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Installs and Imports\n",
    "\n",
    "# # Install necessary libraries\n",
    "# !pip install torch torchvision torchaudio --quiet\n",
    "# !pip install transformers==4.38.2 --quiet # Pinning version for stability\n",
    "# !pip install Pillow tqdm numpy --quiet\n",
    "# !pip install accelerate --quiet # Needed by transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, Blip2Processor, Blip2Model, Blip2Config, Blip2VisionModel, Blip2QFormerModel, BlipImageProcessor\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Capability: {torch.cuda.get_device_capability(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration Class (CFG)\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    data_path = \".\"  # Adjust to where train.json, dev.json, test.json are located\n",
    "    image_path = \".\"  # Base path for resolving image paths in JSON\n",
    "    model_path = \"./ViBLIP_QFormer_Trained\"  # Output directory for saved models\n",
    "\n",
    "    # --- Model Selection ---\n",
    "    blip2_model_name = \"Salesforce/blip2-opt-2.7b\"  # Uses ViT-B by default\n",
    "    text_tokenizer_name = \"vinai/phobert-base\"  # Vietnamese tokenizer\n",
    "\n",
    "    # --- Training Parameters ---\n",
    "    seed = 42\n",
    "    batch_size = 32 \n",
    "    num_workers = 8\n",
    "    qformer_lr = 1e-4\n",
    "    weight_decay = 0.05\n",
    "    patience = 2\n",
    "    factor = 0.8\n",
    "    epochs = 5\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_amp = True\n",
    "\n",
    "    # --- Image/Text Parameters ---\n",
    "    image_size = 224\n",
    "    max_length = 77\n",
    "\n",
    "    # --- Loss/Saving Parameters ---\n",
    "    temperature = 0.07\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"val_itc_acc\"\n",
    "    mode = \"max\"\n",
    "\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"--- ViBLIP Q-Former Training Configuration ---\")\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Base BLIP-2 Model: {config.blip2_model_name}\")\n",
    "print(f\"Text Tokenizer: {config.text_tokenizer_name}\")\n",
    "print(f\"Batch Size: {config.batch_size}\")\n",
    "print(f\"Use AMP: {config.use_amp}\")\n",
    "print(f\"Epochs: {config.epochs}\")\n",
    "print(f\"Q-Former LR: {config.qformer_lr}\")\n",
    "print(f\"Output Path: {config.model_path}\")\n",
    "print(f\"Data Path (JSONs): {os.path.abspath(config.data_path)}\")\n",
    "print(f\"Image Base Path: {os.path.abspath(config.image_path)}\")\n",
    "print(f\"---------------------------------------------\\n\")\n",
    "if config.data_path == \".\" and config.image_path == \".\":\n",
    "    print(\"WARNING: Using current directory for data and image paths. Ensure JSON files and images are present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Seeding for Reproducibility\n",
    "\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Metric Calculation Utilities\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        if torch.is_tensor(val): val = val.item()\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.name}: {self.avg:.4f}\"\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    n = similarity_matrix.shape[1-dim]\n",
    "    if n == 0 or k <= 0: return 0.0\n",
    "    effective_k = min(k, n)\n",
    "    correct_count = 0\n",
    "    top_k_indices = torch.topk(similarity_matrix, effective_k, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "\n",
    "    if dim == 0: # I2T\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]: correct_count += 1\n",
    "    elif dim == 1: # T2I\n",
    "        for txt_idx in range(n):\n",
    "            if ground_truth[txt_idx] in top_k_indices[txt_idx, :]: correct_count += 1\n",
    "    else: raise ValueError(\"dim must be 0 or 1\")\n",
    "    return correct_count / n\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    image_embeddings = image_embeddings.float()\n",
    "    text_embeddings = text_embeddings.float()\n",
    "\n",
    "    sim_matrix = text_embeddings @ image_embeddings.T\n",
    "    n = sim_matrix.shape[0]\n",
    "    default_metrics = {\n",
    "        \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "        \"avg_cosine_sim\": 0.0,\n",
    "        \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "        \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "    }\n",
    "    if n == 0: return default_metrics\n",
    "\n",
    "    try:\n",
    "        ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "        i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "        t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "        i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "        t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "        avg_acc = (i2t_acc + t2i_acc) / 2\n",
    "        diag_len = min(sim_matrix.shape[0], sim_matrix.shape[1])\n",
    "        avg_cosine_sim = torch.diagonal(sim_matrix[:diag_len, :diag_len]).mean().item()\n",
    "\n",
    "        i2t_recall = {}\n",
    "        t2i_recall = {}\n",
    "        for k in [1, 5, 10]:\n",
    "            k_str = f\"R@{k}\"\n",
    "            i2t_recall[k_str] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "            t2i_recall[k_str] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "\n",
    "        return {\n",
    "            \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "            \"avg_cosine_sim\": avg_cosine_sim,\n",
    "            \"i2t_recall\": i2t_recall, \"t2i_recall\": t2i_recall\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error during metric calculation: {e}\")\n",
    "        print(f\"Shapes: ImgEmb={image_embeddings.shape}, TxtEmb={text_embeddings.shape}, SimMtx={sim_matrix.shape}\")\n",
    "        return default_metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Dataset Class Definition\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, tokenizer, image_processor, max_length):\n",
    "        super().__init__()\n",
    "        print(f\"Loading data from: {os.path.abspath(json_path)}\")\n",
    "        self.data = []\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                self.data = json.load(f)\n",
    "            print(f\"Loaded {len(self.data)} samples from {os.path.basename(json_path)}.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: JSON file not found at {json_path}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error loading JSON: {e}\")\n",
    "\n",
    "        self.image_base_path = image_base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "        if not os.path.isdir(self.image_base_path):\n",
    "            print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data): raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path')\n",
    "        caption = item.get('caption', '')\n",
    "        image = None\n",
    "\n",
    "        if relative_image_path:\n",
    "            image_path = os.path.normpath(os.path.join(self.image_base_path, relative_image_path))\n",
    "            try:\n",
    "                img_pil = Image.open(image_path).convert('RGB')\n",
    "                image_processed = self.image_processor(images=img_pil, return_tensors=\"pt\")\n",
    "                image = image_processed['pixel_values'].squeeze(0)\n",
    "            except FileNotFoundError:\n",
    "                image = None\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {image_path}: {e}\")\n",
    "                image = None\n",
    "\n",
    "        if image is None:\n",
    "            c = 3\n",
    "            h = w = config.image_size\n",
    "            image = torch.zeros((c, h, w))\n",
    "\n",
    "        text_inputs = self.tokenizer(\n",
    "            caption, padding='max_length', truncation=True,\n",
    "            max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        input_ids = text_inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = text_inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        if input_ids.dim() == 0: input_ids = input_ids.unsqueeze(0)\n",
    "        if attention_mask.dim() == 0: attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"raw_caption\": caption\n",
    "        }\n",
    "\n",
    "print(\"ImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model Loading & Freezing\n",
    "\n",
    "model = None\n",
    "blip_config_loaded = None\n",
    "model_loaded = False\n",
    "\n",
    "try:\n",
    "    print(f\"Loading BLIP-2 configuration for: {config.blip2_model_name}\")\n",
    "    blip_config_loaded = Blip2Config.from_pretrained(config.blip2_model_name)\n",
    "\n",
    "    print(f\"Loading BLIP-2 model: {config.blip2_model_name}\")\n",
    "    model_dtype = torch.float16 if config.use_amp and config.device == torch.device('cuda') else torch.float32\n",
    "    model = Blip2Model.from_pretrained(\n",
    "        config.blip2_model_name,\n",
    "        config=blip_config_loaded,\n",
    "        torch_dtype=model_dtype\n",
    "    )\n",
    "\n",
    "    print(\"Freezing Vision Model and Language Model parameters...\")\n",
    "    frozen_params_count = 0\n",
    "    total_params = 0\n",
    "\n",
    "    if hasattr(model, 'vision_model'):\n",
    "        for param in model.vision_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            frozen_params_count += param.numel()\n",
    "        print(f\"  Vision model frozen.\")\n",
    "    else:\n",
    "        print(\"  Warning: model.vision_model not found.\")\n",
    "\n",
    "    if hasattr(model, 'language_model'):\n",
    "        for param in model.language_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            frozen_params_count += param.numel()\n",
    "        print(f\"  Language model frozen.\")\n",
    "    else:\n",
    "        print(\"  Warning: model.language_model not found.\")\n",
    "\n",
    "    trainable_params_count = 0\n",
    "    if hasattr(model, 'qformer'):\n",
    "        print(\"Verifying Q-Former parameters are trainable...\")\n",
    "        model.qformer.train()\n",
    "        for param in model.qformer.parameters():\n",
    "            param.requires_grad = True\n",
    "            trainable_params_count += param.numel()\n",
    "\n",
    "        proj_layers_found = 0\n",
    "        for proj_name in ['vision_proj', 'text_proj']:\n",
    "            if hasattr(model, proj_name):\n",
    "                layer = getattr(model, proj_name)\n",
    "                if layer is not None and isinstance(layer, nn.Module):\n",
    "                    print(f\"  Verifying {proj_name} parameters are trainable...\")\n",
    "                    layer.train()\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = True\n",
    "                    trainable_params_count += sum(p.numel() for p in layer.parameters())\n",
    "                    proj_layers_found += 1\n",
    "\n",
    "        if proj_layers_found == 0:\n",
    "            print(\"  Note: Projection layers (vision_proj, text_proj) not found.\")\n",
    "\n",
    "        model.to(config.device)\n",
    "        model_loaded = True\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(\"\\nModel components loaded successfully.\")\n",
    "        print(f\"  Total parameters: ~{total_params / 1e6:.2f} M\")\n",
    "        print(f\"  Frozen parameters: ~{frozen_params_count / 1e6:.2f} M\")\n",
    "        print(f\"  Trainable parameters: ~{trainable_params_count / 1e6:.2f} M\")\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: model.qformer not found!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading model '{config.blip2_model_name}': {e}\")\n",
    "    traceback.print_exc()\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Data Setup (Tokenizer, Image Processor, Datasets, DataLoaders)\n",
    "\n",
    "tokenizer = None\n",
    "image_processor = None\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "data_setup_ok = False\n",
    "\n",
    "if model_loaded:\n",
    "    try:\n",
    "        print(f\"Loading Tokenizer: {config.text_tokenizer_name}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer_name)\n",
    "        print(f\"  PhoBERT Tokenizer Vocab Size: {tokenizer.vocab_size}\")\n",
    "\n",
    "        print(f\"Loading Image Processor for: {config.blip2_model_name}\")\n",
    "        image_processor = BlipImageProcessor.from_pretrained(config.blip2_model_name)\n",
    "        if hasattr(image_processor, 'size'):\n",
    "            processor_size = image_processor.size['height'] if isinstance(image_processor.size, dict) else image_processor.size\n",
    "            if processor_size != config.image_size:\n",
    "                print(f\"  Updating config.image_size from {config.image_size} to {processor_size}\")\n",
    "                config.image_size = processor_size\n",
    "\n",
    "        print(\"\\nCreating datasets...\")\n",
    "        train_json = os.path.join(config.data_path, \"train.json\")\n",
    "        dev_json = os.path.join(config.data_path, \"dev.json\")\n",
    "\n",
    "        train_dataset = ImageCaptionDataset(\n",
    "            json_path=train_json, image_base_path=config.image_path,\n",
    "            tokenizer=tokenizer, image_processor=image_processor, max_length=config.max_length\n",
    "        )\n",
    "        dev_dataset = ImageCaptionDataset(\n",
    "            json_path=dev_json, image_base_path=config.image_path,\n",
    "            tokenizer=tokenizer, image_processor=image_processor, max_length=config.max_length\n",
    "        )\n",
    "\n",
    "        if not train_dataset.data: raise ValueError(\"Training data failed to load.\")\n",
    "        if not dev_dataset.data: print(\"Warning: Validation data not loaded.\")\n",
    "\n",
    "        print(\"\\nCreating dataloaders...\")\n",
    "        num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "        print(f\"Using {num_workers} workers.\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False, drop_last=True\n",
    "        )\n",
    "        print(f\"Train loader created ({len(train_loader)} batches).\")\n",
    "\n",
    "        if dev_dataset.data:\n",
    "            dev_loader = DataLoader(\n",
    "                dev_dataset, batch_size=config.batch_size, shuffle=False, num_workers=num_workers,\n",
    "                pin_memory=True if config.device == torch.device(\"cuda\") else False, drop_last=False\n",
    "            )\n",
    "            print(f\"Validation loader created ({len(dev_loader)} batches).\")\n",
    "\n",
    "        data_setup_ok = True\n",
    "        print(\"\\nData setup complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during data setup: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Skipping data setup because model failed to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Optimizer & Scheduler Setup\n",
    "\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "optimizer_setup_ok = False\n",
    "\n",
    "if model_loaded and data_setup_ok:\n",
    "    print(\"\\nSetting up optimizer and scheduler...\")\n",
    "    try:\n",
    "        trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "        param_count = sum(p.numel() for p in trainable_params)\n",
    "        print(f\"Found {len(trainable_params)} parameter tensors to optimize (~{param_count / 1e6:.2f} M).\")\n",
    "\n",
    "        if not trainable_params:\n",
    "            raise ValueError(\"No trainable parameters found.\")\n",
    "\n",
    "        optimizer = optim.AdamW(trainable_params, lr=config.qformer_lr, weight_decay=config.weight_decay)\n",
    "        print(f\"Optimizer AdamW initialized with lr={config.qformer_lr:.1e}\")\n",
    "\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode=config.mode, factor=config.factor, patience=config.patience, verbose=True\n",
    "        )\n",
    "        print(f\"LR Scheduler ReduceLROnPlateau initialized (mode='{config.mode}')\")\n",
    "        optimizer_setup_ok = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR setting up optimizer/scheduler: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Skipping optimizer setup due to previous errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Loss Function Definitions\n",
    "\n",
    "def calculate_itc_loss(image_feats_norm, text_feats_norm, temperature):\n",
    "    logits = (image_feats_norm @ text_feats_norm.T) / temperature\n",
    "    logits = logits.float()\n",
    "    batch_size = image_feats_norm.shape[0]\n",
    "    if batch_size == 0: return torch.tensor(0.0, device=logits.device)\n",
    "    labels = torch.arange(batch_size, device=logits.device)\n",
    "    loss_i = F.cross_entropy(logits, labels)\n",
    "    loss_t = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i + loss_t) / 2.0\n",
    "\n",
    "def calculate_itm_loss(model, outputs, batch_size, device):\n",
    "    \"\"\"Image-Text Matching Loss with Hard Negative Mining\"\"\"\n",
    "    if batch_size == 0 or not hasattr(model, 'qformer') or not hasattr(outputs, 'qformer_outputs'):\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    try:\n",
    "        # Extract Q-Former multimodal features (CLS token)\n",
    "        multimodal_feats = outputs.qformer_outputs.last_hidden_state[:, 0]  # [batch_size, hidden_size]\n",
    "\n",
    "        # Hard negative mining: Find mismatched pairs with high ITC similarity\n",
    "        image_feats = model.vision_proj(outputs.image_embeds) if hasattr(model, 'vision_proj') else outputs.image_embeds\n",
    "        text_feats = model.text_proj(outputs.text_embeds) if hasattr(model, 'text_proj') else outputs.text_embeds\n",
    "        image_feats_norm = F.normalize(image_feats, dim=-1)\n",
    "        text_feats_norm = F.normalize(text_feats, dim=-1)\n",
    "        sim_matrix = image_feats_norm @ text_feats_norm.T\n",
    "        sim_matrix.fill_diagonal_(-float('inf'))  # Exclude true pairs\n",
    "        hard_neg_indices = torch.argmax(sim_matrix, dim=1)  # [batch_size]\n",
    "\n",
    "        # Create negative pairs by pairing images with hard-negative texts\n",
    "        neg_input_ids = outputs.input_ids[hard_neg_indices]\n",
    "        neg_attention_mask = outputs.attention_mask[hard_neg_indices]\n",
    "        pixel_values = outputs.pixel_values\n",
    "\n",
    "        with torch.no_grad():\n",
    "            neg_outputs = model(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=neg_input_ids,\n",
    "                attention_mask=neg_attention_mask\n",
    "            )\n",
    "        neg_multimodal_feats = neg_outputs.qformer_outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        # Combine positive and negative features\n",
    "        all_feats = torch.cat([multimodal_feats, neg_multimodal_feats], dim=0)  # [2*batch_size, hidden_size]\n",
    "        itm_logits = model.itm_head(all_feats) if hasattr(model, 'itm_head') else nn.Linear(all_feats.size(-1), 2).to(device)(all_feats)\n",
    "\n",
    "        # Labels: 1 for positive pairs, 0 for negative pairs\n",
    "        itm_labels = torch.cat([torch.ones(batch_size, dtype=torch.long), torch.zeros(batch_size, dtype=torch.long)]).to(device)\n",
    "\n",
    "        return F.cross_entropy(itm_logits, itm_labels)\n",
    "\n",
    "def calculate_itg_loss(model_outputs, target_ids, target_mask):\n",
    "    return torch.tensor(0.0, device=target_ids.device)\n",
    "\n",
    "print(\"Loss functions defined: ITC and ITM implemented, ITG is placeholder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training Loop\n",
    "\n",
    "ready_to_train = model_loaded and optimizer_setup_ok and data_setup_ok and train_loader is not None\n",
    "\n",
    "if ready_to_train:\n",
    "    print(f\"\\n=============== Starting Q-Former Training ===============\")\n",
    "    print(f\"Epochs: {config.epochs}, Batch Size: {config.batch_size}, Device: {config.device}, AMP: {config.use_amp}\")\n",
    "    print(f\"Tracking metric: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    history = {'train_loss': [], 'train_itc_loss': [], 'train_itm_loss': [], 'validation_results': []}\n",
    "    start_train_time = time.time()\n",
    "    scaler = GradScaler(enabled=config.use_amp)\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss_meter = AvgMeter(f\"Train Total E{epoch+1}\")\n",
    "        train_itc_meter = AvgMeter(f\"Train ITC E{epoch+1}\")\n",
    "        train_itm_meter = AvgMeter(f\"Train ITM E{epoch+1}\")\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Training E{epoch+1}\", leave=True, unit=\"batch\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            pixel_values = batch['pixel_values'].to(config.device)\n",
    "            input_ids = batch['input_ids'].to(config.device)\n",
    "            attention_mask = batch['attention_mask'].to(config.device)\n",
    "            batch_size = pixel_values.size(0)\n",
    "            if batch_size == 0: continue\n",
    "\n",
    "            expected_dtype = torch.float16 if config.use_amp else torch.float32\n",
    "            pixel_values = pixel_values.to(dtype=expected_dtype)\n",
    "\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                try:\n",
    "                    outputs = model(\n",
    "                        pixel_values=pixel_values,\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "\n",
    "                    image_embeds = outputs.image_embeds\n",
    "                    text_embeds = outputs.text_embeds\n",
    "\n",
    "                    image_feats = model.vision_proj(image_embeds) if hasattr(model, 'vision_proj') else image_embeds\n",
    "                    text_feats = model.text_proj(text_embeds) if hasattr(model, 'text_proj') else text_embeds\n",
    "\n",
    "                    image_feats_norm = F.normalize(image_feats, dim=-1)\n",
    "                    text_feats_norm = F.normalize(text_feats, dim=-1)\n",
    "\n",
    "                    loss_itc = calculate_itc_loss(image_feats_norm, text_feats_norm, config.temperature)\n",
    "                    loss_itm = calculate_itm_loss(model, outputs, batch_size, config.device)\n",
    "                    loss_itg = torch.tensor(0.0, device=config.device)\n",
    "\n",
    "                    total_loss = loss_itc + loss_itm + loss_itg\n",
    "\n",
    "                except Exception as forward_err:\n",
    "                    print(f\"Error at step {step}: {forward_err}\")\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "\n",
    "            if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "                print(f\"Warning: NaN/Inf loss at step {step}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            scaler.scale(total_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss_meter.update(total_loss.item(), batch_size)\n",
    "            train_itc_meter.update(loss_itc.item(), batch_size)\n",
    "            train_itm_meter.update(loss_itm.item(), batch_size)\n",
    "            progress_bar.set_postfix(loss=f\"{train_loss_meter.avg:.4f}\", itc=f\"{train_itc_meter.avg:.4f}\", itm=f\"{train_itm_meter.avg:.4f}\")\n",
    "\n",
    "        history['train_loss'].append(train_loss_meter.avg)\n",
    "        history['train_itc_loss'].append(train_itc_meter.avg)\n",
    "        history['train_itm_loss'].append(train_itm_meter.avg)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss_meter.avg:.4f} (ITC={train_itc_meter.avg:.4f}, ITM={train_itm_meter.avg:.4f})\")\n",
    "\n",
    "        val_results = None\n",
    "        current_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "\n",
    "        if dev_loader:\n",
    "            val_results = validate_qformer_epoch(model, dev_loader, config.device, epoch+1)\n",
    "            history['validation_results'].append(val_results)\n",
    "            current_val_metric = val_results.get(config.metric_to_track, current_val_metric)\n",
    "\n",
    "            try:\n",
    "                metric_for_scheduler = val_results.get(config.metric_to_track, val_results.get('loss', float('inf')))\n",
    "                lr_scheduler.step(metric_for_scheduler)\n",
    "                current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "                print(f\"  Validation Metrics: {val_results}\")\n",
    "                print(f\"  Current LR(s): {[f'{lr:.2e}' for lr in current_lrs]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error stepping scheduler: {e}\")\n",
    "        else:\n",
    "            history['validation_results'].append(None)\n",
    "\n",
    "        is_best = False\n",
    "        if dev_loader:\n",
    "            if config.mode == \"max\" and current_val_metric > best_val_metric: is_best = True\n",
    "            elif config.mode == \"min\" and current_val_metric < best_val_metric: is_best = True\n",
    "            if is_best: best_val_metric = current_val_metric\n",
    "\n",
    "        model.cpu()\n",
    "        save_dict = {\n",
    "            'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'train_loss': train_loss_meter.avg, 'validation_results': val_results,\n",
    "            'best_val_metric': best_val_metric, 'metric_tracked': config.metric_to_track,\n",
    "            'config_blip2_model_name': config.blip2_model_name, 'config_text_tokenizer_name': config.text_tokenizer_name\n",
    "        }\n",
    "        model.to(config.device)\n",
    "\n",
    "        try:\n",
    "            if config.save_best_only and dev_loader:\n",
    "                if is_best:\n",
    "                    best_ckpt_path = os.path.join(config.model_path, \"ViBLIP_QFormer_best.pt\")\n",
    "                    torch.save(save_dict, best_ckpt_path)\n",
    "                    print(f\"  Saved Best Model (Epoch {epoch+1}, {config.metric_to_track}={current_val_metric:.4f})\")\n",
    "            else:\n",
    "                epoch_ckpt_path = os.path.join(config.model_path, f\"ViBLIP_QFormer_epoch_{epoch+1}.pt\")\n",
    "                torch.save(save_dict, epoch_ckpt_path)\n",
    "                print(f\"  Saved Epoch {epoch+1} Checkpoint\")\n",
    "                if is_best and dev_loader:\n",
    "                    best_ckpt_path = os.path.join(config.model_path, \"ViBLIP_QFormer_best.pt\")\n",
    "                    torch.save(save_dict, best_ckpt_path)\n",
    "                    print(f\"  (Also marked as best model)\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR saving checkpoint for epoch {epoch+1}: {e}\")\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {epoch_end_time - epoch_start_time:.2f} seconds ---\")\n",
    "\n",
    "    end_train_time = time.time()\n",
    "    print(f\"\\n=============== Q-Former Training Finished ===============\")\n",
    "    print(f\"Total Training Time: {(end_train_time - start_train_time)/60:.2f} minutes\")\n",
    "\n",
    "    try:\n",
    "        final_model_path = os.path.join(config.model_path, 'ViBLIP_QFormer_final_epoch.pt')\n",
    "        model.cpu()\n",
    "        final_save_dict = {\n",
    "            'epoch': config.epochs, 'model_state_dict': model.state_dict(),\n",
    "            'best_val_metric': best_val_metric, 'metric_tracked': config.metric_to_track\n",
    "        }\n",
    "        torch.save(final_save_dict, final_model_path)\n",
    "        print(f\"Final epoch model state saved to {final_model_path}\")\n",
    "        best_ckpt_path = os.path.join(config.model_path, \"ViBLIP_QFormer_best.pt\")\n",
    "        if os.path.exists(best_ckpt_path):\n",
    "            print(f\"Best model saved to: {best_ckpt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving final model state: {e}\")\n",
    "    print(f\"========================================================\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Validation Loop Implementation\n",
    "\n",
    "def validate_qformer_epoch(model, dataloader, device, epoch_num):\n",
    "    print(f\"--- Running Validation Epoch {epoch_num} ---\")\n",
    "    model.eval()\n",
    "    val_loss_meter = AvgMeter(f\"Val Total E{epoch_num}\")\n",
    "    val_itc_meter = AvgMeter(f\"Val ITC E{epoch_num}\")\n",
    "    val_itm_meter = AvgMeter(f\"Val ITM E{epoch_num}\")\n",
    "    val_itc_acc_meter = AvgMeter(f\"Val ITC Acc E{epoch_num}\")\n",
    "    val_itm_acc_meter = AvgMeter(f\"Val ITM Acc E{epoch_num}\")\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Validating E{epoch_num}\", leave=True, unit=\"batch\")\n",
    "\n",
    "    all_image_feats = []\n",
    "    all_text_feats = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = pixel_values.size(0)\n",
    "            if batch_size == 0: continue\n",
    "\n",
    "            expected_dtype = torch.float16 if config.use_amp else torch.float32\n",
    "            pixel_values = pixel_values.to(dtype=expected_dtype)\n",
    "\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                try:\n",
    "                    outputs = model(\n",
    "                        pixel_values=pixel_values,\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "\n",
    "                    image_embeds = outputs.image_embeds\n",
    "                    text_embeds = outputs.text_embeds\n",
    "\n",
    "                    image_feats = model.vision_proj(image_embeds) if hasattr(model, 'vision_proj') else image_embeds\n",
    "                    text_feats = model.text_proj(text_embeds) if hasattr(model, 'text_proj') else text_embeds\n",
    "\n",
    "                    image_feats_norm = F.normalize(image_feats, dim=-1)\n",
    "                    text_feats_norm = F.normalize(text_feats, dim=-1)\n",
    "\n",
    "                    loss_itc = calculate_itc_loss(image_feats_norm, text_feats_norm, config.temperature)\n",
    "                    loss_itm = calculate_itm_loss(model, outputs, batch_size, device)\n",
    "                    total_loss = loss_itc + loss_itm\n",
    "\n",
    "                    all_image_feats.append(image_feats_norm)\n",
    "                    all_text_feats.append(text_feats_norm)\n",
    "\n",
    "                    metrics = compute_metrics(image_feats_norm, text_feats_norm)\n",
    "                    itc_acc = metrics['avg_acc']\n",
    "\n",
    "                    multimodal_feats = outputs.qformer_outputs.last_hidden_state[:, 0]\n",
    "                    itm_logits = model.itm_head(multimodal_feats) if hasattr(model, 'itm_head') else nn.Linear(multimodal_feats.size(-1), 2).to(device)(multimodal_feats)\n",
    "                    itm_preds = torch.argmax(itm_logits, dim=-1)\n",
    "                    itm_labels = torch.ones(batch_size, dtype=torch.long).to(device)\n",
    "                    itm_acc = (itm_preds == itm_labels).float().mean().item()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in validation batch: {e}\")\n",
    "                    continue\n",
    "\n",
    "                val_loss_meter.update(total_loss.item(), batch_size)\n",
    "                val_itc_meter.update(loss_itc.item(), batch_size)\n",
    "                val_itm_meter.update(loss_itm.item(), batch_size)\n",
    "                val_itc_acc_meter.update(itc_acc, batch_size)\n",
    "                val_itm_acc_meter.update(itm_acc, batch_size)\n",
    "\n",
    "                progress_bar.set_postfix(loss=f\"{val_loss_meter.avg:.4f}\", itc_acc=f\"{val_itc_acc_meter.avg:.4f}\", itm_acc=f\"{val_itm_acc_meter.avg:.4f}\")\n",
    "\n",
    "    all_image_feats = torch.cat(all_image_feats, dim=0)\n",
    "    all_text_feats = torch.cat(all_text_feats, dim=0)\n",
    "    final_metrics = compute_metrics(all_image_feats, all_text_feats)\n",
    "\n",
    "    results = {\n",
    "        'loss': val_loss_meter.avg,\n",
    "        'val_itc_acc': val_itc_acc_meter.avg,\n",
    "        'val_itm_acc': val_itm_acc_meter.avg,\n",
    "        'val_itc_loss': val_itc_meter.avg,\n",
    "        'val_itm_loss': val_itm_meter.avg,\n",
    "        'i2t_recall': final_metrics['i2t_recall'],\n",
    "        't2i_recall': final_metrics['t2i_recall']\n",
    "    }\n",
    "\n",
    "    print(f\"Validation Epoch {epoch_num} Results: Loss={results['loss']:.4f}, ITC Acc={results['val_itc_acc']:.4f}, ITM Acc={results['val_itm_acc']:.4f}\")\n",
    "    print(f\"  I2T Recall: {results['i2t_recall']}\")\n",
    "    print(f\"  T2I Recall: {results['t2i_recall']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"Validation function implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Test Set Evaluation\n",
    "\n",
    "print(\"\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "evaluation_performed = False\n",
    "model_loaded_for_test = False\n",
    "\n",
    "if not (model_loaded and data_setup_ok):\n",
    "    print(\"Skipping test evaluation: Model or data setup failed.\")\n",
    "elif not os.path.exists(test_json_path):\n",
    "    print(f\"Skipping test evaluation: Test JSON not found ({test_json_path}).\")\n",
    "else:\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    if 'tokenizer' in globals() and 'image_processor' in globals():\n",
    "        test_dataset = ImageCaptionDataset(\n",
    "            json_path=test_json_path, image_base_path=config.image_path,\n",
    "            tokenizer=tokenizer, image_processor=image_processor, max_length=config.max_length\n",
    "        )\n",
    "\n",
    "        if test_dataset.data:\n",
    "            num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=num_workers,\n",
    "                pin_memory=True if config.device == torch.device(\"cuda\") else False, drop_last=False\n",
    "            )\n",
    "            print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "\n",
    "            model_to_test = None\n",
    "            try:\n",
    "                blip_config_test = Blip2Config.from_pretrained(config.blip2_model_name)\n",
    "                model_to_test = Blip2Model.from_pretrained(\n",
    "                    config.blip2_model_name, config=blip_config_test,\n",
    "                    torch_dtype=torch.float16 if config.use_amp else torch.float32\n",
    "                )\n",
    "                for param in model_to_test.vision_model.parameters(): param.requires_grad = False\n",
    "                if hasattr(model_to_test, 'language_model'):\n",
    "                    for param in model_to_test.language_model.parameters(): param.requires_grad = False\n",
    "                print(\"Model structure for testing created.\")\n",
    "\n",
    "                best_model_path = os.path.join(config.model_path, \"ViBLIP_QFormer_best.pt\")\n",
    "                if os.path.exists(best_model_path):\n",
    "                    print(f\"Loading best model weights from: {best_model_path}\")\n",
    "                    checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "                    state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "                    if next(iter(state_dict)).startswith('module.'):\n",
    "                        from collections import OrderedDict\n",
    "                        state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "\n",
    "                    load_result = model_to_test.load_state_dict(state_dict, strict=False)\n",
    "                    print(f\"Load Result: {load_result}\")\n",
    "                    model_to_test.to(config.device)\n",
    "                    model_loaded_for_test = True\n",
    "                    print(\"Loaded trained weights into model structure.\")\n",
    "\n",
    "                    print(\"\\nRunning evaluation on test set...\")\n",
    "                    test_results = validate_qformer_epoch(model_to_test, test_loader, config.device, \"Test\")\n",
    "                    evaluation_performed = True\n",
    "                    print(\"\\n--- Test Set Results ---\")\n",
    "                    metric_log_str = f\"  Loss: {test_results['loss']:.4f}\\n\"\n",
    "                    metric_log_str += f\"  ITC Acc: {test_results['val_itc_acc']:.4f}\\n\"\n",
    "                    metric_log_str += f\"  ITM Acc: {test_results['val_itm_acc']:.4f}\\n\"\n",
    "                    metric_log_str += f\"  I2T Recall: {test_results['i2t_recall']}\\n\"\n",
    "                    metric_log_str += f\"  T2I Recall: {test_results['t2i_recall']}\\n\"\n",
    "                    print(metric_log_str.strip())\n",
    "                    print(\"------------------------\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"ERROR: Best model checkpoint not found at {best_model_path}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during test setup or evaluation: {e}\")\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(\"Could not load test data. Skipping test evaluation.\")\n",
    "    else:\n",
    "        print(\"Skipping test evaluation: Tokenizer or Image Processor not available.\")\n",
    "\n",
    "if not evaluation_performed:\n",
    "    print(\"Test set evaluation was not performed.\")\n",
    "\n",
    "print(\"\\n================= Evaluation Finished =================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
