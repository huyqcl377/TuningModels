{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "Transformers Version: 4.50.0\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Import BLIP specific components + AutoModel/Tokenizer\n",
    "from transformers import BlipVisionModel, BlipConfig, BlipImageProcessor\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time # For timing epochs\n",
    "import transformers\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name()}\")\n",
    "    # torch.cuda.set_device()  # Set CUDA device to '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model output path: ./trained_models/ViBLIP_uitopenviic\n",
      "Selected Vision Source: Salesforce/blip-image-captioning-base\n",
      "Selected Text Model: vinai/phobert-base\n",
      "Image base path (for resolving paths in JSON): /root/TuningModels/data/UIT-OpenViIC-dataset\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Configuration Class (CFG) === MODIFIED for BASE Models ===\n",
    "class CFG:\n",
    "    # --- Paths ---\\\n",
    "    data_path = \"./refined_json/\"\n",
    "    image_path = \"./data/UIT-OpenViIC-dataset/\"\n",
    "\n",
    "    # Output directory for saved models\n",
    "    model_path = \"./trained_models/ViBLIP_uitopenviic\"\n",
    "\n",
    "    # --- Model Selection ---\n",
    "    # Source for BLIP Vision Model components - Changed to BASE version\n",
    "    selected_vision_source = \"Salesforce/blip-image-captioning-base\" \n",
    "    # Vietnamese Text Model - Changed to BASE version\n",
    "    selected_text_model = \"vinai/phobert-base\" # <<< CHANGED\n",
    "    text_tokenizer_name = selected_text_model # Use PhoBERT's tokenizer (base/large often share tokenizer type)\n",
    "\n",
    "    # --- Model parameters ---\n",
    "    blip_vision_model_name = selected_vision_source\n",
    "    blip_image_processor_name = selected_vision_source # Processor usually matches the main checkpoint\n",
    "\n",
    "    @property\n",
    "    def text_embedding(self): # PhoBERT-Base output dim\n",
    "        return 768 # <<< CHANGED (from 1024)\n",
    "    @property\n",
    "    def vision_embedding(self): # BLIP-Base (ViT-B) output dim\n",
    "        # Check config if unsure: BlipConfig.from_pretrained(self.selected_vision_source).vision_config.hidden_size\n",
    "        return 768 # <<< CHANGED (from 1024)\n",
    "\n",
    "    projection_dim = 256 # Shared latent space dimension (Can keep this or adjust if desired)\n",
    "\n",
    "    # --- Training parameters ---\n",
    "    seed = 42\n",
    "    # Consider increasing batch_size if GPU memory allows with smaller models\n",
    "    batch_size = 16  # <<< INCREASED (Example - adjust based on your GPU memory)\n",
    "    num_workers = 20\n",
    "    # Learning rates (These might need tuning for base models, but start similarly)\n",
    "    projection_lr = 5e-5\n",
    "    vision_encoder_lr = 1e-5 # Base models might tolerate slightly higher LR, but 1e-5 is also safe start\n",
    "    text_encoder_lr = 2e-5   # Base models might tolerate slightly higher LR, but 1e-5 is also safe start\n",
    "    weight_decay = 1e-3\n",
    "    patience = 3\n",
    "    factor = 0.8\n",
    "    epochs = 20 # Keep or adjust based on convergence speed\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_amp = False # Keep disabled for simplicity\n",
    "\n",
    "    # --- Image/Text parameters ---\n",
    "    max_length = 256 # PhoBERT's default max length (PhoBERT base also typically handles 256)\n",
    "\n",
    "    # --- Loss/Saving parameters ---\n",
    "    temperature = 0.07\n",
    "    learnable_temperature = False\n",
    "    save_best_only = True\n",
    "    # Track a relevant metric (adjust if needed, e.g., 'i2t recall R@1')\n",
    "    metric_to_track = \"avg_acc\"\n",
    "    mode = \"max\"\n",
    "    early_stopping_patience = 3\n",
    "    early_stopping_min_delta = 0.001\n",
    "    # Gradient accumulation (adjust if batch_size is still limited)\n",
    "    accumulation_steps = 1\n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True) # Creates the new directory if needed\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Selected Vision Source: {config.selected_vision_source}\")\n",
    "print(f\"Selected Text Model: {config.selected_text_model}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: Seeding for Reproducibility ===\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # torch.backends.cudnn.deterministic = True\n",
    "        # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Metric Calculation Utilities ===\n",
    "\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        # Ensure val is a scalar number before adding to sum\n",
    "        if torch.is_tensor(val):\n",
    "             val = val.item() # Convert tensor to Python number\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "        # else:\n",
    "            # Optionally print a warning if the value is not usable\n",
    "            # print(f\"Warning: Cannot update AvgMeter '{self.name}' with value type {type(val)}\")\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    n = similarity_matrix.shape[1-dim]\n",
    "    correct_count = 0\n",
    "    top_k_indices = torch.topk(similarity_matrix, k, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "\n",
    "    if dim == 0: # I2T\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]:\n",
    "                correct_count += 1\n",
    "    elif dim == 1: # T2I\n",
    "        for txt_idx in range(n):\n",
    "             if ground_truth[txt_idx] in top_k_indices[txt_idx, :]:\n",
    "                correct_count += 1\n",
    "    else: raise ValueError(\"dim must be 0 or 1\")\n",
    "    return correct_count / n if n > 0 else 0.0\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    sim_matrix = text_embeddings @ image_embeddings.T\n",
    "    sim_matrix = sim_matrix.float() # Ensure float for calculations\n",
    "    n = sim_matrix.shape[0]\n",
    "    if n == 0:\n",
    "        # Return default zero metrics for empty batch\n",
    "        return {\n",
    "            \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "            \"avg_cosine_sim\": 0.0,\n",
    "            \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "        }\n",
    "\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "    i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "    t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "    t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "    avg_acc = (i2t_acc + t2i_acc) / 2\n",
    "    avg_cosine_sim = torch.diagonal(sim_matrix).mean().item()\n",
    "\n",
    "    i2t_recall = {}\n",
    "    t2i_recall = {}\n",
    "    recall_k_values = [k for k in [1, 5, 10] if k <= n]\n",
    "    for k in recall_k_values:\n",
    "        i2t_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "        t2i_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "\n",
    "    # Ensure all keys R@1, R@5, R@10 exist even if k>n\n",
    "    for k in [1, 5, 10]:\n",
    "        k_str = f\"R@{k}\"\n",
    "        if k_str not in i2t_recall: i2t_recall[k_str] = 0.0\n",
    "        if k_str not in t2i_recall: t2i_recall[k_str] = 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "        \"avg_cosine_sim\": avg_cosine_sim,\n",
    "        \"i2t_recall\": i2t_recall, \"t2i_recall\": t2i_recall\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomImageCaptionDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Dataset Class Definition (Separate Tokenizer/Processor) === MODIFIED ===\n",
    "class CustomImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, tokenizer, image_processor, max_length): # Changed arguments\n",
    "        super().__init__()\n",
    "        print(f\"Attempting to load data from: {os.path.abspath(json_path)}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f: self.data = json.load(f)\n",
    "        except Exception as e: print(f\"ERROR loading JSON {json_path}: {e}\"); self.data = []\n",
    "\n",
    "        print(f\"Found {len(self.data)} samples in {os.path.basename(json_path)}.\")\n",
    "        self.image_base_path = image_base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor # Store image processor\n",
    "        self.max_length = max_length\n",
    "        try: # Get image size\n",
    "             # BlipImageProcessor uses 'size' dictionary with 'shortest_edge' or 'height'/'width'\n",
    "             if isinstance(image_processor.size, dict):\n",
    "                 self.img_size = image_processor.size.get('shortest_edge', image_processor.size.get('height', 224))\n",
    "             else: # Older versions might just have an int/tuple\n",
    "                 self.img_size = image_processor.size\n",
    "                 if isinstance(self.img_size, (tuple, list)): self.img_size = self.img_size[0]\n",
    "        except AttributeError:\n",
    "             print(\"Warning: Could not determine image size from processor, defaulting to 224.\")\n",
    "             self.img_size = 224\n",
    "        print(f\"Using image size: {self.img_size}x{self.img_size}\")\n",
    "        if not os.path.isdir(self.image_base_path): print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data): raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path')\n",
    "        # Expecting a list of captions, take the first one\n",
    "        captions = item.get('caption', [])\n",
    "        caption = captions[0] if captions else \"\"\n",
    "\n",
    "        # Load Image\n",
    "        image = None\n",
    "        pixel_values = torch.zeros((3, self.img_size, self.img_size)) # Dummy tensor\n",
    "        if relative_image_path:\n",
    "            image_path = os.path.normpath(os.path.join(self.image_base_path, relative_image_path))\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "            except FileNotFoundError: print(f\"Warning: Img not found: {image_path}. Using dummy for idx {idx}.\")\n",
    "            except Exception as e: print(f\"Warning: Error loading image {image_path}: {e}. Using dummy for idx {idx}.\")\n",
    "        else: print(f\"Warning: Missing 'image_path' for idx {idx}. Using dummy.\")\n",
    "        if image is None: image = Image.new('RGB', (self.img_size, self.img_size))\n",
    "\n",
    "        # Process Image using image_processor\n",
    "        try:\n",
    "            image_inputs = self.image_processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = image_inputs['pixel_values'].squeeze(0) # Remove batch dim\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image idx {idx}: {e}\")\n",
    "            # pixel_values remains the dummy tensor initialized earlier\n",
    "\n",
    "        # Process Text using tokenizer\n",
    "        try:\n",
    "            text_inputs = self.tokenizer(\n",
    "                caption, padding='max_length', truncation=True,\n",
    "                max_length=self.max_length, return_tensors='pt'\n",
    "            )\n",
    "            input_ids = text_inputs['input_ids'].squeeze(0) # Remove batch dim\n",
    "            attention_mask = text_inputs['attention_mask'].squeeze(0) # Remove batch dim\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text '{caption}' idx {idx}: {e}\")\n",
    "            input_ids = torch.zeros(self.max_length, dtype=torch.long)\n",
    "            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "print(\"CustomImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom BLIP+PhoBERT Model components (with corrected vision loading v2) and loss function defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Model Definition (PhoBERT + BLIP Vision) === CORRECTED LOADING v2 ===\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# --- Import BlipForImageTextRetrieval ---\n",
    "from transformers import BlipForImageTextRetrieval, BlipVisionModel, BlipConfig\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig # Text parts (PhoBERT)\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Encodes images using BLIP's Vision Model (Base). - CORRECTED LOADING v2\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        # --- Try loading BlipForImageTextRetrieval first ---\n",
    "        print(f\"Initializing BLIP Vision Encoder from: {config_train.blip_vision_model_name} by loading BlipForImageTextRetrieval first.\")\n",
    "\n",
    "        if pretrained:\n",
    "            try:\n",
    "                # Load a task-specific model like BlipForImageTextRetrieval\n",
    "                print(\"  Loading base BlipForImageTextRetrieval...\")\n",
    "                # Set low_cpu_mem_usage=True to potentially reduce RAM spike\n",
    "                # ignore_mismatched_sizes=True can sometimes help if there are minor mismatches\n",
    "                # between config and checkpoint, though often not needed for base extraction.\n",
    "                full_blip_model = BlipForImageTextRetrieval.from_pretrained(\n",
    "                    config_train.blip_vision_model_name,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    # ignore_mismatched_sizes=True # Uncomment if needed\n",
    "                 )\n",
    "                print(\"  Extracting vision_model from BlipForImageTextRetrieval.\")\n",
    "                # Extract the vision model part\n",
    "                self.vision_model = full_blip_model.vision_model\n",
    "                # Release the reference to the larger model\n",
    "                del full_blip_model\n",
    "                print(\"  Vision model extracted successfully.\")\n",
    "                # Optional garbage collection\n",
    "                # import gc\n",
    "                # gc.collect()\n",
    "                # if torch.cuda.is_available():\n",
    "                #     torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR loading BlipForImageTextRetrieval or extracting vision model: {e}\")\n",
    "                print(\"  Falling back to initializing BlipVisionModel directly (might show warnings).\")\n",
    "                # Fallback to original method if the above fails\n",
    "                self.vision_model = BlipVisionModel.from_pretrained(config_train.blip_vision_model_name)\n",
    "        else:\n",
    "             print(\"  Initializing BlipVisionModel from scratch (as pretrained=False).\")\n",
    "             blip_vision_config = BlipConfig.from_pretrained(config_train.blip_vision_model_name).vision_config\n",
    "             self.vision_model = BlipVisionModel(blip_vision_config)\n",
    "        # --- END MODIFIED LOADING ---\n",
    "\n",
    "        # Get the hidden size directly from the loaded model's config\n",
    "        try:\n",
    "            self.input_features = self.vision_model.config.hidden_size\n",
    "        except AttributeError as e:\n",
    "             print(f\"  ERROR accessing vision_model.config.hidden_size: {e}. Attempting config_train value.\")\n",
    "             self.input_features = config_train.vision_embedding # Fallback\n",
    "\n",
    "        if hasattr(config_train, 'vision_embedding') and self.input_features != config_train.vision_embedding:\n",
    "             print(f\"  WARNING: Configured vision_embedding ({config_train.vision_embedding}) doesn't match loaded model hidden size ({self.input_features}). Using loaded size.\")\n",
    "        else:\n",
    "             print(f\"  Confirmed/Using vision model hidden size: {self.input_features}\")\n",
    "\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        pixel_values = pixel_values.to(self.vision_model.device)\n",
    "        vision_outputs = self.vision_model(pixel_values=pixel_values, return_dict=True)\n",
    "        image_features = vision_outputs.pooler_output\n",
    "        projected_features = self.projection(image_features)\n",
    "        projected_features = F.normalize(projected_features, p=2, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "# --- TextEncoder remains the same ---\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Encodes text using PhoBERT-Base.\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        print(f\"Initializing Text Encoder: {config_train.selected_text_model}\")\n",
    "\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config_train.selected_text_model)\n",
    "        else:\n",
    "            model_config = AutoConfig.from_pretrained(config_train.selected_text_model)\n",
    "            self.model = AutoModel.from_config(model_config)\n",
    "\n",
    "        self.input_features = config_train.text_embedding\n",
    "        actual_hidden_size = self.model.config.hidden_size\n",
    "        if actual_hidden_size != self.input_features:\n",
    "             print(f\"  WARNING: Configured text_embedding ({self.input_features}) does not match PhoBERT hidden size ({actual_hidden_size}). Using actual size.\")\n",
    "             self.input_features = actual_hidden_size\n",
    "        else:\n",
    "            print(f\"  Confirmed text model hidden size: {self.input_features}\")\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        input_ids = input_ids.to(self.model.device)\n",
    "        attention_mask = attention_mask.to(self.model.device)\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        text_features = outputs.last_hidden_state[:, 0, :]\n",
    "        projected_features = self.projection(text_features)\n",
    "        projected_features = F.normalize(projected_features, p=2, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "# --- CustomBlipPhobertModel remains the same ---\n",
    "class CustomBlipPhobertModel(nn.Module):\n",
    "    \"\"\"Combines BLIP Vision encoder and PhoBERT Text encoder for contrastive retrieval.\"\"\"\n",
    "    def __init__(self, image_encoder, text_encoder, config_train):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.config_train = config_train\n",
    "\n",
    "        if config_train.learnable_temperature:\n",
    "            init_val = torch.ones([]) * np.log(1 / config_train.temperature)\n",
    "            self.logit_scale = nn.Parameter(init_val)\n",
    "            print(f\"Using learnable temperature, initialized to {self.logit_scale.exp().item():.4f}\")\n",
    "        else:\n",
    "            # Ensure buffer is created on the correct device initially if possible\n",
    "            # Although .to(device) on the model should handle it later\n",
    "            temp_tensor = torch.tensor(np.log(1 / config_train.temperature))\n",
    "            self.register_buffer('logit_scale', temp_tensor)\n",
    "            print(f\"Using fixed temperature: {config_train.temperature}\")\n",
    "\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        image_features = self.image_encoder(pixel_values)\n",
    "        text_features = self.text_encoder(input_ids, attention_mask)\n",
    "\n",
    "        if isinstance(self.logit_scale, nn.Parameter):\n",
    "             current_logit_scale = self.logit_scale.to(image_features.device).exp()\n",
    "        else:\n",
    "             current_logit_scale = self.logit_scale.exp().to(image_features.device)\n",
    "\n",
    "        logits_per_image = current_logit_scale.float() * image_features.float() @ text_features.float().t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text, image_features, text_features\n",
    "\n",
    "# --- Loss Function (Contrastive Loss) remains the same ---\n",
    "def contrastive_loss(logits_per_image, logits_per_text):\n",
    "    logits_per_image = logits_per_image.float()\n",
    "    logits_per_text = logits_per_text.float()\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    if batch_size == 0: return torch.tensor(0.0, device=logits_per_image.device, requires_grad=True)\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "    loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "    total_loss = (loss_img + loss_txt) / 2.0\n",
    "    return total_loss\n",
    "\n",
    "print(\"Custom BLIP+PhoBERT Model components (with corrected vision loading v2) and loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation epoch functions defined (No AMP).\n"
     ]
    }
   ],
   "source": [
    "# === Cell 7: Training and Validation Epoch Functions (No AMP/Scaler) ===\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch_num):\n",
    "    model.train()\n",
    "    loss_meter = AvgMeter(f\"Train Loss E{epoch_num}\")\n",
    "    try: from tqdm.notebook import tqdm as pbar\n",
    "    except ImportError: from tqdm import tqdm as pbar\n",
    "    progress_bar = pbar(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_size = pixel_values.size(0)\n",
    "        if batch_size == 0: continue\n",
    "\n",
    "        logits_per_image, logits_per_text, _, _ = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "        # Ensure logits are float32 for loss\n",
    "        loss = contrastive_loss(logits_per_image.float(), logits_per_text.float())\n",
    "        loss = loss / config.accumulation_steps # Normalize loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % config.accumulation_steps == 0 or (i + 1) == len(dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_meter.update(loss.item() * config.accumulation_steps, batch_size) # Log un-normalized loss\n",
    "        progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    optimizer.zero_grad() # Clean up at end\n",
    "    return loss_meter.avg\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, device, epoch_num):\n",
    "    model.eval()\n",
    "    loss_meter = AvgMeter(f\"Val Loss E{epoch_num}\")\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    try: from tqdm.notebook import tqdm as pbar\n",
    "    except ImportError: from tqdm import tqdm as pbar\n",
    "    progress_bar = pbar(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = pixel_values.size(0)\n",
    "            if batch_size == 0: continue\n",
    "\n",
    "            logits_per_image, logits_per_text, image_embeds, text_embeds = model(pixel_values, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(logits_per_image.float(), logits_per_text.float()) # Ensure FP32\n",
    "\n",
    "            loss_meter.update(loss.item(), batch_size)\n",
    "            all_image_embeddings.append(image_embeds.cpu())\n",
    "            all_text_embeddings.append(text_embeds.cpu())\n",
    "            progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    if not all_image_embeddings or not all_text_embeddings:\n",
    "         print(\"Warning: No embeddings collected during validation.\")\n",
    "         zero_metrics = { \"loss\": loss_meter.avg, \"avg acc\": 0.0, \"avg cosine sim\": 0.0,\n",
    "                           \"i2t recall R@1\": 0.0, \"i2t recall R@5\": 0.0, \"i2t recall R@10\": 0.0,\n",
    "                           \"t2i recall R@1\": 0.0, \"t2i recall R@5\": 0.0, \"t2i recall R@10\": 0.0,\n",
    "                           \"avg R@1\": 0.0, \"avg R@5\": 0.0, \"avg R@10\": 0.0 }\n",
    "         return zero_metrics\n",
    "\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "\n",
    "    print(f\"\\nComputing metrics over {all_image_embeddings.shape[0]} validation samples...\")\n",
    "    validation_metrics = compute_metrics(all_image_embeddings.to(device), all_text_embeddings.to(device))\n",
    "\n",
    "    final_results = {\"loss\": loss_meter.avg}\n",
    "    for k, v in validation_metrics.items():\n",
    "        if isinstance(v, dict): # Handle recall dicts\n",
    "            for recall_k, recall_v in v.items(): final_results[f\"{k.replace('_', ' ')} {recall_k}\"] = recall_v\n",
    "        else: final_results[k.replace('_', ' ')] = v\n",
    "    return final_results\n",
    "\n",
    "print(\"Training and Validation epoch functions defined (No AMP).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer: vinai/phobert-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhoBERT Tokenizer loaded successfully.\n",
      "Loading Image Processor from: Salesforce/blip-image-captioning-base\n",
      "BLIP Image Processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8: Setup - Tokenizer and Image Processor === MODIFIED ===\n",
    "from transformers import AutoTokenizer, BlipImageProcessor\n",
    "\n",
    "tokenizer = None\n",
    "image_processor = None\n",
    "\n",
    "print(f\"Loading Tokenizer: {config.text_tokenizer_name}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer_name)\n",
    "    print(\"PhoBERT Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading tokenizer '{config.text_tokenizer_name}': {e}\")\n",
    "\n",
    "print(f\"Loading Image Processor from: {config.blip_image_processor_name}\")\n",
    "try:\n",
    "    # Use BlipImageProcessor associated with the vision model source\n",
    "    image_processor = BlipImageProcessor.from_pretrained(config.blip_image_processor_name)\n",
    "    print(\"BLIP Image Processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading image processor '{config.blip_image_processor_name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Attempting to load data from: /root/TuningModels/refined_json/train.json\n",
      "Found 31390 samples in train.json.\n",
      "Using image size: 384x384\n",
      "Attempting to load data from: /root/TuningModels/refined_json/dev.json\n",
      "Found 7266 samples in dev.json.\n",
      "Using image size: 384x384\n",
      "\n",
      "Creating dataloaders...\n",
      "Using 20 workers for DataLoaders.\n",
      "Train loader created with 1961 batches.\n",
      "Validation loader created with 455 batches.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 9: Setup - Datasets and DataLoaders === MODIFIED ===\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "\n",
    "if tokenizer and image_processor: # Check if both loaded\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_json = os.path.join(config.data_path, \"train.json\")\n",
    "    dev_json = os.path.join(config.data_path, \"dev.json\")\n",
    "\n",
    "    train_dataset = CustomImageCaptionDataset( # Use the custom dataset class\n",
    "        json_path=train_json, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, image_processor=image_processor, # Pass separate components\n",
    "        max_length=config.max_length\n",
    "    )\n",
    "    dev_dataset = CustomImageCaptionDataset( # Use the custom dataset class\n",
    "        json_path=dev_json, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, image_processor=image_processor, # Pass separate components\n",
    "        max_length=config.max_length\n",
    "    )\n",
    "\n",
    "    if not train_dataset.data: print(\"\\nERROR: Failed to load training data.\")\n",
    "    if not dev_dataset.data: print(\"\\nWARNING: Failed to load validation data.\")\n",
    "\n",
    "    print(\"\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    if train_dataset.data:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=True\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "    else: print(\"Skipping train loader creation.\")\n",
    "\n",
    "    if dev_dataset.data:\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "    else: print(\"Skipping validation loader creation.\")\n",
    "\n",
    "    if not train_loader: print(\"\\nERROR: Train loader could not be created.\")\n",
    "else:\n",
    "     print(\"ERROR: Tokenizer or Image Processor not loaded. Skipping dataset/loader creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model components...\n",
      "Initializing BLIP Vision Encoder from: Salesforce/blip-image-captioning-base by loading BlipForImageTextRetrieval first.\n",
      "  Loading base BlipForImageTextRetrieval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipForImageTextRetrieval were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['itm_head.bias', 'itm_head.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.word_embeddings.weight', 'text_proj.bias', 'text_proj.weight', 'vision_proj.bias', 'vision_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracting vision_model from BlipForImageTextRetrieval.\n",
      "  Vision model extracted successfully.\n",
      "  Confirmed/Using vision model hidden size: 768\n",
      "  Added projection head: 768 -> 256\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Confirmed text model hidden size: 768\n",
      "  Added projection head: 768 -> 256\n",
      "Using fixed temperature: 0.07\n",
      "\n",
      "CustomBlipPhobertModel initialized successfully on cuda.\n",
      "Total parameters: 221.48 M\n",
      "Trainable parameters: 221.48 M\n",
      "\n",
      "Setting up optimizer...\n",
      "  Param counts (Trainable): VisionBase=150, VisionHead=1, TextBase=199, TextHead=1, LogitScale=0\n",
      "Optimizer AdamW initialized.\n",
      "LR Scheduler ReduceLROnPlateau initialized (mode='max', factor=0.8, patience=3)\n",
      "Early stopping initialized (patience=3, min_delta=0.001)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10: Setup - Model, Optimizer, Scheduler === MODIFIED ===\n",
    "model = None\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "\n",
    "print(\"\\nInitializing model components...\")\n",
    "try:\n",
    "    # Instantiate the modified encoders and model wrapper\n",
    "    image_encoder = ImageEncoder(config).to(config.device) # Uses BlipVisionModel\n",
    "    text_encoder = TextEncoder(config).to(config.device)   # Uses PhoBERT-Large\n",
    "    model = CustomBlipPhobertModel(image_encoder, text_encoder, config).to(config.device)\n",
    "    print(f\"\\nCustomBlipPhobertModel initialized successfully on {config.device}.\")\n",
    "    num_params_total = sum(p.numel() for p in model.parameters())\n",
    "    num_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {num_params_total / 1e6:.2f} M\")\n",
    "    print(f\"Trainable parameters: {num_params_trainable / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing model components: {e}\")\n",
    "    model = None\n",
    "\n",
    "if model:\n",
    "    print(\"\\nSetting up optimizer...\")\n",
    "    # Get parameters from the specific model components\n",
    "    image_encoder_base_params = [p for p in model.image_encoder.vision_model.parameters() if p.requires_grad]\n",
    "    image_head_params = [p for p in model.image_encoder.projection.parameters() if p.requires_grad]\n",
    "    text_encoder_base_params = [p for p in model.text_encoder.model.parameters() if p.requires_grad] # PhoBERT base\n",
    "    text_head_params = [p for p in model.text_encoder.projection.parameters() if p.requires_grad]\n",
    "    logit_scale_param = [model.logit_scale] if isinstance(model.logit_scale, nn.Parameter) else []\n",
    "\n",
    "    print(f\"  Param counts (Trainable): VisionBase={len(image_encoder_base_params)}, VisionHead={len(image_head_params)}, TextBase={len(text_encoder_base_params)}, TextHead={len(text_head_params)}, LogitScale={len(logit_scale_param)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": image_encoder_base_params, \"lr\": config.vision_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": image_head_params, \"lr\": config.projection_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": text_encoder_base_params, \"lr\": config.text_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": text_head_params, \"lr\": config.projection_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": logit_scale_param, \"lr\": config.projection_lr, \"weight_decay\": 0}\n",
    "    ]\n",
    "\n",
    "    optimizer_grouped_parameters = [g for g in optimizer_grouped_parameters if g['params']]\n",
    "\n",
    "    if not optimizer_grouped_parameters:\n",
    "         print(\"ERROR: No trainable parameters found for the optimizer.\")\n",
    "    else:\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "        print(f\"Optimizer AdamW initialized.\")\n",
    "\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode=config.mode, factor=config.factor, patience=config.patience\n",
    "        )\n",
    "        print(f\"LR Scheduler ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.factor}, patience={config.patience})\")\n",
    "\n",
    "        early_stopping_counter = 0 # Renamed from config.early_stopping_counter\n",
    "        print(f\"Early stopping initialized (patience={config.early_stopping_patience}, min_delta={config.early_stopping_min_delta})\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not initialized. Skipping optimizer/scheduler setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 20 epochs...\n",
      "Tracking metric: 'avg_acc' (mode: max)\n",
      "\n",
      "--- Epoch 1/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d66245b8ce489c82fce3daec5a5419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E1:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.7287\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b5d8e96c6d44af8b953b2dbba5b7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E1:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0629 | avg cosine sim: 0.5312 | i2t acc: 0.0751 | i2t recall R@1: 0.0751 | i2t recall R@5: 0.2466 | i2t recall R@10: 0.3519 | loss: 0.4315 | t2i acc: 0.0506 | t2i recall R@1: 0.0506 | t2i recall R@5: 0.2256 | t2i recall R@10: 0.3346\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from -inf to 0.0629\n",
      "  Saved Best Model (Epoch 1) to ./trained_models/ViBLIP_uitopenviic/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 1 Time: 623.44 seconds ---\n",
      "\n",
      "--- Epoch 2/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11665d21eae41a3baaddeb92536e3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E2:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.3506\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c39fdc1f14c49acb94acf2992c69e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E2:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0796 | avg cosine sim: 0.5547 | i2t acc: 0.0940 | i2t recall R@1: 0.0940 | i2t recall R@5: 0.3054 | i2t recall R@10: 0.4297 | loss: 0.3480 | t2i acc: 0.0652 | t2i recall R@1: 0.0652 | t2i recall R@5: 0.2883 | t2i recall R@10: 0.4177\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.0629 to 0.0796\n",
      "  Saved Best Model (Epoch 2) to ./trained_models/ViBLIP_uitopenviic/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 2 Time: 622.91 seconds ---\n",
      "\n",
      "--- Epoch 3/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f88fd7a87994e9290a73d56cbe4de65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E3:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.2431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19b7d92c0b848f9b4423de634e99184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E3:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0809 | avg cosine sim: 0.5685 | i2t acc: 0.0914 | i2t recall R@1: 0.0914 | i2t recall R@5: 0.2977 | i2t recall R@10: 0.4178 | loss: 0.3556 | t2i acc: 0.0705 | t2i recall R@1: 0.0705 | t2i recall R@5: 0.3011 | t2i recall R@10: 0.4243\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.0796 to 0.0809\n",
      "  Saved Best Model (Epoch 3) to ./trained_models/ViBLIP_uitopenviic/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 3 Time: 622.12 seconds ---\n",
      "\n",
      "--- Epoch 4/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a6255865f24223baabcab9a8da92b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E4:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.1844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5671a752254f93aa2fde4a215a5045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E4:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0886 | avg cosine sim: 0.5727 | i2t acc: 0.1021 | i2t recall R@1: 0.1021 | i2t recall R@5: 0.3293 | i2t recall R@10: 0.4529 | loss: 0.3315 | t2i acc: 0.0751 | t2i recall R@1: 0.0751 | t2i recall R@5: 0.3204 | t2i recall R@10: 0.4454\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.0809 to 0.0886\n",
      "  Saved Best Model (Epoch 4) to ./trained_models/ViBLIP_uitopenviic/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 4 Time: 623.50 seconds ---\n",
      "\n",
      "--- Epoch 5/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd686898c02445fa4c6777a313febe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E5:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.1465\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffa96ce5bb04233aac4992361e97541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E5:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0898 | avg cosine sim: 0.5790 | i2t acc: 0.0996 | i2t recall R@1: 0.0996 | i2t recall R@5: 0.3340 | i2t recall R@10: 0.4561 | loss: 0.3239 | t2i acc: 0.0800 | t2i recall R@1: 0.0800 | t2i recall R@5: 0.3287 | t2i recall R@10: 0.4503\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.0886 to 0.0898\n",
      "  Saved Best Model (Epoch 5) to ./trained_models/ViBLIP_uitopenviic/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 5 Time: 623.10 seconds ---\n",
      "\n",
      "--- Epoch 6/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f4cab12dc04caeaf82d374b19c7794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E6:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.1287\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cb6704e7d54e419fcf463a112da554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E6:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0906 | avg cosine sim: 0.5818 | i2t acc: 0.1054 | i2t recall R@1: 0.1054 | i2t recall R@5: 0.3332 | i2t recall R@10: 0.4550 | loss: 0.3175 | t2i acc: 0.0758 | t2i recall R@1: 0.0758 | t2i recall R@5: 0.3293 | t2i recall R@10: 0.4546\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0898. Counter: 1/3\n",
      "--- Epoch 6 Time: 621.42 seconds ---\n",
      "\n",
      "--- Epoch 7/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9386456cbf4a84a5a8c03dbd283a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E7:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n",
      "Epoch 7: Train Loss = 0.1111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb945391476413b8ded46606c74b596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E7:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0965 | avg cosine sim: 0.5883 | i2t acc: 0.1115 | i2t recall R@1: 0.1115 | i2t recall R@5: 0.3404 | i2t recall R@10: 0.4683 | loss: 0.3084 | t2i acc: 0.0815 | t2i recall R@1: 0.0815 | t2i recall R@5: 0.3355 | t2i recall R@10: 0.4576\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.0898 to 0.0965\n",
      "  Saved Best Model (Epoch 7) to ./trained_models/ViBLIP_uitopenviic/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 7 Time: 623.39 seconds ---\n",
      "\n",
      "--- Epoch 8/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57858c142b1745f7a64a2bcc6b9c22db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E8:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.0978\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e1f226040f434f85c7d58af8d13a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E8:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0949 | avg cosine sim: 0.5885 | i2t acc: 0.1096 | i2t recall R@1: 0.1096 | i2t recall R@5: 0.3442 | i2t recall R@10: 0.4780 | loss: 0.3144 | t2i acc: 0.0802 | t2i recall R@1: 0.0802 | t2i recall R@5: 0.3369 | t2i recall R@10: 0.4637\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0965. Counter: 1/3\n",
      "--- Epoch 8 Time: 620.80 seconds ---\n",
      "\n",
      "--- Epoch 9/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411d69551ef44852b1865f2e63c97f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E9:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.0878\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a401192f4b427b9a372f6a091e5817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E9:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0968 | avg cosine sim: 0.5885 | i2t acc: 0.1108 | i2t recall R@1: 0.1108 | i2t recall R@5: 0.3450 | i2t recall R@10: 0.4694 | loss: 0.3281 | t2i acc: 0.0829 | t2i recall R@1: 0.0829 | t2i recall R@5: 0.3428 | t2i recall R@10: 0.4697\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0965. Counter: 2/3\n",
      "--- Epoch 9 Time: 620.30 seconds ---\n",
      "\n",
      "--- Epoch 10/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f31b96cb144c8286c1bdcec7bbdffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E10:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.0797\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024a7f23d30f4c608a6871dd917578b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E10:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0989 | avg cosine sim: 0.5921 | i2t acc: 0.1113 | i2t recall R@1: 0.1113 | i2t recall R@5: 0.3456 | i2t recall R@10: 0.4751 | loss: 0.3137 | t2i acc: 0.0864 | t2i recall R@1: 0.0864 | t2i recall R@5: 0.3534 | t2i recall R@10: 0.4809\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.0965 to 0.0989\n",
      "  Saved Best Model (Epoch 10) to ./trained_models/ViBLIP_uitopenviic/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 10 Time: 621.61 seconds ---\n",
      "\n",
      "--- Epoch 11/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2540c08885040e7964d9b9e519819ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E11:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 0.0720\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9832c6d7c5a4f648bb16521811fd3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E11:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.1011 | avg cosine sim: 0.5942 | i2t acc: 0.1152 | i2t recall R@1: 0.1152 | i2t recall R@5: 0.3606 | i2t recall R@10: 0.4913 | loss: 0.3111 | t2i acc: 0.0870 | t2i recall R@1: 0.0870 | t2i recall R@5: 0.3644 | t2i recall R@10: 0.4938\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.0989 to 0.1011\n",
      "  Saved Best Model (Epoch 11) to ./trained_models/ViBLIP_uitopenviic/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 11 Time: 622.80 seconds ---\n",
      "\n",
      "--- Epoch 12/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c61e2080c9c4df89dfc002c49cd2311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E12:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 0.0686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b8e110ebc644a0b088bf22755c8553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E12:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0914 | avg cosine sim: 0.5719 | i2t acc: 0.1027 | i2t recall R@1: 0.1027 | i2t recall R@5: 0.3325 | i2t recall R@10: 0.4566 | loss: 0.3363 | t2i acc: 0.0801 | t2i recall R@1: 0.0801 | t2i recall R@5: 0.3269 | t2i recall R@10: 0.4568\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.1011. Counter: 1/3\n",
      "--- Epoch 12 Time: 621.80 seconds ---\n",
      "\n",
      "--- Epoch 13/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1e0e3bed48402f93851cb9d9b0495c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E13:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 0.0628\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ada84d647364157a0f7405f6133c4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E13:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0976 | avg cosine sim: 0.5862 | i2t acc: 0.1094 | i2t recall R@1: 0.1094 | i2t recall R@5: 0.3584 | i2t recall R@10: 0.4869 | loss: 0.3161 | t2i acc: 0.0859 | t2i recall R@1: 0.0859 | t2i recall R@5: 0.3470 | t2i recall R@10: 0.4776\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.1011. Counter: 2/3\n",
      "--- Epoch 13 Time: 621.19 seconds ---\n",
      "\n",
      "--- Epoch 14/20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3cad60b5a448858dc971a3e8ed043e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E14:   0%|          | 0/1961 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 25930.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 26501.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000007169.jpg: image file is truncated (1 bytes not processed). Using dummy for idx 1691.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 0.0570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f93b527fe44f69b563797193e1bfa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E14:   0%|          | 0/455 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/root/TuningModels/.venv/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 7266 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.0984 | avg cosine sim: 0.5785 | i2t acc: 0.1120 | i2t recall R@1: 0.1120 | i2t recall R@5: 0.3532 | i2t recall R@10: 0.4850 | loss: 0.3215 | t2i acc: 0.0848 | t2i recall R@1: 0.0848 | t2i recall R@5: 0.3497 | t2i recall R@10: 0.4806\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.1011. Counter: 3/3\n",
      "--- Epoch 14 Time: 621.62 seconds ---\n",
      "\n",
      "Early stopping triggered after 3 epochs without improvement.\n",
      "\n",
      "=============== Training Finished ================\n",
      "Total Training Time: 8709.99 seconds (145.17 minutes)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:626] . unexpected pos 607429888 vs 607429776",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/TuningModels/.venv/lib/python3.10/site-packages/torch/serialization.py:944\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 944\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/TuningModels/.venv/lib/python3.10/site-packages/torch/serialization.py:1216\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/152: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Update final model path name\u001b[39;00m\n\u001b[1;32m    101\u001b[0m final_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mmodel_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphobert_blip_retrieval_final_epoch.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal epoch model state saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m best_model_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mmodel_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphobert_blip_retrieval_best.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/TuningModels/.venv/lib/python3.10/site-packages/torch/serialization.py:943\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    940\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 943\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    944\u001b[0m         _save(\n\u001b[1;32m    945\u001b[0m             obj,\n\u001b[1;32m    946\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    949\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    950\u001b[0m         )\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/TuningModels/.venv/lib/python3.10/site-packages/torch/serialization.py:784\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:626] . unexpected pos 607429888 vs 607429776"
     ]
    }
   ],
   "source": [
    "# === Cell 11: Training Loop (No AMP) === MODIFIED ===\n",
    "if model and train_loader and optimizer and lr_scheduler:\n",
    "    print(f\"\\nStarting training for {config.epochs} epochs...\")\n",
    "    print(f\"Tracking metric: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    history = {'train_loss': [], 'validation_results': []}\n",
    "    start_train_time = time.time()\n",
    "    no_improve_epochs = 0 # Use the variable defined in Cell 10\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "\n",
    "        # --- Training ---\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, config.device, epoch+1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_results = {\"loss\": float('inf'), config.metric_to_track.replace('_', ' '): (-float('inf') if config.mode == 'max' else float('inf'))}\n",
    "        if dev_loader:\n",
    "            val_results = validate_epoch(model, dev_loader, config.device, epoch+1)\n",
    "            history['validation_results'].append(val_results)\n",
    "            print(\"  Validation Metrics:\")\n",
    "            metric_log_str = \"  \"\n",
    "            sorted_keys = sorted(val_results.keys(), key=lambda x: (x.split()[0], int(x.split('@')[-1]) if '@' in x else -1))\n",
    "            for name in sorted_keys: metric_log_str += f\"{name}: {val_results[name]:.4f} | \"\n",
    "            print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "            # --- Scheduler Step ---\n",
    "            current_val_metric_for_scheduler = val_results.get(config.metric_to_track.replace('_', ' '), None)\n",
    "            if current_val_metric_for_scheduler is not None:\n",
    "                lr_scheduler.step(current_val_metric_for_scheduler)\n",
    "                current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "                # Updated LR string for this specific setup\n",
    "                lr_str = f\"  Current LRs: VisionBase={current_lrs[0]:.2e}, VisionHead={current_lrs[1]:.2e}, TextBase={current_lrs[2]:.2e}, TextHead={current_lrs[3]:.2e}\"\n",
    "                if len(current_lrs) > 4: lr_str += f\", LogitScale={current_lrs[4]:.2e}\"\n",
    "                print(lr_str)\n",
    "            else: print(f\"  Warning: Metric '{config.metric_to_track}' not found. Scheduler not stepped.\")\n",
    "        else:\n",
    "            print(\"  Validation skipped (no dev_loader).\")\n",
    "            history['validation_results'].append(None)\n",
    "\n",
    "        # --- Save Checkpoint & Early Stopping Logic ---\n",
    "        current_val_metric = val_results.get(config.metric_to_track.replace('_', ' '), -float('inf') if config.mode == \"max\" else float('inf'))\n",
    "        is_best = False\n",
    "        if dev_loader:\n",
    "            if config.mode == \"max\":\n",
    "                if current_val_metric > best_val_metric + config.early_stopping_min_delta: is_best = True\n",
    "            else: # min mode\n",
    "                if current_val_metric < best_val_metric - config.early_stopping_min_delta: is_best = True\n",
    "\n",
    "            if is_best:\n",
    "                print(f\"  Metric '{config.metric_to_track}' improved from {best_val_metric:.4f} to {current_val_metric:.4f}\")\n",
    "                best_val_metric = current_val_metric; no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                print(f\"  Metric '{config.metric_to_track}' did not improve. Best: {best_val_metric:.4f}. Counter: {no_improve_epochs}/{config.early_stopping_patience}\")\n",
    "\n",
    "        # Update checkpoint names\n",
    "        best_checkpoint_path = os.path.join(config.model_path, \"phobert_blip_retrieval_best.pt\")\n",
    "        final_epoch_path = os.path.join(config.model_path, f\"phobert_blip_retrieval_epoch_{epoch+1}.pt\")\n",
    "\n",
    "        # Save necessary configs and state dicts\n",
    "        save_dict = {\n",
    "            'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'train_loss': train_loss, 'validation_results': val_results,\n",
    "            'best_val_metric': best_val_metric, 'metric_tracked': config.metric_to_track,\n",
    "            # Save relevant configs for reloading this specific architecture\n",
    "            'vision_model_name': config.blip_vision_model_name,\n",
    "            'text_model_name': config.selected_text_model,\n",
    "            'projection_dim': config.projection_dim,\n",
    "            'learnable_temperature': config.learnable_temperature\n",
    "        }\n",
    "\n",
    "        if config.save_best_only and dev_loader:\n",
    "            if is_best:\n",
    "                torch.save(save_dict, best_checkpoint_path)\n",
    "                print(f\"  Saved Best Model (Epoch {epoch+1}) to {best_checkpoint_path}\")\n",
    "        else:\n",
    "            torch.save(save_dict, final_epoch_path)\n",
    "            print(f\"  Saved Epoch {epoch+1} Checkpoint to {final_epoch_path}\")\n",
    "            if is_best and dev_loader:\n",
    "                 torch.save(save_dict, best_checkpoint_path)\n",
    "                 print(f\"  (Also saved as best model)\")\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {epoch_end_time - epoch_start_time:.2f} seconds ---\")\n",
    "\n",
    "        if dev_loader and no_improve_epochs >= config.early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {config.early_stopping_patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    end_train_time = time.time()\n",
    "    total_train_time = end_train_time - start_train_time\n",
    "    print(f\"\\n=============== Training Finished ================\")\n",
    "    print(f\"Total Training Time: {total_train_time:.2f} seconds ({total_train_time/60:.2f} minutes)\")\n",
    "    # Update final model path name\n",
    "    final_model_path = os.path.join(config.model_path, 'phobert_blip_retrieval_final_epoch.pt')\n",
    "    torch.save(save_dict, final_model_path)\n",
    "    print(f\"Final epoch model state saved to {final_model_path}\")\n",
    "    best_model_file = os.path.join(config.model_path, \"phobert_blip_retrieval_best.pt\")\n",
    "    if dev_loader and os.path.exists(best_model_file):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) is saved at: {best_model_file}\")\n",
    "    elif dev_loader: print(\"Best model checkpoint file not found.\")\n",
    "    print(f\"=================================================\")\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training (model, dataloader, optimizer, scheduler) not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Starting Test Set Evaluation ===============\n",
      "Loading test data from: ./refined_json/test.json\n",
      "Attempting to load data from: /root/TuningModels/refined_json/test.json\n",
      "Found 8093 samples in test.json.\n",
      "Using image size: 384x384\n",
      "Test loader created with 506 batches.\n",
      "\n",
      "Loading best model: ./trained_models/ViBLIP_uitopenviic/phobert_blip_retrieval_best.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-creating model structure for testing...\n",
      "  Using Vision Source: Salesforce/blip-image-captioning-base\n",
      "  Using Text Model: vinai/phobert-base\n",
      "Initializing BLIP Vision Encoder from: Salesforce/blip-image-captioning-base by loading BlipForImageTextRetrieval first.\n",
      "  Loading base BlipForImageTextRetrieval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipForImageTextRetrieval were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['itm_head.bias', 'itm_head.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.word_embeddings.weight', 'text_proj.bias', 'text_proj.weight', 'vision_proj.bias', 'vision_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracting vision_model from BlipForImageTextRetrieval.\n",
      "  Vision model extracted successfully.\n",
      "  Confirmed/Using vision model hidden size: 768\n",
      "  Added projection head: 768 -> 256\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Confirmed text model hidden size: 768\n",
      "  Added projection head: 768 -> 256\n",
      "Using fixed temperature: 0.07\n",
      "  State dict loading result: <All keys matched successfully>\n",
      "Model weights loaded successfully.\n",
      "\n",
      "Running evaluation on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716a7682ef7b4565bb7c1306071536ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation ETest:   0%|          | 0/506 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000001010.jpg: image file is truncated (155 bytes not processed). Using dummy for idx 706.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000000768.jpg: image file is truncated (18 bytes not processed). Using dummy for idx 771.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000005095.jpg: image file is truncated (76 bytes not processed). Using dummy for idx 2663.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000001010.jpg: image file is truncated (155 bytes not processed). Using dummy for idx 3020.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000000768.jpg: image file is truncated (18 bytes not processed). Using dummy for idx 3301.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000005095.jpg: image file is truncated (76 bytes not processed). Using dummy for idx 4862.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000001010.jpg: image file is truncated (155 bytes not processed). Using dummy for idx 5580.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000001010.jpg: image file is truncated (155 bytes not processed). Using dummy for idx 5706.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000000768.jpg: image file is truncated (18 bytes not processed). Using dummy for idx 5802.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000000768.jpg: image file is truncated (18 bytes not processed). Using dummy for idx 6406.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000005095.jpg: image file is truncated (76 bytes not processed). Using dummy for idx 6410.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000000768.jpg: image file is truncated (18 bytes not processed). Using dummy for idx 6903.\n",
      "Warning: Error loading image data/UIT-OpenViIC-dataset/images/00000005095.jpg: image file is truncated (76 bytes not processed). Using dummy for idx 7670.\n",
      "\n",
      "Computing metrics over 8093 validation samples...\n",
      "\n",
      "--- Test Set Results ---\n",
      "avg acc: 0.0893\n",
      "  avg cosine sim: 0.5858\n",
      "  i2t acc: 0.1042\n",
      "  i2t recall R@1: 0.1042\n",
      "  i2t recall R@5: 0.3273\n",
      "  i2t recall R@10: 0.4513\n",
      "  loss: 0.3445\n",
      "  t2i acc: 0.0744\n",
      "  t2i recall R@1: 0.0744\n",
      "  t2i recall R@5: 0.3271\n",
      "  t2i recall R@10: 0.4536\n",
      "------------------------\n",
      "\n",
      "================= Evaluation Finished ==================\n"
     ]
    }
   ],
   "source": [
    "# === Cell 12: Final Evaluation on Test Set === MODIFIED ===\n",
    "import traceback\n",
    "from types import SimpleNamespace\n",
    "from collections import OrderedDict\n",
    "\n",
    "print(\"\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_loader = None\n",
    "model_to_test = None\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "\n",
    "# 1. Check prerequisites\n",
    "if os.path.exists(test_json_path) and 'tokenizer' in globals() and tokenizer and 'image_processor' in globals() and image_processor:\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    try:\n",
    "        test_dataset = CustomImageCaptionDataset( # Use the custom dataset\n",
    "            json_path=test_json_path, image_base_path=config.image_path,\n",
    "            tokenizer=tokenizer, image_processor=image_processor, # Pass separate components\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "        if test_dataset.data:\n",
    "            num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "                num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "        else: print(\"Test dataset loaded but is empty.\")\n",
    "    except Exception as e: print(f\"Error creating test dataset/loader: {e}\")\n",
    "else: print(\"Skipping test evaluation: Test JSON, Tokenizer or Image Processor not found/loaded.\")\n",
    "\n",
    "# 2. Load Model for Testing if test_loader was created\n",
    "if test_loader:\n",
    "    try:\n",
    "        # Checkpoint names match the ones saved in Cell 11\n",
    "        best_model_path = os.path.join(config.model_path, \"phobert_blip_retrieval_best.pt\")\n",
    "        final_model_path = os.path.join(config.model_path, \"phobert_blip_retrieval_final_epoch.pt\")\n",
    "        load_path = None\n",
    "        if os.path.exists(best_model_path): load_path = best_model_path; print(f\"\\nLoading best model: {load_path}\")\n",
    "        elif os.path.exists(final_model_path): load_path = final_model_path; print(f\"\\nLoading final model: {load_path}\")\n",
    "        else: print(f\"\\nWARNING: No checkpoints found in {config.model_path}.\")\n",
    "\n",
    "        if load_path:\n",
    "            checkpoint = torch.load(load_path, map_location=config.device)\n",
    "            print(\"Re-creating model structure for testing...\")\n",
    "\n",
    "            # --- >>> CORRECTED temp_config_dict CREATION <<< ---\n",
    "            # Use attribute names expected by the Encoder __init__ methods\n",
    "            temp_config_dict = {\n",
    "                # For ImageEncoder\n",
    "                'blip_vision_model_name': checkpoint.get('vision_model_name', config.selected_vision_source),\n",
    "                'vision_embedding': config.vision_embedding, # Get from current config or save/load if needed\n",
    "                # For TextEncoder\n",
    "                'selected_text_model': checkpoint.get('text_model_name', config.selected_text_model),\n",
    "                'text_embedding': config.text_embedding, # Get from current config or save/load if needed\n",
    "                # For BlipRetrievalModel / Loss\n",
    "                'projection_dim': checkpoint.get('projection_dim', config.projection_dim),\n",
    "                'learnable_temperature': checkpoint.get('learnable_temperature', config.learnable_temperature),\n",
    "                'temperature': config.temperature # Use current config temp if not saved\n",
    "            }\n",
    "            # Create a namespace object from the dictionary\n",
    "            temp_config = SimpleNamespace(**temp_config_dict)\n",
    "            # --- >>> END CORRECTION <<< ---\n",
    "\n",
    "            print(f\"  Using Vision Source: {temp_config.blip_vision_model_name}\")\n",
    "            print(f\"  Using Text Model: {temp_config.selected_text_model}\")\n",
    "\n",
    "            # Instantiate using the temporary config object\n",
    "            test_image_encoder = ImageEncoder(temp_config).to(config.device)\n",
    "            test_text_encoder = TextEncoder(temp_config).to(config.device)\n",
    "            model_to_test = CustomBlipPhobertModel(test_image_encoder, test_text_encoder, temp_config).to(config.device)\n",
    "\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "                print(\"Detected 'module.' prefix, removing.\")\n",
    "                state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "\n",
    "            load_result = model_to_test.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"  State dict loading result: {load_result}\")\n",
    "            if load_result.missing_keys: print(f\"  Warning: Missing keys: {load_result.missing_keys}\")\n",
    "            if load_result.unexpected_keys: print(f\"  Warning: Unexpected keys: {load_result.unexpected_keys}\")\n",
    "            print(f\"Model weights loaded successfully.\")\n",
    "\n",
    "            print(\"\\nRunning evaluation on test set...\")\n",
    "            test_results = validate_epoch(model_to_test, test_loader, config.device, epoch_num=\"Test\")\n",
    "\n",
    "            print(\"\\n--- Test Set Results ---\")\n",
    "            metric_log_str = \"\"\n",
    "            sorted_keys = sorted(test_results.keys(), key=lambda x: (x.split()[0], int(x.split('@')[-1]) if '@' in x else -1))\n",
    "            for name in sorted_keys: metric_log_str += f\"  {name}: {test_results[name]:.4f}\\n\"\n",
    "            print(metric_log_str.strip())\n",
    "            print(\"------------------------\")\n",
    "        else: print(\"Evaluation skipped (no weights found).\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR during test setup/evaluation: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n================= Evaluation Finished ==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot directory ensured at: /root/TuningModels/train_plot/ViBLIP_uitopenviic\n",
      "Saved loss plot to: train_plot/ViBLIP_uitopenviic/training_loss.png\n",
      "Saved accuracy plot to: train_plot/ViBLIP_uitopenviic/training_accuracy.png\n",
      "Saved i2t_recall plot to: train_plot/ViBLIP_uitopenviic/training_i2t_recall.png\n",
      "Saved t2i_recall plot to: train_plot/ViBLIP_uitopenviic/training_t2i_recall.png\n",
      "Saved combined plot to: train_plot/ViBLIP_uitopenviic/training_metrics_combined.png\n"
     ]
    }
   ],
   "source": [
    "# === Cell 13: Training Visualization (Adapted) ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plot_dir = \"train_plot/ViBLIP_uitopenviic\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "print(f\"Plot directory ensured at: {os.path.abspath(plot_dir)}\")\n",
    "\n",
    "def save_subplot_as_figure(subplot, save_path):\n",
    "    fig_new = plt.figure(figsize=(8, 6))\n",
    "    ax_new = fig_new.add_subplot(111)\n",
    "    lines = subplot.get_lines()\n",
    "    if not lines: print(f\"Warning: No lines in subplot for {save_path}\"); plt.close(fig_new); return\n",
    "    labels = [line.get_label() for line in lines]\n",
    "    for line in lines:\n",
    "        ax_new.plot(line.get_xdata(), line.get_ydata(), color=line.get_color(),\n",
    "                    linestyle=line.get_linestyle(), marker=line.get_marker(), label=line.get_label())\n",
    "    ax_new.set_title(subplot.get_title()); ax_new.set_xlabel(subplot.get_xlabel()); ax_new.set_ylabel(subplot.get_ylabel())\n",
    "    ax_new.grid(True)\n",
    "    if any(label and not label.startswith('_') for label in labels): ax_new.legend()\n",
    "    plt.tight_layout()\n",
    "    fig_new.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig_new)\n",
    "\n",
    "def plot_training_metrics(history):\n",
    "    if not history or not history.get('train_loss') or not history.get('validation_results'):\n",
    "        print(\"No/incomplete training history available.\"); return\n",
    "    valid_results = [res for res in history['validation_results'] if res is not None]\n",
    "    if not valid_results:\n",
    "        print(\"No valid validation results. Plotting only training loss.\")\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "        ax.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')\n",
    "        ax.set_title('Training Loss over Epochs'); ax.set_xlabel('Epoch'); ax.set_ylabel('Loss'); ax.legend(); ax.grid(True)\n",
    "        save_path = os.path.join(plot_dir, 'training_loss.png')\n",
    "        fig.savefig(save_path, bbox_inches='tight', dpi=300); print(f\"Saved loss plot to: {save_path}\"); plt.close(fig)\n",
    "        return\n",
    "\n",
    "    num_epochs_trained = len(history['train_loss']); num_epochs_validated = len(valid_results)\n",
    "    epochs_train = range(1, num_epochs_trained + 1); epochs_val = range(1, num_epochs_validated + 1)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 13))\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16, y=1.02)\n",
    "\n",
    "    val_loss = [res.get('loss', float('nan')) for res in valid_results]\n",
    "    axes[0, 0].plot(epochs_train, history['train_loss'], 'b-o', label='Training Loss')\n",
    "    axes[0, 0].plot(epochs_val, val_loss, 'r-s', label='Validation Loss')\n",
    "    axes[0, 0].set_title('Loss'); axes[0, 0].set_xlabel('Epoch'); axes[0, 0].set_ylabel('Loss'); axes[0, 0].legend(); axes[0, 0].grid(True)\n",
    "\n",
    "    metric_key_acc = 'avg acc'\n",
    "    if metric_key_acc in valid_results[0]:\n",
    "        val_acc = [res[metric_key_acc] for res in valid_results]\n",
    "        axes[0, 1].plot(epochs_val, val_acc, 'g-^', label='Avg Accuracy (Val)')\n",
    "        axes[0, 1].set_title('Avg Accuracy'); axes[0, 1].set_xlabel('Epoch'); axes[0, 1].set_ylabel('Accuracy'); axes[0, 1].legend(); axes[0, 1].grid(True)\n",
    "    else: axes[0, 1].set_title(f'{metric_key_acc} (Not Found)')\n",
    "\n",
    "    has_recall = 'i2t recall R@1' in valid_results[0]\n",
    "    if has_recall:\n",
    "        for k in [1, 5, 10]:\n",
    "            axes[1, 0].plot(epochs_val, [res.get(f'i2t recall R@{k}', float('nan')) for res in valid_results], marker='o', label=f'I2T R@{k}')\n",
    "        axes[1, 0].set_title('I2T Recall (Val)'); axes[1, 0].set_xlabel('Epoch'); axes[1, 0].set_ylabel('Recall'); axes[1, 0].legend(); axes[1, 0].grid(True)\n",
    "        for k in [1, 5, 10]:\n",
    "            axes[1, 1].plot(epochs_val, [res.get(f't2i recall R@{k}', float('nan')) for res in valid_results], marker='s', label=f'T2I R@{k}')\n",
    "        axes[1, 1].set_title('T2I Recall (Val)'); axes[1, 1].set_xlabel('Epoch'); axes[1, 1].set_ylabel('Recall'); axes[1, 1].legend(); axes[1, 1].grid(True)\n",
    "    else: axes[1, 0].set_title('I2T Recall (Not Found)'); axes[1, 1].set_title('T2I Recall (Not Found)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plot_names = ['loss', 'accuracy', 'i2t_recall', 't2i_recall']\n",
    "    for idx, name in enumerate(plot_names):\n",
    "        i, j = divmod(idx, 2); save_path = os.path.join(plot_dir, f'training_{name}.png')\n",
    "        if axes[i, j].has_data(): save_subplot_as_figure(axes[i, j], save_path); print(f\"Saved {name} plot to: {save_path}\")\n",
    "        else: print(f\"Skipping save for {name} plot (no data).\")\n",
    "    combined_save_path = os.path.join(plot_dir, 'training_metrics_combined.png')\n",
    "    fig.savefig(combined_save_path, bbox_inches='tight', dpi=300); print(f\"Saved combined plot to: {combined_save_path}\")\n",
    "    # plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "if 'history' in locals() and isinstance(history, dict) and history.get('train_loss'):\n",
    "    plot_training_metrics(history)\n",
    "else:\n",
    "    print(\"No training history found. Run training first.\")\n",
    "\n",
    "# --- END OF SCRIPT ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
