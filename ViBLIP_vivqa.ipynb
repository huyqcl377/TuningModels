{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c61a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"ViBLIP_finetune.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/notebooks/empty.ipynb\n",
    "\n",
    "Adapts the ViCLIP training script for fine-tuning BLIP-2 on a Vietnamese Image Captioning dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Cell 1: Installs\n",
    "\n",
    "# Cell 2: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms # Still potentially useful for basic loading if not using datasets library\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration, Blip2Config\n",
    "from transformers import get_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # Optional: Can still use ReduceLROnPlateau or use transformers schedulers\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time # For timing epochs\n",
    "import nltk # For BLEU/ROUGE calculation if not using evaluate\n",
    "import evaluate # Using Hugging Face evaluate library for metrics\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available, training will be on CPU.\")\n",
    "\n",
    "# Cell 3: Configuration Class (CFG) - Adapted for ViBLIP\n",
    "\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    # Base directory where your train.json, dev.json, test.json are located\n",
    "    image_path = \"./data/OpenViVQA-dataset/\"\n",
    "\n",
    "    # Output directory for saved models\n",
    "\n",
    "    data_path = \"./json_data/\" # Assumes json files are in the same directory as the notebook\n",
    "    image_base_path = \"./data/OpenViVQA-dataset/\" # Base directory for images referenced in JSON\n",
    "\n",
    "    # Output directory for saved models and logs\n",
    "    output_dir = \"./ViBLIP_vivqa\"\n",
    "    logging_dir = f\"{output_dir}/logs\"\n",
    "    checkpoint_dir = f\"{output_dir}/checkpoints\"\n",
    "\n",
    "    # --- BLIP-2 Model Selection ---\n",
    "    # Using Salesforce/blip2-opt-2.7b as a base. Replace if a better Vietnamese BLIP2 exists.\n",
    "    # The processor and model should match.\n",
    "    blip2_processor_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "    blip2_model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "    # --- Training parameters ---\n",
    "    seed = 42\n",
    "    batch_size = 16 # Adjust based on GPU memory (BLIP2 is larger than CLIP components)\n",
    "    num_workers = 4  # Adjust based on your system\n",
    "    learning_rate = 1e-5 # Lower LR often better for fine-tuning large models\n",
    "    weight_decay = 1e-4\n",
    "    # Scheduler options (using transformers scheduler or ReduceLROnPlateau)\n",
    "    lr_scheduler_type = \"linear\" # e.g., linear, cosine, constant, reduce_lr_on_plateau\n",
    "    num_warmup_steps = 100\n",
    "    # ReduceLROnPlateau specific (if lr_scheduler_type='reduce_lr_on_plateau')\n",
    "    rlrop_factor = 0.8\n",
    "    rlrop_patience = 3\n",
    "\n",
    "    epochs = 10 # Adjust as needed\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gradient_accumulation_steps = 2 # Effective batch size = batch_size * gradient_accumulation_steps\n",
    "\n",
    "    # --- Text Generation parameters (for evaluation) ---\n",
    "    generation_max_length = 50 # Max length of generated captions\n",
    "    num_beams = 4 # Beam search size\n",
    "\n",
    "    # --- Checkpointing/Logging parameters ---\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"bleu\" # Metric for best model saving and LR scheduling (e.g., 'bleu', 'rougeL', 'loss')\n",
    "    mode = \"max\" if metric_to_track != \"loss\" else \"min\" # For comparing tracked metric\n",
    "    log_interval = 10 # Log training loss every N steps\n",
    "    eval_strategy = \"epoch\" # Evaluate every epoch ('steps' also possible)\n",
    "    save_strategy = \"epoch\" # Save checkpoint every epoch ('steps' also possible)\n",
    "    save_total_limit = 2 # Keep only the best and the last checkpoint\n",
    "\n",
    "\n",
    "# --- Instantiate Config and Create Output Dirs ---\n",
    "config = CFG()\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(config.logging_dir, exist_ok=True)\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Output path: {os.path.abspath(config.output_dir)}\")\n",
    "print(f\"Image base path: {os.path.abspath(config.image_base_path)}\")\n",
    "print(f\"Using BLIP-2 Processor: {config.blip2_processor_name}\")\n",
    "print(f\"Using BLIP-2 Model: {config.blip2_model_name}\")\n",
    "\n",
    "# Cell 4: Seeding for Reproducibility\n",
    "\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # For multi-GPU\n",
    "        # Deterministic operations can impact performance, use if needed\n",
    "        # torch.backends.cudnn.deterministic = True\n",
    "        # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Cell 5: Metric Calculation Utilities & Setup\n",
    "\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        if torch.is_tensor(val):\n",
    "             val = val.item()\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "# Load evaluation metrics from 'evaluate' library\n",
    "try:\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "    # You can add more metrics like meteor, cider, spice if needed\n",
    "    # meteor_metric = evaluate.load(\"meteor\")\n",
    "    # cider_metric = evaluate.load(\"cider\") # May require installing dependencies\n",
    "    # spice_metric = evaluate.load(\"spice\") # May require installing dependencies\n",
    "    print(\"Evaluation metrics (BLEU, ROUGE) loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading evaluation metrics: {e}\")\n",
    "    print(\"Please ensure 'evaluate' and 'nltk' are installed correctly.\")\n",
    "    bleu_metric = None\n",
    "    rouge_metric = None\n",
    "\n",
    "\n",
    "def compute_captioning_metrics(predictions, references):\n",
    "    \"\"\"Computes BLEU and ROUGE scores using the evaluate library.\"\"\"\n",
    "    metrics = {}\n",
    "    if bleu_metric:\n",
    "        try:\n",
    "            # BLEU expects lists of strings\n",
    "            bleu_results = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references]) # References need to be list of lists\n",
    "            # Extract main BLEU score (often BLEU-4) and potentially others\n",
    "            metrics['bleu'] = bleu_results.get('bleu', 0.0) # Overall BLEU\n",
    "            # metrics['bleu-1'] = bleu_results.get('precisions', [0]*4)[0]\n",
    "            # metrics['bleu-4'] = bleu_results.get('precisions', [0]*4)[3]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute BLEU score: {e}\")\n",
    "            metrics['bleu'] = 0.0\n",
    "    else:\n",
    "        metrics['bleu'] = 0.0\n",
    "\n",
    "\n",
    "    if rouge_metric:\n",
    "        try:\n",
    "            # ROUGE also expects lists of strings\n",
    "            rouge_results = rouge_metric.compute(predictions=predictions, references=references) # References can be list of strings here\n",
    "            metrics.update({k: v for k, v in rouge_results.items()}) # Add rougeL, rouge1, etc.\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute ROUGE score: {e}\")\n",
    "            # Initialize ROUGE keys to 0 if computation fails\n",
    "            metrics.update({'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0})\n",
    "\n",
    "    else:\n",
    "         metrics.update({'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0})\n",
    "\n",
    "    # Add more metrics here if loaded (e.g., meteor, cider, spice)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"Metric utilities defined.\")\n",
    "\n",
    "# Cell 6: Dataset Class Definition - Adapted for ViBLIP Processor\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, processor):\n",
    "        super().__init__()\n",
    "        print(f\"Attempting to load data from: {os.path.abspath(json_path)}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                self.data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: JSON file not found at {json_path}\")\n",
    "            self.data = []\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Could not decode JSON from {json_path}\")\n",
    "            self.data = []\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred loading {json_path}: {e}\")\n",
    "            self.data = []\n",
    "\n",
    "        print(f\"Found {len(self.data)} samples in {os.path.basename(json_path)}.\")\n",
    "        self.image_base_path = image_base_path\n",
    "        self.processor = processor\n",
    "\n",
    "        if not os.path.isdir(self.image_base_path):\n",
    "             print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data):\n",
    "             raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path', None)\n",
    "        captions = item.get('caption', []) # Expecting a list\n",
    "\n",
    "        # Use the first caption if available, otherwise provide an empty string\n",
    "        caption = captions[0] if captions else \"\"\n",
    "\n",
    "        image = None\n",
    "        if relative_image_path:\n",
    "            image_path = os.path.normpath(os.path.join(self.image_base_path, relative_image_path))\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Img not found: {image_path}. Using blank image.\")\n",
    "                image = Image.new('RGB', (224, 224), color = 'white') # Create blank image\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading image {image_path}: {e}. Using blank image.\")\n",
    "                image = Image.new('RGB', (224, 224), color = 'white') # Create blank image\n",
    "        else:\n",
    "            print(f\"Warning: Missing 'image_path' for item at index {idx}. Using blank image.\")\n",
    "            image = Image.new('RGB', (224, 224), color = 'white') # Create blank image\n",
    "\n",
    "        # Process image and text using the BLIP-2 processor\n",
    "        # For training, we need pixel_values, input_ids, attention_mask, and labels\n",
    "        # The processor handles image preprocessing and tokenization\n",
    "        # We provide the caption as 'text' which becomes 'input_ids' and 'attention_mask'\n",
    "        # For the decoder labels during training, we typically use the same caption tokens\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            padding=\"max_length\", # Pad text to max length defined in processor/config\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # The processor might return tensors with a batch dimension of 1, remove it.\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "        # Set labels for language modeling loss (usually same as input_ids for captioning)\n",
    "        encoding[\"labels\"] = encoding[\"input_ids\"].clone()\n",
    "\n",
    "        # Optional: Mask padding tokens in labels if processor doesn't handle it automatically for loss\n",
    "        # padding_token_id = self.processor.tokenizer.pad_token_id\n",
    "        # if padding_token_id is not None:\n",
    "        #     encoding[\"labels\"][encoding[\"labels\"] == padding_token_id] = -100 # Common practice to ignore padding in loss\n",
    "\n",
    "        # Add raw caption for evaluation purposes\n",
    "        encoding[\"raw_caption\"] = caption\n",
    "\n",
    "        return encoding\n",
    "\n",
    "print(\"ImageCaptionDataset class defined for BLIP-2.\")\n",
    "\n",
    "# Cell 7: Training and Validation Epoch Functions - Adapted for ViBLIP\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device, epoch_num, scaler=None, grad_accumulation_steps=1):\n",
    "    model.train()\n",
    "    loss_meter = AvgMeter(f\"Train Loss E{epoch_num}\")\n",
    "    optimizer.zero_grad() # Zero gradients at the start of the epoch\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "        # Use autocast if scaler is provided (for mixed precision)\n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(\n",
    "                    pixel_values=batch['pixel_values'],\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "        else:\n",
    "            outputs = model(\n",
    "                pixel_values=batch['pixel_values'],\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "        # Normalize loss for gradient accumulation\n",
    "        if grad_accumulation_steps > 1:\n",
    "            loss = loss / grad_accumulation_steps\n",
    "\n",
    "        # Backpropagation\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        # Optimizer step (conditionally based on accumulation)\n",
    "        if (step + 1) % grad_accumulation_steps == 0:\n",
    "            if scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            scheduler.step() # Step scheduler after optimizer step\n",
    "            optimizer.zero_grad() # Zero gradients after stepping\n",
    "\n",
    "        loss_meter.update(loss.item() * grad_accumulation_steps, batch['pixel_values'].size(0)) # Use original loss for meter\n",
    "        progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\", lr=f\"{scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        # Log loss periodically\n",
    "        # if (step + 1) % (config.log_interval * grad_accumulation_steps) == 0:\n",
    "        #     print(f\"  Step {step+1}/{len(dataloader)*config.epochs}: Train Loss = {loss_meter.avg:.4f}\")\n",
    "\n",
    "\n",
    "    return loss_meter.avg\n",
    "\n",
    "def validate_epoch(model, processor, dataloader, device, epoch_num):\n",
    "    model.eval()\n",
    "    loss_meter = AvgMeter(f\"Val Loss E{epoch_num}\")\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # Store raw reference captions\n",
    "            raw_captions = batch.pop(\"raw_caption\", []) # Pop non-tensor data before moving batch\n",
    "            all_references.extend(raw_captions)\n",
    "\n",
    "            # Move tensors to device\n",
    "            batch_tensors = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            # --- Calculate Loss ---\n",
    "            # Use autocast for consistency, though grads aren't needed\n",
    "            if device == torch.device(\"cuda\"):\n",
    "                 with torch.cuda.amp.autocast():\n",
    "                    outputs = model(\n",
    "                        pixel_values=batch_tensors['pixel_values'],\n",
    "                        input_ids=batch_tensors['input_ids'],\n",
    "                        attention_mask=batch_tensors['attention_mask'],\n",
    "                        labels=batch_tensors['labels']\n",
    "                    )\n",
    "                    loss = outputs.loss\n",
    "            else:\n",
    "                 outputs = model(\n",
    "                     pixel_values=batch_tensors['pixel_values'],\n",
    "                     input_ids=batch_tensors['input_ids'],\n",
    "                     attention_mask=batch_tensors['attention_mask'],\n",
    "                     labels=batch_tensors['labels']\n",
    "                 )\n",
    "                 loss = outputs.loss\n",
    "\n",
    "            loss_meter.update(loss.item(), batch_tensors['pixel_values'].size(0))\n",
    "\n",
    "            # --- Generate Captions ---\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values=batch_tensors['pixel_values'],\n",
    "                max_length=config.generation_max_length,\n",
    "                num_beams=config.num_beams,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            # Decode generated captions\n",
    "            generated_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            all_predictions.extend([caption.strip() for caption in generated_captions])\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    # Ensure predictions and references are lists of strings\n",
    "    all_predictions = [str(p) for p in all_predictions]\n",
    "    all_references = [str(r) for r in all_references]\n",
    "\n",
    "\n",
    "    # --- Compute Metrics ---\n",
    "    validation_metrics = compute_captioning_metrics(all_predictions, all_references)\n",
    "    validation_metrics['loss'] = loss_meter.avg\n",
    "\n",
    "    # Optionally print a few generated vs reference captions\n",
    "    print(\"\\n--- Sample Generation vs Reference ---\")\n",
    "    for i in range(min(3, len(all_predictions))):\n",
    "        print(f\"  Pred {i+1}: {all_predictions[i]}\")\n",
    "        print(f\"  Ref  {i+1}: {all_references[i]}\")\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    return validation_metrics\n",
    "\n",
    "\n",
    "print(\"Training and Validation epoch functions defined for ViBLIP.\")\n",
    "\n",
    "# Cell 8: Setup - Processor\n",
    "\n",
    "print(f\"Loading BLIP-2 Processor: {config.blip2_processor_name}\")\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(config.blip2_processor_name)\n",
    "    print(\"Processor loaded successfully.\")\n",
    "    # Set decoder_start_token_id if needed (some models require it explicitly)\n",
    "    # if not hasattr(model.config, \"decoder_start_token_id\") or model.config.decoder_start_token_id is None:\n",
    "    #    print(\"Setting decoder_start_token_id to pad_token_id\")\n",
    "    #    model.config.decoder_start_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading processor '{config.blip2_processor_name}': {e}\")\n",
    "    print(\"Please ensure the model name is correct and you have internet access or the model is cached.\")\n",
    "    processor = None # Set to None to prevent errors in subsequent cells\n",
    "\n",
    "# Cell 9: Setup - Datasets and DataLoaders\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "train_json = os.path.join(config.data_path, \"train.json\")\n",
    "dev_json = os.path.join(config.data_path, \"dev.json\")\n",
    "test_json = os.path.join(config.data_path, \"test.json\")\n",
    "\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if processor: # Only proceed if processor loaded correctly\n",
    "    train_dataset = ImageCaptionDataset(\n",
    "        json_path=train_json,\n",
    "        image_base_path=config.image_base_path,\n",
    "        processor=processor\n",
    "    )\n",
    "    dev_dataset = ImageCaptionDataset(\n",
    "        json_path=dev_json,\n",
    "        image_base_path=config.image_base_path,\n",
    "        processor=processor\n",
    "    )\n",
    "\n",
    "    # Basic checks\n",
    "    if not train_dataset.data:\n",
    "        print(\"\\nERROR: Failed to load training data.\")\n",
    "    if not dev_dataset.data:\n",
    "        print(\"\\nWARNING: Failed to load validation data. Validation steps will be skipped.\")\n",
    "\n",
    "    print(\"\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    if train_dataset.data:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False, # Keep last batch for training\n",
    "            collate_fn=lambda batch: processor.pad(batch, return_tensors=\"pt\") # Use processor for dynamic padding\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches (batch size: {config.batch_size}).\")\n",
    "\n",
    "    if dev_dataset.data:\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset,\n",
    "            batch_size=config.batch_size * 2, # Often can use larger batch size for eval\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False,\n",
    "            collate_fn=lambda batch: processor.pad(batch, return_tensors=\"pt\")\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches (batch size: {config.batch_size * 2}).\")\n",
    "\n",
    "    # Optional: Create test loader later if needed\n",
    "    if os.path.exists(test_json):\n",
    "         test_dataset = ImageCaptionDataset(\n",
    "            json_path=test_json,\n",
    "            image_base_path=config.image_base_path,\n",
    "            processor=processor\n",
    "         )\n",
    "         if test_dataset.data:\n",
    "             test_loader = DataLoader(\n",
    "                 test_dataset,\n",
    "                 batch_size=config.batch_size * 2,\n",
    "                 shuffle=False,\n",
    "                 num_workers=num_workers,\n",
    "                 pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "                 drop_last=False,\n",
    "                 collate_fn=lambda batch: processor.pad(batch, return_tensors=\"pt\")\n",
    "             )\n",
    "             print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "         else:\n",
    "            print(\"Test JSON loaded but was empty.\")\n",
    "    else:\n",
    "        print(\"Test JSON not found. Skipping test loader creation.\")\n",
    "\n",
    "    if not train_loader:\n",
    "         print(\"\\nERROR: Train loader could not be created.\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Processor not loaded. Cannot create datasets and dataloaders.\")\n",
    "\n",
    "# Cell 10: Setup - Model, Optimizer, Scheduler\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "scaler = None # For mixed precision\n",
    "\n",
    "if processor and train_loader: # Only proceed if processor and train_loader exist\n",
    "    print(\"\\nInitializing BLIP-2 model...\")\n",
    "    try:\n",
    "        model = Blip2ForConditionalGeneration.from_pretrained(config.blip2_model_name)\n",
    "        model.to(config.device)\n",
    "        print(f\"BLIP-2 model '{config.blip2_model_name}' loaded successfully on {config.device}.\")\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable parameters: {num_params / 1e6:.2f} M\")\n",
    "\n",
    "        # Optional: Freeze parts of the model if desired (e.g., vision encoder)\n",
    "        # for param in model.vision_model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        # print(f\"Trainable parameters after freezing vision encoder: {num_params / 1e6:.2f} M\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR initializing model '{config.blip2_model_name}': {e}\")\n",
    "        print(\"Check model name, internet connection, and available memory.\")\n",
    "        model = None # Ensure model is None if loading fails\n",
    "\n",
    "    if model:\n",
    "        print(\"\\nSetting up optimizer...\")\n",
    "        # Use AdamW optimizer, common for transformers\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        print(f\"Optimizer AdamW initialized (LR={config.learning_rate}, WD={config.weight_decay}).\")\n",
    "\n",
    "        # --- LR Scheduler Setup ---\n",
    "        num_training_steps = config.epochs * len(train_loader) // config.gradient_accumulation_steps\n",
    "        if config.lr_scheduler_type == 'reduce_lr_on_plateau':\n",
    "             lr_scheduler = ReduceLROnPlateau(\n",
    "                 optimizer, mode=config.mode, factor=config.rlrop_factor, patience=config.rlrop_patience\n",
    "             )\n",
    "             print(f\"LR Scheduler ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.rlrop_factor}, patience={config.rlrop_patience})\")\n",
    "        else:\n",
    "            # Use Hugging Face scheduler\n",
    "            lr_scheduler = get_scheduler(\n",
    "                name=config.lr_scheduler_type,\n",
    "                optimizer=optimizer,\n",
    "                num_warmup_steps=config.num_warmup_steps * config.gradient_accumulation_steps,\n",
    "                num_training_steps=num_training_steps\n",
    "            )\n",
    "            print(f\"LR Scheduler '{config.lr_scheduler_type}' initialized (Warmup: {config.num_warmup_steps}, Total Steps: {num_training_steps})\")\n",
    "\n",
    "        # --- Mixed Precision Setup (Optional but recommended for large models) ---\n",
    "        if config.device == torch.device(\"cuda\"):\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "            print(\"CUDA GradScaler enabled for mixed precision.\")\n",
    "\n",
    "else:\n",
    "     print(\"ERROR: Model, Processor or Train Loader not available. Skipping optimizer/scheduler setup.\")\n",
    "\n",
    "# Cell 11: Training Loop\n",
    "\n",
    "if model and train_loader and optimizer and lr_scheduler and processor: # Check prerequisites\n",
    "    print(f\"\\nStarting training for {config.epochs} epochs...\")\n",
    "    print(f\"Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "    print(f\"Tracking metric: '{config.metric_to_track}' (mode: {config.mode}) for best model saving.\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    history = {'train_loss': [], 'validation_results': []}\n",
    "    start_train_time = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "\n",
    "        # --- Training ---\n",
    "        avg_train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, lr_scheduler, config.device,\n",
    "            epoch+1, scaler, config.gradient_accumulation_steps\n",
    "        )\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1}: Average Train Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_results = {\"loss\": float('inf'), config.metric_to_track: best_val_metric} # Default if no validation\n",
    "        if dev_loader and config.eval_strategy == \"epoch\":\n",
    "            print(\"Running validation...\")\n",
    "            val_results = validate_epoch(model, processor, dev_loader, config.device, epoch+1)\n",
    "            history['validation_results'].append(val_results)\n",
    "            # Print validation metrics\n",
    "            print(\"  Validation Metrics:\")\n",
    "            metric_log_str = \"  \"\n",
    "            for name, value in val_results.items():\n",
    "                metric_log_str += f\"{name}: {value:.4f} | \"\n",
    "            print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "            current_val_metric = val_results.get(config.metric_to_track, None)\n",
    "\n",
    "            # Step ReduceLROnPlateau scheduler if using it\n",
    "            if isinstance(lr_scheduler, ReduceLROnPlateau):\n",
    "                 if current_val_metric is not None:\n",
    "                    lr_scheduler.step(current_val_metric)\n",
    "                    print(f\"  Current LR (ReduceLROnPlateau): {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "                 else:\n",
    "                    print(f\"  Warning: Metric '{config.metric_to_track}' not found. ReduceLROnPlateau scheduler not stepped.\")\n",
    "\n",
    "        else:\n",
    "             print(\"  Validation skipped for this epoch based on strategy.\")\n",
    "             history['validation_results'].append(None) # Append None if no validation\n",
    "\n",
    "\n",
    "        # --- Save Checkpoint ---\n",
    "        if config.save_strategy == \"epoch\":\n",
    "            checkpoint_path = os.path.join(config.checkpoint_dir, f\"epoch_{epoch+1}\")\n",
    "            model.save_pretrained(checkpoint_path)\n",
    "            processor.save_pretrained(checkpoint_path)\n",
    "            print(f\"  Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "            is_best = False\n",
    "            if current_val_metric is not None:\n",
    "                if config.mode == \"max\" and current_val_metric > best_val_metric:\n",
    "                    is_best = True\n",
    "                    best_val_metric = current_val_metric\n",
    "                elif config.mode == \"min\" and current_val_metric < best_val_metric:\n",
    "                    is_best = True\n",
    "                    best_val_metric = current_val_metric\n",
    "\n",
    "            if is_best:\n",
    "                best_checkpoint_path = os.path.join(config.checkpoint_dir, \"best_model\")\n",
    "                model.save_pretrained(best_checkpoint_path)\n",
    "                processor.save_pretrained(best_checkpoint_path)\n",
    "                print(f\"  Saved Best Model (Epoch {epoch+1}, {config.metric_to_track}={current_val_metric:.4f}) to {best_checkpoint_path}\")\n",
    "\n",
    "                # Save training history with best model (optional)\n",
    "                history_path = os.path.join(best_checkpoint_path, \"training_history.json\")\n",
    "                with open(history_path, 'w') as f:\n",
    "                    json.dump(history, f, indent=4)\n",
    "\n",
    "\n",
    "        # --- Manage Checkpoints (Remove older ones if limit is set) ---\n",
    "        if config.save_total_limit is not None and config.save_strategy == \"epoch\":\n",
    "            checkpoints = sorted(\n",
    "                [d for d in os.listdir(config.checkpoint_dir) if d.startswith(\"epoch_\")],\n",
    "                key=lambda x: int(x.split('_')[1])\n",
    "            )\n",
    "            if len(checkpoints) > config.save_total_limit:\n",
    "                import shutil\n",
    "                checkpoint_to_remove = os.path.join(config.checkpoint_dir, checkpoints[0])\n",
    "                print(f\"  Removing old checkpoint: {checkpoint_to_remove}\")\n",
    "                shutil.rmtree(checkpoint_to_remove)\n",
    "\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {epoch_end_time - epoch_start_time:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "    # --- End of Training ---\n",
    "    end_train_time = time.time()\n",
    "    total_train_time = end_train_time - start_train_time\n",
    "    print(f\"\\n=============== Training Finished ================\")\n",
    "    print(f\"Total Training Time: {total_train_time:.2f} seconds ({total_train_time/60:.2f} minutes)\")\n",
    "\n",
    "    # Save final model state\n",
    "    final_model_path = os.path.join(config.checkpoint_dir, 'final_model')\n",
    "    model.save_pretrained(final_model_path)\n",
    "    processor.save_pretrained(final_model_path)\n",
    "    print(f\"Final model state saved to {final_model_path}\")\n",
    "\n",
    "    # Save final training history\n",
    "    history_path = os.path.join(final_model_path, \"training_history.json\")\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "    print(f\"Final training history saved to {history_path}\")\n",
    "\n",
    "    if os.path.exists(os.path.join(config.checkpoint_dir, \"best_model\")):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) saved in: {os.path.join(config.checkpoint_dir, 'best_model')}\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training not met (Model, Processor, Train Loader, Optimizer, or Scheduler). Training loop skipped.\")\n",
    "\n",
    "# Cell 12: Final Evaluation on Test Set\n",
    "\n",
    "print(\"\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "if test_loader and processor:\n",
    "    # --- Load Best Model for Testing ---\n",
    "    best_model_path = os.path.join(config.checkpoint_dir, \"best_model\")\n",
    "    final_model_path = os.path.join(config.checkpoint_dir, \"final_model\")\n",
    "    model_to_test = None\n",
    "    processor_to_test = None\n",
    "\n",
    "    load_path = None\n",
    "    if os.path.exists(best_model_path):\n",
    "        load_path = best_model_path\n",
    "        print(f\"Attempting to load best model weights from: {load_path}\")\n",
    "    elif os.path.exists(final_model_path):\n",
    "        load_path = final_model_path\n",
    "        print(f\"Best model not found. Attempting to load final model weights from: {load_path}\")\n",
    "    else:\n",
    "        print(\"WARNING: No saved model checkpoints ('best_model' or 'final_model') found.\")\n",
    "        print(\"         Evaluation will not be performed.\")\n",
    "\n",
    "    if load_path:\n",
    "        try:\n",
    "            print(f\"Loading model from {load_path}...\")\n",
    "            model_to_test = Blip2ForConditionalGeneration.from_pretrained(load_path)\n",
    "            processor_to_test = AutoProcessor.from_pretrained(load_path)\n",
    "            model_to_test.to(config.device)\n",
    "            print(\"Model and processor for testing loaded successfully.\")\n",
    "\n",
    "            # --- Run Evaluation ---\n",
    "            print(\"\\nRunning evaluation on test set...\")\n",
    "            test_results = validate_epoch(model_to_test, processor_to_test, test_loader, config.device, epoch_num=\"Test\")\n",
    "\n",
    "            print(\"\\n--- Test Set Results ---\")\n",
    "            metric_log_str = \"\"\n",
    "            for name, value in test_results.items():\n",
    "                metric_log_str += f\"  {name}: {value:.4f}\\n\"\n",
    "            print(metric_log_str.strip())\n",
    "            print(\"------------------------\")\n",
    "\n",
    "            # Save test results\n",
    "            test_results_path = os.path.join(config.output_dir, \"test_results.json\")\n",
    "            with open(test_results_path, 'w') as f:\n",
    "                json.dump(test_results, f, indent=4)\n",
    "            print(f\"Test results saved to {test_results_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR loading model/processor or running evaluation from {load_path}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    if not test_loader:\n",
    "         print(\"Test loader not available. Skipping test evaluation.\")\n",
    "    if not processor:\n",
    "         print(\"Processor not available. Skipping test evaluation.\")\n",
    "\n",
    "print(\"\\n================= Evaluation Finished ==================\")\n",
    "\n",
    "# Cell 13: Training Visualization (Adapted for Captioning Metrics)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Create plot directory if it doesn't exist\n",
    "plot_dir = os.path.join(config.output_dir, \"plots\")\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "print(f\"\\nPlot directory created at: {os.path.abspath(plot_dir)}\")\n",
    "\n",
    "# --- Load History ---\n",
    "# Try loading from best model first, then final model\n",
    "history_loaded = None\n",
    "best_history_path = os.path.join(config.checkpoint_dir, \"best_model\", \"training_history.json\")\n",
    "final_history_path = os.path.join(config.checkpoint_dir, \"final_model\", \"training_history.json\")\n",
    "\n",
    "if os.path.exists(best_history_path):\n",
    "    print(f\"Loading history from {best_history_path}\")\n",
    "    with open(best_history_path, 'r') as f:\n",
    "        history_loaded = json.load(f)\n",
    "elif os.path.exists(final_history_path):\n",
    "    print(f\"Loading history from {final_history_path}\")\n",
    "    with open(final_history_path, 'r') as f:\n",
    "        history_loaded = json.load(f)\n",
    "elif 'history' in globals():\n",
    "     print(\"Using history from current training run.\")\n",
    "     history_loaded = history # Use history from the training loop if available\n",
    "\n",
    "\n",
    "def save_plot(fig, save_path):\n",
    "    \"\"\"Helper function to save a figure\"\"\"\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved plot to: {save_path}\")\n",
    "    plt.close(fig) # Close figure after saving\n",
    "\n",
    "\n",
    "def plot_training_metrics_blip(history_data):\n",
    "    if not history_data or not history_data.get('train_loss') or not history_data.get('validation_results'):\n",
    "        print(\"No valid training history available to plot.\")\n",
    "        return\n",
    "\n",
    "    # Filter out None entries in validation results if any epoch was skipped\n",
    "    valid_val_results = [res for res in history_data['validation_results'] if res]\n",
    "    if not valid_val_results:\n",
    "         print(\"No valid validation results found in history.\")\n",
    "         # Plot only training loss if available\n",
    "         if history_data.get('train_loss'):\n",
    "            epochs = range(1, len(history_data['train_loss']) + 1)\n",
    "            fig_loss, ax_loss = plt.subplots(1, 1, figsize=(8, 6))\n",
    "            ax_loss.plot(epochs, history_data['train_loss'], 'b-', label='Training Loss')\n",
    "            ax_loss.set_title('Training Loss over Epochs')\n",
    "            ax_loss.set_xlabel('Epoch')\n",
    "            ax_loss.set_ylabel('Loss')\n",
    "            ax_loss.legend()\n",
    "            ax_loss.grid(True)\n",
    "            save_plot(fig_loss, os.path.join(plot_dir, 'training_loss.png'))\n",
    "            plt.show()\n",
    "         return\n",
    "\n",
    "    # Determine number of epochs based on validation results\n",
    "    epochs = range(1, len(valid_val_results) + 1)\n",
    "    train_loss_to_plot = history_data['train_loss'][:len(epochs)] # Match length if training loss is longer\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    val_loss = [res.get('loss', np.nan) for res in valid_val_results] # Use np.nan for missing keys\n",
    "    val_bleu = [res.get('bleu', np.nan) for res in valid_val_results]\n",
    "    val_rougeL = [res.get('rougeL', np.nan) for res in valid_val_results]\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6)) # Adjusted for 3 main plots\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16, y=1.02)\n",
    "\n",
    "    # Plot Loss\n",
    "    axes[0].plot(epochs, train_loss_to_plot, 'b-', label='Training Loss')\n",
    "    axes[0].plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "    axes[0].set_title('Loss over Epochs')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    save_subplot_as_figure(axes[0], os.path.join(plot_dir, 'training_loss.png'))\n",
    "\n",
    "\n",
    "    # Plot BLEU Score\n",
    "    axes[1].plot(epochs, val_bleu, 'g-', label='Validation BLEU')\n",
    "    axes[1].set_title('Validation BLEU Score over Epochs')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('BLEU')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    save_subplot_as_figure(axes[1], os.path.join(plot_dir, 'training_bleu.png'))\n",
    "\n",
    "\n",
    "    # Plot ROUGE-L Score\n",
    "    axes[2].plot(epochs, val_rougeL, 'm-', label='Validation ROUGE-L')\n",
    "    axes[2].set_title('Validation ROUGE-L Score over Epochs')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('ROUGE-L')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    save_subplot_as_figure(axes[2], os.path.join(plot_dir, 'training_rougeL.png'))\n",
    "\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "\n",
    "    # Save combined plot\n",
    "    combined_save_path = os.path.join(plot_dir, 'training_metrics_combined.png')\n",
    "    fig.savefig(combined_save_path, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved combined plot to: {combined_save_path}\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot if history was loaded or exists\n",
    "if history_loaded:\n",
    "    plot_training_metrics_blip(history_loaded)\n",
    "else:\n",
    "    print(\"No training history found to plot.\")\n",
    "\n",
    "# Cell 14: Helper function to save subplots (needed for plot_training_metrics_blip)\n",
    "# NOTE: This needs to be defined *before* calling plot_training_metrics_blip\n",
    "# Moved it here for logical flow, ensure it's run before Cell 13 if running cells individually.\n",
    "\n",
    "def save_subplot_as_figure(subplot, save_path):\n",
    "    \"\"\"Helper function to save a subplot axes as a separate figure\"\"\"\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Copy lines and labels\n",
    "    lines = subplot.get_lines()\n",
    "    if not lines: # Skip if no lines to plot\n",
    "        plt.close(fig)\n",
    "        return\n",
    "    labels = [line.get_label() for line in lines]\n",
    "\n",
    "    for line in lines:\n",
    "        ax.plot(line.get_xdata(), line.get_ydata(),\n",
    "                color=line.get_color(),\n",
    "                label=line.get_label(),\n",
    "                linestyle=line.get_linestyle(),\n",
    "                marker=line.get_marker()) # Copy marker style too\n",
    "\n",
    "    # Copy title, labels, grid, and legend\n",
    "    ax.set_title(subplot.get_title())\n",
    "    ax.set_xlabel(subplot.get_xlabel())\n",
    "    ax.set_ylabel(subplot.get_ylabel())\n",
    "    ax.grid(subplot.axes.has_grid()) # Copy grid status\n",
    "    if any(label and not label.startswith('_') for label in labels): # Check if there are valid labels for legend\n",
    "        ax.legend()\n",
    "\n",
    "    save_plot(fig, save_path) # Use the save_plot helper\n",
    "\n",
    "\n",
    "print(\"Subplot saving helper function defined.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
