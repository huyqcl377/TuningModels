{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "Transformers Version: 4.50.0\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Import BLIP specific components + AutoModel/Tokenizer\n",
    "from transformers import BlipVisionModel, BlipConfig, BlipImageProcessor\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time # For timing epochs\n",
    "import transformers\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name()}\")\n",
    "    # torch.cuda.set_device()  # Set CUDA device to '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model output path: ./trained_models/ViCLIP_vivqa\n",
      "Selected Vision Source: Salesforce/blip-image-captioning-base\n",
      "Selected Text Model: vinai/phobert-base\n",
      "Image base path (for resolving paths in JSON): /root/TuningModels/data/OpenViVQA-dataset\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Configuration Class (CFG) === MODIFIED for BASE Models ===\n",
    "class CFG:\n",
    "    # --- Paths ---\\\n",
    "    data_path = \"./json_data/\"\n",
    "    image_path = \"./data/OpenViVQA-dataset/\"\n",
    "\n",
    "    # Output directory for saved models\n",
    "    model_path = \"./trained_models/ViCLIP_vivqa\"\n",
    "\n",
    "    # --- Model Selection ---\n",
    "    # Source for BLIP Vision Model components - Changed to BASE version\n",
    "    selected_vision_source = \"Salesforce/blip-image-captioning-base\" \n",
    "    # Vietnamese Text Model - Changed to BASE version\n",
    "    selected_text_model = \"vinai/phobert-base\" # <<< CHANGED\n",
    "    text_tokenizer_name = selected_text_model # Use PhoBERT's tokenizer (base/large often share tokenizer type)\n",
    "\n",
    "    # --- Model parameters ---\n",
    "    blip_vision_model_name = selected_vision_source\n",
    "    blip_image_processor_name = selected_vision_source # Processor usually matches the main checkpoint\n",
    "\n",
    "    @property\n",
    "    def text_embedding(self): # PhoBERT-Base output dim\n",
    "        return 768 # <<< CHANGED (from 1024)\n",
    "    @property\n",
    "    def vision_embedding(self): # BLIP-Base (ViT-B) output dim\n",
    "        # Check config if unsure: BlipConfig.from_pretrained(self.selected_vision_source).vision_config.hidden_size\n",
    "        return 768 # <<< CHANGED (from 1024)\n",
    "\n",
    "    projection_dim = 256 # Shared latent space dimension (Can keep this or adjust if desired)\n",
    "\n",
    "    # --- Training parameters ---\n",
    "    seed = 42\n",
    "    # Consider increasing batch_size if GPU memory allows with smaller models\n",
    "    batch_size = 16  # <<< INCREASED (Example - adjust based on your GPU memory)\n",
    "    num_workers = 20\n",
    "    # Learning rates (These might need tuning for base models, but start similarly)\n",
    "    projection_lr = 5e-5\n",
    "    vision_encoder_lr = 1e-5 # Base models might tolerate slightly higher LR, but 1e-5 is also safe start\n",
    "    text_encoder_lr = 2e-5   # Base models might tolerate slightly higher LR, but 1e-5 is also safe start\n",
    "    weight_decay = 1e-3\n",
    "    patience = 3\n",
    "    factor = 0.8\n",
    "    epochs = 40\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_amp = False # Keep disabled for simplicity\n",
    "\n",
    "    # --- Image/Text parameters ---\n",
    "    max_length = 256 # PhoBERT's default max length (PhoBERT base also typically handles 256)\n",
    "\n",
    "    # --- Loss/Saving parameters ---\n",
    "    temperature = 0.07\n",
    "    learnable_temperature = False\n",
    "    save_best_only = True\n",
    "    # Track a relevant metric (adjust if needed, e.g., 'i2t recall R@1')\n",
    "    metric_to_track = \"avg_acc\"\n",
    "    mode = \"max\"\n",
    "    early_stopping_patience = 5\n",
    "    early_stopping_min_delta = 0.001\n",
    "    # Gradient accumulation (adjust if batch_size is still limited)\n",
    "    accumulation_steps = 1\n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True) # Creates the new directory if needed\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Selected Vision Source: {config.selected_vision_source}\")\n",
    "print(f\"Selected Text Model: {config.selected_text_model}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: Seeding for Reproducibility ===\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # torch.backends.cudnn.deterministic = True\n",
    "        # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Metric Calculation Utilities ===\n",
    "\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        # Ensure val is a scalar number before adding to sum\n",
    "        if torch.is_tensor(val):\n",
    "             val = val.item() # Convert tensor to Python number\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "        # else:\n",
    "            # Optionally print a warning if the value is not usable\n",
    "            # print(f\"Warning: Cannot update AvgMeter '{self.name}' with value type {type(val)}\")\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    n = similarity_matrix.shape[1-dim]\n",
    "    correct_count = 0\n",
    "    top_k_indices = torch.topk(similarity_matrix, k, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "\n",
    "    if dim == 0: # I2T\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]:\n",
    "                correct_count += 1\n",
    "    elif dim == 1: # T2I\n",
    "        for txt_idx in range(n):\n",
    "             if ground_truth[txt_idx] in top_k_indices[txt_idx, :]:\n",
    "                correct_count += 1\n",
    "    else: raise ValueError(\"dim must be 0 or 1\")\n",
    "    return correct_count / n if n > 0 else 0.0\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    sim_matrix = text_embeddings @ image_embeddings.T\n",
    "    sim_matrix = sim_matrix.float() # Ensure float for calculations\n",
    "    n = sim_matrix.shape[0]\n",
    "    if n == 0:\n",
    "        # Return default zero metrics for empty batch\n",
    "        return {\n",
    "            \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "            \"avg_cosine_sim\": 0.0,\n",
    "            \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "        }\n",
    "\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "    i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "    t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "    t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "    avg_acc = (i2t_acc + t2i_acc) / 2\n",
    "    avg_cosine_sim = torch.diagonal(sim_matrix).mean().item()\n",
    "\n",
    "    i2t_recall = {}\n",
    "    t2i_recall = {}\n",
    "    recall_k_values = [k for k in [1, 5, 10] if k <= n]\n",
    "    for k in recall_k_values:\n",
    "        i2t_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "        t2i_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "\n",
    "    # Ensure all keys R@1, R@5, R@10 exist even if k>n\n",
    "    for k in [1, 5, 10]:\n",
    "        k_str = f\"R@{k}\"\n",
    "        if k_str not in i2t_recall: i2t_recall[k_str] = 0.0\n",
    "        if k_str not in t2i_recall: t2i_recall[k_str] = 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "        \"avg_cosine_sim\": avg_cosine_sim,\n",
    "        \"i2t_recall\": i2t_recall, \"t2i_recall\": t2i_recall\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomImageCaptionDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Dataset Class Definition (Separate Tokenizer/Processor) === MODIFIED ===\n",
    "class CustomImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, tokenizer, image_processor, max_length): # Changed arguments\n",
    "        super().__init__()\n",
    "        print(f\"Attempting to load data from: {os.path.abspath(json_path)}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f: self.data = json.load(f)\n",
    "        except Exception as e: print(f\"ERROR loading JSON {json_path}: {e}\"); self.data = []\n",
    "\n",
    "        print(f\"Found {len(self.data)} samples in {os.path.basename(json_path)}.\")\n",
    "        self.image_base_path = image_base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor # Store image processor\n",
    "        self.max_length = max_length\n",
    "        try: # Get image size\n",
    "             # BlipImageProcessor uses 'size' dictionary with 'shortest_edge' or 'height'/'width'\n",
    "             if isinstance(image_processor.size, dict):\n",
    "                 self.img_size = image_processor.size.get('shortest_edge', image_processor.size.get('height', 224))\n",
    "             else: # Older versions might just have an int/tuple\n",
    "                 self.img_size = image_processor.size\n",
    "                 if isinstance(self.img_size, (tuple, list)): self.img_size = self.img_size[0]\n",
    "        except AttributeError:\n",
    "             print(\"Warning: Could not determine image size from processor, defaulting to 224.\")\n",
    "             self.img_size = 224\n",
    "        print(f\"Using image size: {self.img_size}x{self.img_size}\")\n",
    "        if not os.path.isdir(self.image_base_path): print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data): raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path')\n",
    "        # Expecting a list of captions, take the first one\n",
    "        captions = item.get('caption', [])\n",
    "        caption = captions[0] if captions else \"\"\n",
    "\n",
    "        # Load Image\n",
    "        image = None\n",
    "        pixel_values = torch.zeros((3, self.img_size, self.img_size)) # Dummy tensor\n",
    "        if relative_image_path:\n",
    "            image_path = os.path.normpath(os.path.join(self.image_base_path, relative_image_path))\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "            except FileNotFoundError: print(f\"Warning: Img not found: {image_path}. Using dummy for idx {idx}.\")\n",
    "            except Exception as e: print(f\"Warning: Error loading image {image_path}: {e}. Using dummy for idx {idx}.\")\n",
    "        else: print(f\"Warning: Missing 'image_path' for idx {idx}. Using dummy.\")\n",
    "        if image is None: image = Image.new('RGB', (self.img_size, self.img_size))\n",
    "\n",
    "        # Process Image using image_processor\n",
    "        try:\n",
    "            image_inputs = self.image_processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = image_inputs['pixel_values'].squeeze(0) # Remove batch dim\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image idx {idx}: {e}\")\n",
    "            # pixel_values remains the dummy tensor initialized earlier\n",
    "\n",
    "        # Process Text using tokenizer\n",
    "        try:\n",
    "            text_inputs = self.tokenizer(\n",
    "                caption, padding='max_length', truncation=True,\n",
    "                max_length=self.max_length, return_tensors='pt'\n",
    "            )\n",
    "            input_ids = text_inputs['input_ids'].squeeze(0) # Remove batch dim\n",
    "            attention_mask = text_inputs['attention_mask'].squeeze(0) # Remove batch dim\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text '{caption}' idx {idx}: {e}\")\n",
    "            input_ids = torch.zeros(self.max_length, dtype=torch.long)\n",
    "            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "print(\"CustomImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom BLIP+PhoBERT Model components (with corrected vision loading v2) and loss function defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Model Definition (PhoBERT + BLIP Vision) === CORRECTED LOADING v2 ===\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# --- Import BlipForImageTextRetrieval ---\n",
    "from transformers import BlipForImageTextRetrieval, BlipVisionModel, BlipConfig\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig # Text parts (PhoBERT)\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Encodes images using BLIP's Vision Model (Base). - CORRECTED LOADING v2\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        # --- Try loading BlipForImageTextRetrieval first ---\n",
    "        print(f\"Initializing BLIP Vision Encoder from: {config_train.blip_vision_model_name} by loading BlipForImageTextRetrieval first.\")\n",
    "\n",
    "        if pretrained:\n",
    "            try:\n",
    "                # Load a task-specific model like BlipForImageTextRetrieval\n",
    "                print(\"  Loading base BlipForImageTextRetrieval...\")\n",
    "                # Set low_cpu_mem_usage=True to potentially reduce RAM spike\n",
    "                # ignore_mismatched_sizes=True can sometimes help if there are minor mismatches\n",
    "                # between config and checkpoint, though often not needed for base extraction.\n",
    "                full_blip_model = BlipForImageTextRetrieval.from_pretrained(\n",
    "                    config_train.blip_vision_model_name,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    # ignore_mismatched_sizes=True # Uncomment if needed\n",
    "                 )\n",
    "                print(\"  Extracting vision_model from BlipForImageTextRetrieval.\")\n",
    "                # Extract the vision model part\n",
    "                self.vision_model = full_blip_model.vision_model\n",
    "                # Release the reference to the larger model\n",
    "                del full_blip_model\n",
    "                print(\"  Vision model extracted successfully.\")\n",
    "                # Optional garbage collection\n",
    "                # import gc\n",
    "                # gc.collect()\n",
    "                # if torch.cuda.is_available():\n",
    "                #     torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR loading BlipForImageTextRetrieval or extracting vision model: {e}\")\n",
    "                print(\"  Falling back to initializing BlipVisionModel directly (might show warnings).\")\n",
    "                # Fallback to original method if the above fails\n",
    "                self.vision_model = BlipVisionModel.from_pretrained(config_train.blip_vision_model_name)\n",
    "        else:\n",
    "             print(\"  Initializing BlipVisionModel from scratch (as pretrained=False).\")\n",
    "             blip_vision_config = BlipConfig.from_pretrained(config_train.blip_vision_model_name).vision_config\n",
    "             self.vision_model = BlipVisionModel(blip_vision_config)\n",
    "        # --- END MODIFIED LOADING ---\n",
    "\n",
    "        # Get the hidden size directly from the loaded model's config\n",
    "        try:\n",
    "            self.input_features = self.vision_model.config.hidden_size\n",
    "        except AttributeError as e:\n",
    "             print(f\"  ERROR accessing vision_model.config.hidden_size: {e}. Attempting config_train value.\")\n",
    "             self.input_features = config_train.vision_embedding # Fallback\n",
    "\n",
    "        if hasattr(config_train, 'vision_embedding') and self.input_features != config_train.vision_embedding:\n",
    "             print(f\"  WARNING: Configured vision_embedding ({config_train.vision_embedding}) doesn't match loaded model hidden size ({self.input_features}). Using loaded size.\")\n",
    "        else:\n",
    "             print(f\"  Confirmed/Using vision model hidden size: {self.input_features}\")\n",
    "\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        pixel_values = pixel_values.to(self.vision_model.device)\n",
    "        vision_outputs = self.vision_model(pixel_values=pixel_values, return_dict=True)\n",
    "        image_features = vision_outputs.pooler_output\n",
    "        projected_features = self.projection(image_features)\n",
    "        projected_features = F.normalize(projected_features, p=2, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "# --- TextEncoder remains the same ---\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Encodes text using PhoBERT-Base.\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        print(f\"Initializing Text Encoder: {config_train.selected_text_model}\")\n",
    "\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config_train.selected_text_model)\n",
    "        else:\n",
    "            model_config = AutoConfig.from_pretrained(config_train.selected_text_model)\n",
    "            self.model = AutoModel.from_config(model_config)\n",
    "\n",
    "        self.input_features = config_train.text_embedding\n",
    "        actual_hidden_size = self.model.config.hidden_size\n",
    "        if actual_hidden_size != self.input_features:\n",
    "             print(f\"  WARNING: Configured text_embedding ({self.input_features}) does not match PhoBERT hidden size ({actual_hidden_size}). Using actual size.\")\n",
    "             self.input_features = actual_hidden_size\n",
    "        else:\n",
    "            print(f\"  Confirmed text model hidden size: {self.input_features}\")\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        input_ids = input_ids.to(self.model.device)\n",
    "        attention_mask = attention_mask.to(self.model.device)\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        text_features = outputs.last_hidden_state[:, 0, :]\n",
    "        projected_features = self.projection(text_features)\n",
    "        projected_features = F.normalize(projected_features, p=2, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "# --- CustomBlipPhobertModel remains the same ---\n",
    "class CustomBlipPhobertModel(nn.Module):\n",
    "    \"\"\"Combines BLIP Vision encoder and PhoBERT Text encoder for contrastive retrieval.\"\"\"\n",
    "    def __init__(self, image_encoder, text_encoder, config_train):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.config_train = config_train\n",
    "\n",
    "        if config_train.learnable_temperature:\n",
    "            init_val = torch.ones([]) * np.log(1 / config_train.temperature)\n",
    "            self.logit_scale = nn.Parameter(init_val)\n",
    "            print(f\"Using learnable temperature, initialized to {self.logit_scale.exp().item():.4f}\")\n",
    "        else:\n",
    "            # Ensure buffer is created on the correct device initially if possible\n",
    "            # Although .to(device) on the model should handle it later\n",
    "            temp_tensor = torch.tensor(np.log(1 / config_train.temperature))\n",
    "            self.register_buffer('logit_scale', temp_tensor)\n",
    "            print(f\"Using fixed temperature: {config_train.temperature}\")\n",
    "\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        image_features = self.image_encoder(pixel_values)\n",
    "        text_features = self.text_encoder(input_ids, attention_mask)\n",
    "\n",
    "        if isinstance(self.logit_scale, nn.Parameter):\n",
    "             current_logit_scale = self.logit_scale.to(image_features.device).exp()\n",
    "        else:\n",
    "             current_logit_scale = self.logit_scale.exp().to(image_features.device)\n",
    "\n",
    "        logits_per_image = current_logit_scale.float() * image_features.float() @ text_features.float().t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text, image_features, text_features\n",
    "\n",
    "# --- Loss Function (Contrastive Loss) remains the same ---\n",
    "def contrastive_loss(logits_per_image, logits_per_text):\n",
    "    logits_per_image = logits_per_image.float()\n",
    "    logits_per_text = logits_per_text.float()\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    if batch_size == 0: return torch.tensor(0.0, device=logits_per_image.device, requires_grad=True)\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "    loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "    total_loss = (loss_img + loss_txt) / 2.0\n",
    "    return total_loss\n",
    "\n",
    "print(\"Custom BLIP+PhoBERT Model components (with corrected vision loading v2) and loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation epoch functions defined (No AMP).\n"
     ]
    }
   ],
   "source": [
    "# === Cell 7: Training and Validation Epoch Functions (No AMP/Scaler) ===\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch_num):\n",
    "    model.train()\n",
    "    loss_meter = AvgMeter(f\"Train Loss E{epoch_num}\")\n",
    "    try: from tqdm.notebook import tqdm as pbar\n",
    "    except ImportError: from tqdm import tqdm as pbar\n",
    "    progress_bar = pbar(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_size = pixel_values.size(0)\n",
    "        if batch_size == 0: continue\n",
    "\n",
    "        logits_per_image, logits_per_text, _, _ = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "        # Ensure logits are float32 for loss\n",
    "        loss = contrastive_loss(logits_per_image.float(), logits_per_text.float())\n",
    "        loss = loss / config.accumulation_steps # Normalize loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % config.accumulation_steps == 0 or (i + 1) == len(dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_meter.update(loss.item() * config.accumulation_steps, batch_size) # Log un-normalized loss\n",
    "        progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    optimizer.zero_grad() # Clean up at end\n",
    "    return loss_meter.avg\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, device, epoch_num):\n",
    "    model.eval()\n",
    "    loss_meter = AvgMeter(f\"Val Loss E{epoch_num}\")\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    try: from tqdm.notebook import tqdm as pbar\n",
    "    except ImportError: from tqdm import tqdm as pbar\n",
    "    progress_bar = pbar(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = pixel_values.size(0)\n",
    "            if batch_size == 0: continue\n",
    "\n",
    "            logits_per_image, logits_per_text, image_embeds, text_embeds = model(pixel_values, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(logits_per_image.float(), logits_per_text.float()) # Ensure FP32\n",
    "\n",
    "            loss_meter.update(loss.item(), batch_size)\n",
    "            all_image_embeddings.append(image_embeds.cpu())\n",
    "            all_text_embeddings.append(text_embeds.cpu())\n",
    "            progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    if not all_image_embeddings or not all_text_embeddings:\n",
    "         print(\"Warning: No embeddings collected during validation.\")\n",
    "         zero_metrics = { \"loss\": loss_meter.avg, \"avg acc\": 0.0, \"avg cosine sim\": 0.0,\n",
    "                           \"i2t recall R@1\": 0.0, \"i2t recall R@5\": 0.0, \"i2t recall R@10\": 0.0,\n",
    "                           \"t2i recall R@1\": 0.0, \"t2i recall R@5\": 0.0, \"t2i recall R@10\": 0.0,\n",
    "                           \"avg R@1\": 0.0, \"avg R@5\": 0.0, \"avg R@10\": 0.0 }\n",
    "         return zero_metrics\n",
    "\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "\n",
    "    print(f\"\\nComputing metrics over {all_image_embeddings.shape[0]} validation samples...\")\n",
    "    validation_metrics = compute_metrics(all_image_embeddings.to(device), all_text_embeddings.to(device))\n",
    "\n",
    "    final_results = {\"loss\": loss_meter.avg}\n",
    "    for k, v in validation_metrics.items():\n",
    "        if isinstance(v, dict): # Handle recall dicts\n",
    "            for recall_k, recall_v in v.items(): final_results[f\"{k.replace('_', ' ')} {recall_k}\"] = recall_v\n",
    "        else: final_results[k.replace('_', ' ')] = v\n",
    "    return final_results\n",
    "\n",
    "print(\"Training and Validation epoch functions defined (No AMP).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer: vinai/phobert-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhoBERT Tokenizer loaded successfully.\n",
      "Loading Image Processor from: Salesforce/blip-image-captioning-base\n",
      "BLIP Image Processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8: Setup - Tokenizer and Image Processor === MODIFIED ===\n",
    "from transformers import AutoTokenizer, BlipImageProcessor\n",
    "\n",
    "tokenizer = None\n",
    "image_processor = None\n",
    "\n",
    "print(f\"Loading Tokenizer: {config.text_tokenizer_name}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer_name)\n",
    "    print(\"PhoBERT Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading tokenizer '{config.text_tokenizer_name}': {e}\")\n",
    "\n",
    "print(f\"Loading Image Processor from: {config.blip_image_processor_name}\")\n",
    "try:\n",
    "    # Use BlipImageProcessor associated with the vision model source\n",
    "    image_processor = BlipImageProcessor.from_pretrained(config.blip_image_processor_name)\n",
    "    print(\"BLIP Image Processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading image processor '{config.blip_image_processor_name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Attempting to load data from: /root/TuningModels/json_data/train.json\n",
      "Found 18899 samples in train.json.\n",
      "Using image size: 384x384\n",
      "Attempting to load data from: /root/TuningModels/json_data/dev.json\n",
      "Found 2239 samples in dev.json.\n",
      "Using image size: 384x384\n",
      "\n",
      "Creating dataloaders...\n",
      "Using 20 workers for DataLoaders.\n",
      "Train loader created with 1181 batches.\n",
      "Validation loader created with 140 batches.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 9: Setup - Datasets and DataLoaders === MODIFIED ===\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "\n",
    "if tokenizer and image_processor: # Check if both loaded\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_json = os.path.join(config.data_path, \"train.json\")\n",
    "    dev_json = os.path.join(config.data_path, \"dev.json\")\n",
    "\n",
    "    train_dataset = CustomImageCaptionDataset( # Use the custom dataset class\n",
    "        json_path=train_json, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, image_processor=image_processor, # Pass separate components\n",
    "        max_length=config.max_length\n",
    "    )\n",
    "    dev_dataset = CustomImageCaptionDataset( # Use the custom dataset class\n",
    "        json_path=dev_json, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, image_processor=image_processor, # Pass separate components\n",
    "        max_length=config.max_length\n",
    "    )\n",
    "\n",
    "    if not train_dataset.data: print(\"\\nERROR: Failed to load training data.\")\n",
    "    if not dev_dataset.data: print(\"\\nWARNING: Failed to load validation data.\")\n",
    "\n",
    "    print(\"\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    if train_dataset.data:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=True\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "    else: print(\"Skipping train loader creation.\")\n",
    "\n",
    "    if dev_dataset.data:\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "    else: print(\"Skipping validation loader creation.\")\n",
    "\n",
    "    if not train_loader: print(\"\\nERROR: Train loader could not be created.\")\n",
    "else:\n",
    "     print(\"ERROR: Tokenizer or Image Processor not loaded. Skipping dataset/loader creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model components...\n",
      "Initializing BLIP Vision Encoder from: Salesforce/blip-image-captioning-base by loading BlipForImageTextRetrieval first.\n",
      "  Loading base BlipForImageTextRetrieval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipForImageTextRetrieval were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['itm_head.bias', 'itm_head.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.word_embeddings.weight', 'text_proj.bias', 'text_proj.weight', 'vision_proj.bias', 'vision_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracting vision_model from BlipForImageTextRetrieval.\n",
      "  Vision model extracted successfully.\n",
      "  Confirmed/Using vision model hidden size: 768\n",
      "  Added projection head: 768 -> 256\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Confirmed text model hidden size: 768\n",
      "  Added projection head: 768 -> 256\n",
      "Using fixed temperature: 0.07\n",
      "\n",
      "CustomBlipPhobertModel initialized successfully on cuda.\n",
      "Total parameters: 221.48 M\n",
      "Trainable parameters: 221.48 M\n",
      "\n",
      "Setting up optimizer...\n",
      "  Param counts (Trainable): VisionBase=150, VisionHead=1, TextBase=199, TextHead=1, LogitScale=0\n",
      "Optimizer AdamW initialized.\n",
      "LR Scheduler ReduceLROnPlateau initialized (mode='max', factor=0.8, patience=3)\n",
      "Early stopping initialized (patience=5, min_delta=0.001)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10: Setup - Model, Optimizer, Scheduler === MODIFIED ===\n",
    "model = None\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "\n",
    "print(\"\\nInitializing model components...\")\n",
    "try:\n",
    "    # Instantiate the modified encoders and model wrapper\n",
    "    image_encoder = ImageEncoder(config).to(config.device) # Uses BlipVisionModel\n",
    "    text_encoder = TextEncoder(config).to(config.device)   # Uses PhoBERT-Large\n",
    "    model = CustomBlipPhobertModel(image_encoder, text_encoder, config).to(config.device)\n",
    "    print(f\"\\nCustomBlipPhobertModel initialized successfully on {config.device}.\")\n",
    "    num_params_total = sum(p.numel() for p in model.parameters())\n",
    "    num_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {num_params_total / 1e6:.2f} M\")\n",
    "    print(f\"Trainable parameters: {num_params_trainable / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing model components: {e}\")\n",
    "    model = None\n",
    "\n",
    "if model:\n",
    "    print(\"\\nSetting up optimizer...\")\n",
    "    # Get parameters from the specific model components\n",
    "    image_encoder_base_params = [p for p in model.image_encoder.vision_model.parameters() if p.requires_grad]\n",
    "    image_head_params = [p for p in model.image_encoder.projection.parameters() if p.requires_grad]\n",
    "    text_encoder_base_params = [p for p in model.text_encoder.model.parameters() if p.requires_grad] # PhoBERT base\n",
    "    text_head_params = [p for p in model.text_encoder.projection.parameters() if p.requires_grad]\n",
    "    logit_scale_param = [model.logit_scale] if isinstance(model.logit_scale, nn.Parameter) else []\n",
    "\n",
    "    print(f\"  Param counts (Trainable): VisionBase={len(image_encoder_base_params)}, VisionHead={len(image_head_params)}, TextBase={len(text_encoder_base_params)}, TextHead={len(text_head_params)}, LogitScale={len(logit_scale_param)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": image_encoder_base_params, \"lr\": config.vision_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": image_head_params, \"lr\": config.projection_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": text_encoder_base_params, \"lr\": config.text_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": text_head_params, \"lr\": config.projection_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": logit_scale_param, \"lr\": config.projection_lr, \"weight_decay\": 0}\n",
    "    ]\n",
    "\n",
    "    optimizer_grouped_parameters = [g for g in optimizer_grouped_parameters if g['params']]\n",
    "\n",
    "    if not optimizer_grouped_parameters:\n",
    "         print(\"ERROR: No trainable parameters found for the optimizer.\")\n",
    "    else:\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "        print(f\"Optimizer AdamW initialized.\")\n",
    "\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode=config.mode, factor=config.factor, patience=config.patience\n",
    "        )\n",
    "        print(f\"LR Scheduler ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.factor}, patience={config.patience})\")\n",
    "\n",
    "        early_stopping_counter = 0 # Renamed from config.early_stopping_counter\n",
    "        print(f\"Early stopping initialized (patience={config.early_stopping_patience}, min_delta={config.early_stopping_min_delta})\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not initialized. Skipping optimizer/scheduler setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 40 epochs...\n",
      "Tracking metric: 'avg_acc' (mode: max)\n",
      "\n",
      "--- Epoch 1/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070e03b94c1f4e3fa51601e9d116b6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E1:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05971b8d9db44ccaaa1a07831c756eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E1:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.1891 | avg cosine sim: 0.5564 | i2t acc: 0.2081 | i2t recall R@1: 0.2081 | i2t recall R@5: 0.5489 | i2t recall R@10: 0.6932 | loss: 0.3715 | t2i acc: 0.1702 | t2i recall R@1: 0.1702 | t2i recall R@5: 0.5159 | t2i recall R@10: 0.6632\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from -inf to 0.1891\n",
      "  Saved Best Model (Epoch 1) to ./trained_models/ViCLIP_vivqa/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 1 Time: 357.09 seconds ---\n",
      "\n",
      "--- Epoch 2/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f808f3239934c158c374da4ff3c70bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E2:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.2733\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159484a51e3b40afa5b99f187244e61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E2:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2271 | avg cosine sim: 0.5806 | i2t acc: 0.2470 | i2t recall R@1: 0.2470 | i2t recall R@5: 0.6168 | i2t recall R@10: 0.7472 | loss: 0.3162 | t2i acc: 0.2072 | t2i recall R@1: 0.2072 | t2i recall R@5: 0.5851 | t2i recall R@10: 0.7173\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.1891 to 0.2271\n",
      "  Saved Best Model (Epoch 2) to ./trained_models/ViCLIP_vivqa/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 2 Time: 354.95 seconds ---\n",
      "\n",
      "--- Epoch 3/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fee6e24c6e647a09212df87e750dbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E3:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.1987\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27082a706197462f9b16ecdd10c9edea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E3:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2260 | avg cosine sim: 0.5755 | i2t acc: 0.2430 | i2t recall R@1: 0.2430 | i2t recall R@5: 0.6088 | i2t recall R@10: 0.7383 | loss: 0.3103 | t2i acc: 0.2090 | t2i recall R@1: 0.2090 | t2i recall R@5: 0.6110 | t2i recall R@10: 0.7423\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2271. Counter: 1/5\n",
      "--- Epoch 3 Time: 353.09 seconds ---\n",
      "\n",
      "--- Epoch 4/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1a96e153cb4e5b86fb25c8bd59090b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E4:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.1287\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa1341284594d17905d21bd915087fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E4:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2470 | avg cosine sim: 0.5810 | i2t acc: 0.2604 | i2t recall R@1: 0.2604 | i2t recall R@5: 0.6512 | i2t recall R@10: 0.7745 | loss: 0.2923 | t2i acc: 0.2336 | t2i recall R@1: 0.2336 | t2i recall R@5: 0.6530 | t2i recall R@10: 0.7758\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.2271 to 0.2470\n",
      "  Saved Best Model (Epoch 4) to ./trained_models/ViCLIP_vivqa/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 4 Time: 355.35 seconds ---\n",
      "\n",
      "--- Epoch 5/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ab8d8e4c1447c1b3a9e097f2f37119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E5:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.0907\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166beb34bc944b63b6995f3b23e564fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E5:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2503 | avg cosine sim: 0.6039 | i2t acc: 0.2640 | i2t recall R@1: 0.2640 | i2t recall R@5: 0.6579 | i2t recall R@10: 0.7745 | loss: 0.2895 | t2i acc: 0.2367 | t2i recall R@1: 0.2367 | t2i recall R@5: 0.6722 | t2i recall R@10: 0.7803\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.2470 to 0.2503\n",
      "  Saved Best Model (Epoch 5) to ./trained_models/ViCLIP_vivqa/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 5 Time: 355.18 seconds ---\n",
      "\n",
      "--- Epoch 6/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a0614170544a70b3f88aa608a5a723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E6:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.0759\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b665fb4a6664ca581623280d18c9683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E6:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2588 | avg cosine sim: 0.6065 | i2t acc: 0.2827 | i2t recall R@1: 0.2827 | i2t recall R@5: 0.6771 | i2t recall R@10: 0.7825 | loss: 0.2624 | t2i acc: 0.2349 | t2i recall R@1: 0.2349 | t2i recall R@5: 0.6726 | t2i recall R@10: 0.7896\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.2503 to 0.2588\n",
      "  Saved Best Model (Epoch 6) to ./trained_models/ViCLIP_vivqa/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 6 Time: 354.74 seconds ---\n",
      "\n",
      "--- Epoch 7/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f469e9464e94382a5be907ac57a2d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E7:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.0632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6613b7b65fe544dd9eb6c06820ab61a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E7:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2506 | avg cosine sim: 0.6033 | i2t acc: 0.2684 | i2t recall R@1: 0.2684 | i2t recall R@5: 0.6744 | i2t recall R@10: 0.7896 | loss: 0.2883 | t2i acc: 0.2327 | t2i recall R@1: 0.2327 | t2i recall R@5: 0.6588 | t2i recall R@10: 0.7700\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2588. Counter: 1/5\n",
      "--- Epoch 7 Time: 353.02 seconds ---\n",
      "\n",
      "--- Epoch 8/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13bc9b69ea714593bc3933c1038cc6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E8:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.0561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f6234b1a6e414fbd0d7b144a5fad22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E8:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2693 | avg cosine sim: 0.6217 | i2t acc: 0.2854 | i2t recall R@1: 0.2854 | i2t recall R@5: 0.6869 | i2t recall R@10: 0.7887 | loss: 0.2731 | t2i acc: 0.2532 | t2i recall R@1: 0.2532 | t2i recall R@5: 0.6731 | t2i recall R@10: 0.7968\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.2588 to 0.2693\n",
      "  Saved Best Model (Epoch 8) to ./trained_models/ViCLIP_vivqa/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 8 Time: 355.11 seconds ---\n",
      "\n",
      "--- Epoch 9/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780868b5cf334f1f970ebfecebf542b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E9:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.0561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6353c4546d3e4348a595331bd5f59841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E9:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2691 | avg cosine sim: 0.6197 | i2t acc: 0.2827 | i2t recall R@1: 0.2827 | i2t recall R@5: 0.6807 | i2t recall R@10: 0.7981 | loss: 0.2680 | t2i acc: 0.2555 | t2i recall R@1: 0.2555 | t2i recall R@5: 0.6869 | t2i recall R@10: 0.8106\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2693. Counter: 1/5\n",
      "--- Epoch 9 Time: 353.06 seconds ---\n",
      "\n",
      "--- Epoch 10/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152fa563ac95426395ff27bd0b260b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E10:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.0452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a532cfb9599c488eacefbb0525caa483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E10:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2671 | avg cosine sim: 0.6202 | i2t acc: 0.2814 | i2t recall R@1: 0.2814 | i2t recall R@5: 0.6923 | i2t recall R@10: 0.7950 | loss: 0.2707 | t2i acc: 0.2528 | t2i recall R@1: 0.2528 | t2i recall R@5: 0.6820 | t2i recall R@10: 0.8079\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2693. Counter: 2/5\n",
      "--- Epoch 10 Time: 353.40 seconds ---\n",
      "\n",
      "--- Epoch 11/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43c08e50dd84dcdad11e7fa8bef3587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E11:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 0.0425\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebad114e49724005b8b8e407aee12fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E11:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2745 | avg cosine sim: 0.6258 | i2t acc: 0.2876 | i2t recall R@1: 0.2876 | i2t recall R@5: 0.6842 | i2t recall R@10: 0.7995 | loss: 0.2595 | t2i acc: 0.2613 | t2i recall R@1: 0.2613 | t2i recall R@5: 0.6958 | t2i recall R@10: 0.8017\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' improved from 0.2693 to 0.2745\n",
      "  Saved Best Model (Epoch 11) to ./trained_models/ViCLIP_vivqa/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 11 Time: 357.59 seconds ---\n",
      "\n",
      "--- Epoch 12/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aad0f8163de462ab41527e29b476bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E12:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 0.0383\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834870eebd5249a78ab16f31db267508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E12:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2727 | avg cosine sim: 0.6309 | i2t acc: 0.2832 | i2t recall R@1: 0.2832 | i2t recall R@5: 0.6999 | i2t recall R@10: 0.8008 | loss: 0.2574 | t2i acc: 0.2622 | t2i recall R@1: 0.2622 | t2i recall R@5: 0.7092 | t2i recall R@10: 0.8142\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2745. Counter: 1/5\n",
      "--- Epoch 12 Time: 354.20 seconds ---\n",
      "\n",
      "--- Epoch 13/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeec7818b1a443c0adbce927714bb98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E13:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 0.0348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70512a141f024855882a865886cdd7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E13:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2678 | avg cosine sim: 0.6119 | i2t acc: 0.2814 | i2t recall R@1: 0.2814 | i2t recall R@5: 0.6883 | i2t recall R@10: 0.7937 | loss: 0.2777 | t2i acc: 0.2541 | t2i recall R@1: 0.2541 | t2i recall R@5: 0.6860 | t2i recall R@10: 0.7910\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2745. Counter: 2/5\n",
      "--- Epoch 13 Time: 354.58 seconds ---\n",
      "\n",
      "--- Epoch 14/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843a95db710f431795daae7a08b5e93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E14:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 0.0335\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f73643cc5d4557b883f6018315e193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E14:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2740 | avg cosine sim: 0.6287 | i2t acc: 0.2899 | i2t recall R@1: 0.2899 | i2t recall R@5: 0.6749 | i2t recall R@10: 0.7972 | loss: 0.2705 | t2i acc: 0.2582 | t2i recall R@1: 0.2582 | t2i recall R@5: 0.6896 | t2i recall R@10: 0.7892\n",
      "  Current LRs: VisionBase=1.00e-05, VisionHead=5.00e-05, TextBase=2.00e-05, TextHead=5.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2745. Counter: 3/5\n",
      "--- Epoch 14 Time: 353.26 seconds ---\n",
      "\n",
      "--- Epoch 15/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7792d67a1b3f4e5e827972d766fd5ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E15:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 0.0329\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5397d322d84fee844b37ab97409b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E15:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2624 | avg cosine sim: 0.6258 | i2t acc: 0.2684 | i2t recall R@1: 0.2684 | i2t recall R@5: 0.6802 | i2t recall R@10: 0.7932 | loss: 0.2728 | t2i acc: 0.2564 | t2i recall R@1: 0.2564 | t2i recall R@5: 0.6860 | t2i recall R@10: 0.7923\n",
      "  Current LRs: VisionBase=8.00e-06, VisionHead=4.00e-05, TextBase=1.60e-05, TextHead=4.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2745. Counter: 4/5\n",
      "--- Epoch 15 Time: 352.73 seconds ---\n",
      "\n",
      "--- Epoch 16/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a97640ffb242cd9b5f5e6f31845c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E16:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss = 0.0254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d16cf163d14063b2a958661079c899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E16:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2834 | avg cosine sim: 0.6509 | i2t acc: 0.3010 | i2t recall R@1: 0.3010 | i2t recall R@5: 0.7133 | i2t recall R@10: 0.8209 | loss: 0.2456 | t2i acc: 0.2657 | t2i recall R@1: 0.2657 | t2i recall R@5: 0.7146 | t2i recall R@10: 0.8240\n",
      "  Current LRs: VisionBase=8.00e-06, VisionHead=4.00e-05, TextBase=1.60e-05, TextHead=4.00e-05\n",
      "  Metric 'avg_acc' improved from 0.2745 to 0.2834\n",
      "  Saved Best Model (Epoch 16) to ./trained_models/ViCLIP_vivqa/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 16 Time: 355.32 seconds ---\n",
      "\n",
      "--- Epoch 17/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0149ec0d5164118a85a1ec2af0884d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E17:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss = 0.0246\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86add76615e0423181dee94ac01e8c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E17:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2816 | avg cosine sim: 0.6338 | i2t acc: 0.2921 | i2t recall R@1: 0.2921 | i2t recall R@5: 0.7142 | i2t recall R@10: 0.8200 | loss: 0.2627 | t2i acc: 0.2711 | t2i recall R@1: 0.2711 | t2i recall R@5: 0.7155 | t2i recall R@10: 0.8231\n",
      "  Current LRs: VisionBase=8.00e-06, VisionHead=4.00e-05, TextBase=1.60e-05, TextHead=4.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2834. Counter: 1/5\n",
      "--- Epoch 17 Time: 355.04 seconds ---\n",
      "\n",
      "--- Epoch 18/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebc22aa29e349198f5562f714fd22be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E18:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss = 0.0215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2f2d5e8af94fe2846b47ba071e2f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E18:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2807 | avg cosine sim: 0.6459 | i2t acc: 0.2921 | i2t recall R@1: 0.2921 | i2t recall R@5: 0.7168 | i2t recall R@10: 0.8111 | loss: 0.2616 | t2i acc: 0.2693 | t2i recall R@1: 0.2693 | t2i recall R@5: 0.7177 | t2i recall R@10: 0.8106\n",
      "  Current LRs: VisionBase=8.00e-06, VisionHead=4.00e-05, TextBase=1.60e-05, TextHead=4.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2834. Counter: 2/5\n",
      "--- Epoch 18 Time: 353.06 seconds ---\n",
      "\n",
      "--- Epoch 19/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb8c6f5896a45d7919f99a1d6ef4a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E19:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss = 0.0237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c32e1d21964a3cb32af9b7b7fe228c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E19:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2919 | avg cosine sim: 0.6360 | i2t acc: 0.3086 | i2t recall R@1: 0.3086 | i2t recall R@5: 0.7177 | i2t recall R@10: 0.8173 | loss: 0.2544 | t2i acc: 0.2751 | t2i recall R@1: 0.2751 | t2i recall R@5: 0.7226 | t2i recall R@10: 0.8245\n",
      "  Current LRs: VisionBase=8.00e-06, VisionHead=4.00e-05, TextBase=1.60e-05, TextHead=4.00e-05\n",
      "  Metric 'avg_acc' improved from 0.2834 to 0.2919\n",
      "  Saved Best Model (Epoch 19) to ./trained_models/ViCLIP_vivqa/phobert_blip_retrieval_best.pt\n",
      "--- Epoch 19 Time: 355.28 seconds ---\n",
      "\n",
      "--- Epoch 20/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b18a901c7264a49b771aaa909896865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E20:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss = 0.0217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b7a9013cf0435b969085bc4765333b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E20:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2742 | avg cosine sim: 0.6344 | i2t acc: 0.2845 | i2t recall R@1: 0.2845 | i2t recall R@5: 0.6999 | i2t recall R@10: 0.8062 | loss: 0.2760 | t2i acc: 0.2640 | t2i recall R@1: 0.2640 | t2i recall R@5: 0.7052 | t2i recall R@10: 0.8160\n",
      "  Current LRs: VisionBase=8.00e-06, VisionHead=4.00e-05, TextBase=1.60e-05, TextHead=4.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2919. Counter: 1/5\n",
      "--- Epoch 20 Time: 353.16 seconds ---\n",
      "\n",
      "--- Epoch 21/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a166688f9e4e9da69cbf840a984e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E21:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss = 0.0192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7566168fb3a4a5882ca8f78cacc92c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E21:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2863 | avg cosine sim: 0.6365 | i2t acc: 0.2992 | i2t recall R@1: 0.2992 | i2t recall R@5: 0.7218 | i2t recall R@10: 0.8289 | loss: 0.2592 | t2i acc: 0.2733 | t2i recall R@1: 0.2733 | t2i recall R@5: 0.7101 | t2i recall R@10: 0.8169\n",
      "  Current LRs: VisionBase=8.00e-06, VisionHead=4.00e-05, TextBase=1.60e-05, TextHead=4.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2919. Counter: 2/5\n",
      "--- Epoch 21 Time: 352.92 seconds ---\n",
      "\n",
      "--- Epoch 22/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30b629037a34397a6594a9c0ce4ceaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E22:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss = 0.0193\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f604337ed54378870214aca4077aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E22:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2818 | avg cosine sim: 0.6282 | i2t acc: 0.2983 | i2t recall R@1: 0.2983 | i2t recall R@5: 0.7084 | i2t recall R@10: 0.8187 | loss: 0.2533 | t2i acc: 0.2653 | t2i recall R@1: 0.2653 | t2i recall R@5: 0.7039 | t2i recall R@10: 0.8142\n",
      "  Current LRs: VisionBase=8.00e-06, VisionHead=4.00e-05, TextBase=1.60e-05, TextHead=4.00e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2919. Counter: 3/5\n",
      "--- Epoch 22 Time: 353.03 seconds ---\n",
      "\n",
      "--- Epoch 23/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de068c0a4674c23a94b1ec6f56493f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E23:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss = 0.0200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ad767b95a34a7e925260ab19fc4948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E23:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2778 | avg cosine sim: 0.6430 | i2t acc: 0.2876 | i2t recall R@1: 0.2876 | i2t recall R@5: 0.7146 | i2t recall R@10: 0.8249 | loss: 0.2528 | t2i acc: 0.2680 | t2i recall R@1: 0.2680 | t2i recall R@5: 0.7128 | t2i recall R@10: 0.8160\n",
      "  Current LRs: VisionBase=6.40e-06, VisionHead=3.20e-05, TextBase=1.28e-05, TextHead=3.20e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2919. Counter: 4/5\n",
      "--- Epoch 23 Time: 364.53 seconds ---\n",
      "\n",
      "--- Epoch 24/40 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834e410ee7f34ea4bce9e2cff16452be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E24:   0%|          | 0/1181 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss = 0.0169\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5ef6915df54ab3b3acb2886785f734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E24:   0%|          | 0/140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2239 validation samples...\n",
      "  Validation Metrics:\n",
      "avg acc: 0.2910 | avg cosine sim: 0.6487 | i2t acc: 0.3091 | i2t recall R@1: 0.3091 | i2t recall R@5: 0.7262 | i2t recall R@10: 0.8321 | loss: 0.2455 | t2i acc: 0.2729 | t2i recall R@1: 0.2729 | t2i recall R@5: 0.7244 | t2i recall R@10: 0.8370\n",
      "  Current LRs: VisionBase=6.40e-06, VisionHead=3.20e-05, TextBase=1.28e-05, TextHead=3.20e-05\n",
      "  Metric 'avg_acc' did not improve. Best: 0.2919. Counter: 5/5\n",
      "--- Epoch 24 Time: 356.53 seconds ---\n",
      "\n",
      "Early stopping triggered after 5 epochs without improvement.\n",
      "\n",
      "=============== Training Finished ================\n",
      "Total Training Time: 8516.25 seconds (141.94 minutes)\n",
      "Final epoch model state saved to ./trained_models/ViCLIP_vivqa/phobert_blip_retrieval_final_epoch.pt\n",
      "Best model based on 'avg_acc' (0.2919) is saved at: ./trained_models/ViCLIP_vivqa/phobert_blip_retrieval_best.pt\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "# === Cell 11: Training Loop (No AMP) === MODIFIED ===\n",
    "if model and train_loader and optimizer and lr_scheduler:\n",
    "    print(f\"\\nStarting training for {config.epochs} epochs...\")\n",
    "    print(f\"Tracking metric: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    history = {'train_loss': [], 'validation_results': []}\n",
    "    start_train_time = time.time()\n",
    "    no_improve_epochs = 0 # Use the variable defined in Cell 10\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "\n",
    "        # --- Training ---\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, config.device, epoch+1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_results = {\"loss\": float('inf'), config.metric_to_track.replace('_', ' '): (-float('inf') if config.mode == 'max' else float('inf'))}\n",
    "        if dev_loader:\n",
    "            val_results = validate_epoch(model, dev_loader, config.device, epoch+1)\n",
    "            history['validation_results'].append(val_results)\n",
    "            print(\"  Validation Metrics:\")\n",
    "            metric_log_str = \"  \"\n",
    "            sorted_keys = sorted(val_results.keys(), key=lambda x: (x.split()[0], int(x.split('@')[-1]) if '@' in x else -1))\n",
    "            for name in sorted_keys: metric_log_str += f\"{name}: {val_results[name]:.4f} | \"\n",
    "            print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "            # --- Scheduler Step ---\n",
    "            current_val_metric_for_scheduler = val_results.get(config.metric_to_track.replace('_', ' '), None)\n",
    "            if current_val_metric_for_scheduler is not None:\n",
    "                lr_scheduler.step(current_val_metric_for_scheduler)\n",
    "                current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "                # Updated LR string for this specific setup\n",
    "                lr_str = f\"  Current LRs: VisionBase={current_lrs[0]:.2e}, VisionHead={current_lrs[1]:.2e}, TextBase={current_lrs[2]:.2e}, TextHead={current_lrs[3]:.2e}\"\n",
    "                if len(current_lrs) > 4: lr_str += f\", LogitScale={current_lrs[4]:.2e}\"\n",
    "                print(lr_str)\n",
    "            else: print(f\"  Warning: Metric '{config.metric_to_track}' not found. Scheduler not stepped.\")\n",
    "        else:\n",
    "            print(\"  Validation skipped (no dev_loader).\")\n",
    "            history['validation_results'].append(None)\n",
    "\n",
    "        # --- Save Checkpoint & Early Stopping Logic ---\n",
    "        current_val_metric = val_results.get(config.metric_to_track.replace('_', ' '), -float('inf') if config.mode == \"max\" else float('inf'))\n",
    "        is_best = False\n",
    "        if dev_loader:\n",
    "            if config.mode == \"max\":\n",
    "                if current_val_metric > best_val_metric + config.early_stopping_min_delta: is_best = True\n",
    "            else: # min mode\n",
    "                if current_val_metric < best_val_metric - config.early_stopping_min_delta: is_best = True\n",
    "\n",
    "            if is_best:\n",
    "                print(f\"  Metric '{config.metric_to_track}' improved from {best_val_metric:.4f} to {current_val_metric:.4f}\")\n",
    "                best_val_metric = current_val_metric; no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                print(f\"  Metric '{config.metric_to_track}' did not improve. Best: {best_val_metric:.4f}. Counter: {no_improve_epochs}/{config.early_stopping_patience}\")\n",
    "\n",
    "        # Update checkpoint names\n",
    "        best_checkpoint_path = os.path.join(config.model_path, \"phobert_blip_retrieval_best.pt\")\n",
    "        final_epoch_path = os.path.join(config.model_path, f\"phobert_blip_retrieval_epoch_{epoch+1}.pt\")\n",
    "\n",
    "        # Save necessary configs and state dicts\n",
    "        save_dict = {\n",
    "            'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'train_loss': train_loss, 'validation_results': val_results,\n",
    "            'best_val_metric': best_val_metric, 'metric_tracked': config.metric_to_track,\n",
    "            # Save relevant configs for reloading this specific architecture\n",
    "            'vision_model_name': config.blip_vision_model_name,\n",
    "            'text_model_name': config.selected_text_model,\n",
    "            'projection_dim': config.projection_dim,\n",
    "            'learnable_temperature': config.learnable_temperature\n",
    "        }\n",
    "\n",
    "        if config.save_best_only and dev_loader:\n",
    "            if is_best:\n",
    "                torch.save(save_dict, best_checkpoint_path)\n",
    "                print(f\"  Saved Best Model (Epoch {epoch+1}) to {best_checkpoint_path}\")\n",
    "        else:\n",
    "            torch.save(save_dict, final_epoch_path)\n",
    "            print(f\"  Saved Epoch {epoch+1} Checkpoint to {final_epoch_path}\")\n",
    "            if is_best and dev_loader:\n",
    "                 torch.save(save_dict, best_checkpoint_path)\n",
    "                 print(f\"  (Also saved as best model)\")\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {epoch_end_time - epoch_start_time:.2f} seconds ---\")\n",
    "\n",
    "        if dev_loader and no_improve_epochs >= config.early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {config.early_stopping_patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    end_train_time = time.time()\n",
    "    total_train_time = end_train_time - start_train_time\n",
    "    print(f\"\\n=============== Training Finished ================\")\n",
    "    print(f\"Total Training Time: {total_train_time:.2f} seconds ({total_train_time/60:.2f} minutes)\")\n",
    "    # Update final model path name\n",
    "    final_model_path = os.path.join(config.model_path, 'phobert_blip_retrieval_final_epoch.pt')\n",
    "    torch.save(save_dict, final_model_path)\n",
    "    print(f\"Final epoch model state saved to {final_model_path}\")\n",
    "    best_model_file = os.path.join(config.model_path, \"phobert_blip_retrieval_best.pt\")\n",
    "    if dev_loader and os.path.exists(best_model_file):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) is saved at: {best_model_file}\")\n",
    "    elif dev_loader: print(\"Best model checkpoint file not found.\")\n",
    "    print(f\"=================================================\")\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training (model, dataloader, optimizer, scheduler) not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Starting Test Set Evaluation ===============\n",
      "Loading test data from: ./json_data/test.json\n",
      "Attempting to load data from: /root/TuningModels/json_data/test.json\n",
      "Found 2176 samples in test.json.\n",
      "Using image size: 384x384\n",
      "Test loader created with 136 batches.\n",
      "\n",
      "Loading best model: ./trained_models/ViCLIP_vivqa/phobert_blip_retrieval_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11023/1601218102.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(load_path, map_location=config.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-creating model structure for testing...\n",
      "  Using Vision Source: Salesforce/blip-image-captioning-base\n",
      "  Using Text Model: vinai/phobert-base\n",
      "Initializing BLIP Vision Encoder from: Salesforce/blip-image-captioning-base by loading BlipForImageTextRetrieval first.\n",
      "  Loading base BlipForImageTextRetrieval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipForImageTextRetrieval were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['itm_head.bias', 'itm_head.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.word_embeddings.weight', 'text_proj.bias', 'text_proj.weight', 'vision_proj.bias', 'vision_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracting vision_model from BlipForImageTextRetrieval.\n",
      "  Vision model extracted successfully.\n",
      "  Confirmed/Using vision model hidden size: 768\n",
      "  Added projection head: 768 -> 256\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Confirmed text model hidden size: 768\n",
      "  Added projection head: 768 -> 256\n",
      "Using fixed temperature: 0.07\n",
      "  State dict loading result: <All keys matched successfully>\n",
      "Model weights loaded successfully.\n",
      "\n",
      "Running evaluation on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f2eed54bb443658233e92c8c714ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation ETest:   0%|          | 0/136 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics over 2176 validation samples...\n",
      "\n",
      "--- Test Set Results ---\n",
      "avg acc: 0.2691\n",
      "  avg cosine sim: 0.6170\n",
      "  i2t acc: 0.2826\n",
      "  i2t recall R@1: 0.2826\n",
      "  i2t recall R@5: 0.6792\n",
      "  i2t recall R@10: 0.7886\n",
      "  loss: 0.3843\n",
      "  t2i acc: 0.2555\n",
      "  t2i recall R@1: 0.2555\n",
      "  t2i recall R@5: 0.6820\n",
      "  t2i recall R@10: 0.7794\n",
      "------------------------\n",
      "\n",
      "================= Evaluation Finished ==================\n"
     ]
    }
   ],
   "source": [
    "# === Cell 12: Final Evaluation on Test Set === MODIFIED ===\n",
    "import traceback\n",
    "from types import SimpleNamespace\n",
    "from collections import OrderedDict\n",
    "\n",
    "print(\"\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_loader = None\n",
    "model_to_test = None\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "\n",
    "# 1. Check prerequisites\n",
    "if os.path.exists(test_json_path) and 'tokenizer' in globals() and tokenizer and 'image_processor' in globals() and image_processor:\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    try:\n",
    "        test_dataset = CustomImageCaptionDataset( # Use the custom dataset\n",
    "            json_path=test_json_path, image_base_path=config.image_path,\n",
    "            tokenizer=tokenizer, image_processor=image_processor, # Pass separate components\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "        if test_dataset.data:\n",
    "            num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "                num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "        else: print(\"Test dataset loaded but is empty.\")\n",
    "    except Exception as e: print(f\"Error creating test dataset/loader: {e}\")\n",
    "else: print(\"Skipping test evaluation: Test JSON, Tokenizer or Image Processor not found/loaded.\")\n",
    "\n",
    "# 2. Load Model for Testing if test_loader was created\n",
    "if test_loader:\n",
    "    try:\n",
    "        # Checkpoint names match the ones saved in Cell 11\n",
    "        best_model_path = os.path.join(config.model_path, \"phobert_blip_retrieval_best.pt\")\n",
    "        final_model_path = os.path.join(config.model_path, \"phobert_blip_retrieval_final_epoch.pt\")\n",
    "        load_path = None\n",
    "        if os.path.exists(best_model_path): load_path = best_model_path; print(f\"\\nLoading best model: {load_path}\")\n",
    "        elif os.path.exists(final_model_path): load_path = final_model_path; print(f\"\\nLoading final model: {load_path}\")\n",
    "        else: print(f\"\\nWARNING: No checkpoints found in {config.model_path}.\")\n",
    "\n",
    "        if load_path:\n",
    "            checkpoint = torch.load(load_path, map_location=config.device)\n",
    "            print(\"Re-creating model structure for testing...\")\n",
    "\n",
    "            # --- >>> CORRECTED temp_config_dict CREATION <<< ---\n",
    "            # Use attribute names expected by the Encoder __init__ methods\n",
    "            temp_config_dict = {\n",
    "                # For ImageEncoder\n",
    "                'blip_vision_model_name': checkpoint.get('vision_model_name', config.selected_vision_source),\n",
    "                'vision_embedding': config.vision_embedding, # Get from current config or save/load if needed\n",
    "                # For TextEncoder\n",
    "                'selected_text_model': checkpoint.get('text_model_name', config.selected_text_model),\n",
    "                'text_embedding': config.text_embedding, # Get from current config or save/load if needed\n",
    "                # For BlipRetrievalModel / Loss\n",
    "                'projection_dim': checkpoint.get('projection_dim', config.projection_dim),\n",
    "                'learnable_temperature': checkpoint.get('learnable_temperature', config.learnable_temperature),\n",
    "                'temperature': config.temperature # Use current config temp if not saved\n",
    "            }\n",
    "            # Create a namespace object from the dictionary\n",
    "            temp_config = SimpleNamespace(**temp_config_dict)\n",
    "            # --- >>> END CORRECTION <<< ---\n",
    "\n",
    "            print(f\"  Using Vision Source: {temp_config.blip_vision_model_name}\")\n",
    "            print(f\"  Using Text Model: {temp_config.selected_text_model}\")\n",
    "\n",
    "            # Instantiate using the temporary config object\n",
    "            test_image_encoder = ImageEncoder(temp_config).to(config.device)\n",
    "            test_text_encoder = TextEncoder(temp_config).to(config.device)\n",
    "            model_to_test = CustomBlipPhobertModel(test_image_encoder, test_text_encoder, temp_config).to(config.device)\n",
    "\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "                print(\"Detected 'module.' prefix, removing.\")\n",
    "                state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "\n",
    "            load_result = model_to_test.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"  State dict loading result: {load_result}\")\n",
    "            if load_result.missing_keys: print(f\"  Warning: Missing keys: {load_result.missing_keys}\")\n",
    "            if load_result.unexpected_keys: print(f\"  Warning: Unexpected keys: {load_result.unexpected_keys}\")\n",
    "            print(f\"Model weights loaded successfully.\")\n",
    "\n",
    "            print(\"\\nRunning evaluation on test set...\")\n",
    "            test_results = validate_epoch(model_to_test, test_loader, config.device, epoch_num=\"Test\")\n",
    "\n",
    "            print(\"\\n--- Test Set Results ---\")\n",
    "            metric_log_str = \"\"\n",
    "            sorted_keys = sorted(test_results.keys(), key=lambda x: (x.split()[0], int(x.split('@')[-1]) if '@' in x else -1))\n",
    "            for name in sorted_keys: metric_log_str += f\"  {name}: {test_results[name]:.4f}\\n\"\n",
    "            print(metric_log_str.strip())\n",
    "            print(\"------------------------\")\n",
    "        else: print(\"Evaluation skipped (no weights found).\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR during test setup/evaluation: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n================= Evaluation Finished ==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot directory ensured at: /root/TuningModels/train_plot/ViBLIP_vivqa\n",
      "Saved loss plot to: train_plot/ViBLIP_vivqa/training_loss.png\n",
      "Saved accuracy plot to: train_plot/ViBLIP_vivqa/training_accuracy.png\n",
      "Saved i2t_recall plot to: train_plot/ViBLIP_vivqa/training_i2t_recall.png\n",
      "Saved t2i_recall plot to: train_plot/ViBLIP_vivqa/training_t2i_recall.png\n",
      "Saved combined plot to: train_plot/ViBLIP_vivqa/training_metrics_combined.png\n"
     ]
    }
   ],
   "source": [
    "# === Cell 13: Training Visualization (Adapted) ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plot_dir = \"train_plot/ViBLIP_vivqa\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "print(f\"Plot directory ensured at: {os.path.abspath(plot_dir)}\")\n",
    "\n",
    "def save_subplot_as_figure(subplot, save_path):\n",
    "    fig_new = plt.figure(figsize=(8, 6))\n",
    "    ax_new = fig_new.add_subplot(111)\n",
    "    lines = subplot.get_lines()\n",
    "    if not lines: print(f\"Warning: No lines in subplot for {save_path}\"); plt.close(fig_new); return\n",
    "    labels = [line.get_label() for line in lines]\n",
    "    for line in lines:\n",
    "        ax_new.plot(line.get_xdata(), line.get_ydata(), color=line.get_color(),\n",
    "                    linestyle=line.get_linestyle(), marker=line.get_marker(), label=line.get_label())\n",
    "    ax_new.set_title(subplot.get_title()); ax_new.set_xlabel(subplot.get_xlabel()); ax_new.set_ylabel(subplot.get_ylabel())\n",
    "    ax_new.grid(True)\n",
    "    if any(label and not label.startswith('_') for label in labels): ax_new.legend()\n",
    "    plt.tight_layout()\n",
    "    fig_new.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig_new)\n",
    "\n",
    "def plot_training_metrics(history):\n",
    "    if not history or not history.get('train_loss') or not history.get('validation_results'):\n",
    "        print(\"No/incomplete training history available.\"); return\n",
    "    valid_results = [res for res in history['validation_results'] if res is not None]\n",
    "    if not valid_results:\n",
    "        print(\"No valid validation results. Plotting only training loss.\")\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "        ax.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')\n",
    "        ax.set_title('Training Loss over Epochs'); ax.set_xlabel('Epoch'); ax.set_ylabel('Loss'); ax.legend(); ax.grid(True)\n",
    "        save_path = os.path.join(plot_dir, 'training_loss.png')\n",
    "        fig.savefig(save_path, bbox_inches='tight', dpi=300); print(f\"Saved loss plot to: {save_path}\"); plt.close(fig)\n",
    "        return\n",
    "\n",
    "    num_epochs_trained = len(history['train_loss']); num_epochs_validated = len(valid_results)\n",
    "    epochs_train = range(1, num_epochs_trained + 1); epochs_val = range(1, num_epochs_validated + 1)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 13))\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16, y=1.02)\n",
    "\n",
    "    val_loss = [res.get('loss', float('nan')) for res in valid_results]\n",
    "    axes[0, 0].plot(epochs_train, history['train_loss'], 'b-o', label='Training Loss')\n",
    "    axes[0, 0].plot(epochs_val, val_loss, 'r-s', label='Validation Loss')\n",
    "    axes[0, 0].set_title('Loss'); axes[0, 0].set_xlabel('Epoch'); axes[0, 0].set_ylabel('Loss'); axes[0, 0].legend(); axes[0, 0].grid(True)\n",
    "\n",
    "    metric_key_acc = 'avg acc'\n",
    "    if metric_key_acc in valid_results[0]:\n",
    "        val_acc = [res[metric_key_acc] for res in valid_results]\n",
    "        axes[0, 1].plot(epochs_val, val_acc, 'g-^', label='Avg Accuracy (Val)')\n",
    "        axes[0, 1].set_title('Avg Accuracy'); axes[0, 1].set_xlabel('Epoch'); axes[0, 1].set_ylabel('Accuracy'); axes[0, 1].legend(); axes[0, 1].grid(True)\n",
    "    else: axes[0, 1].set_title(f'{metric_key_acc} (Not Found)')\n",
    "\n",
    "    has_recall = 'i2t recall R@1' in valid_results[0]\n",
    "    if has_recall:\n",
    "        for k in [1, 5, 10]:\n",
    "            axes[1, 0].plot(epochs_val, [res.get(f'i2t recall R@{k}', float('nan')) for res in valid_results], marker='o', label=f'I2T R@{k}')\n",
    "        axes[1, 0].set_title('I2T Recall (Val)'); axes[1, 0].set_xlabel('Epoch'); axes[1, 0].set_ylabel('Recall'); axes[1, 0].legend(); axes[1, 0].grid(True)\n",
    "        for k in [1, 5, 10]:\n",
    "            axes[1, 1].plot(epochs_val, [res.get(f't2i recall R@{k}', float('nan')) for res in valid_results], marker='s', label=f'T2I R@{k}')\n",
    "        axes[1, 1].set_title('T2I Recall (Val)'); axes[1, 1].set_xlabel('Epoch'); axes[1, 1].set_ylabel('Recall'); axes[1, 1].legend(); axes[1, 1].grid(True)\n",
    "    else: axes[1, 0].set_title('I2T Recall (Not Found)'); axes[1, 1].set_title('T2I Recall (Not Found)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plot_names = ['loss', 'accuracy', 'i2t_recall', 't2i_recall']\n",
    "    for idx, name in enumerate(plot_names):\n",
    "        i, j = divmod(idx, 2); save_path = os.path.join(plot_dir, f'training_{name}.png')\n",
    "        if axes[i, j].has_data(): save_subplot_as_figure(axes[i, j], save_path); print(f\"Saved {name} plot to: {save_path}\")\n",
    "        else: print(f\"Skipping save for {name} plot (no data).\")\n",
    "    combined_save_path = os.path.join(plot_dir, 'training_metrics_combined.png')\n",
    "    fig.savefig(combined_save_path, bbox_inches='tight', dpi=300); print(f\"Saved combined plot to: {combined_save_path}\")\n",
    "    # plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "if 'history' in locals() and isinstance(history, dict) and history.get('train_loss'):\n",
    "    plot_training_metrics(history)\n",
    "else:\n",
    "    print(\"No training history found. Run training first.\")\n",
    "\n",
    "# --- END OF SCRIPT ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
