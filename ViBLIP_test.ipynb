{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "Transformers Version: 4.50.0\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ViBLIP Fine-tuning for Vietnamese Image Retrieval by Text\n",
    "# Generated from Jupyter Notebook\n",
    "\n",
    "\n",
    "# === Cell 1: Installs and Imports ===\n",
    "# !pip install -q transformers torch torchvision torchaudio Pillow tqdm accelerate bitsandbytes sentencepiece\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Blip2Processor, Blip2Model, Blip2Config, AutoTokenizer # AutoTokenizer might be needed for processor loading edge cases\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm # Use standard tqdm if not in notebook: from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time # For timing epochs\n",
    "import transformers\n",
    "from torch.cuda.amp import GradScaler, autocast # For mixed precision\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model output path: ./ViBLIP_vivqa\n",
      "Selected BLIP2 Model: Salesforce/blip2-flan-t5-xl\n",
      "Image base path (for resolving paths in JSON): /home/researcher/huypq69/TuningModels/data/OpenViVQA-dataset\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Configuration Class (CFG) ===\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    # Base directory where your train.json, dev.json, test.json are located\n",
    "    data_path = \"./json_data/\"\n",
    "    image_path = \"./data/OpenViVQA-dataset/\"\n",
    "\n",
    "    # Output directory for saved models\n",
    "    model_path = \"./ViBLIP_vivqa\"\n",
    "\n",
    "    # --- BLIP Model Selection ---\n",
    "    # Common options: Salesforce/blip2-opt-2.7b, Salesforce/blip2-flan-t5-xl\n",
    "    # Choose based on available resources. OPT models might be slightly smaller.\n",
    "    selected_blip_model = \"Salesforce/blip2-flan-t5-xl\" # Smaller model to avoid CUDA OOM errors\n",
    "\n",
    "    # --- Model parameters ---\n",
    "    blip_model_name = selected_blip_model\n",
    "    blip_processor_name = selected_blip_model\n",
    "    projection_dim = 256 # Shared latent space dimension (e.g., 256, 512, 768)\n",
    "    # Freeze parts of the model? BLIP2's LLM is very large.\n",
    "    freeze_vision_model = False\n",
    "    freeze_language_model = True\n",
    "    freeze_qformer = False\n",
    "    # Quantization (Requires bitsandbytes). Reduces memory significantly.\n",
    "    # Set to False to avoid precision mismatches\n",
    "    load_in_8bit = False \n",
    "\n",
    "    # --- Training parameters ---\n",
    "    seed = 42\n",
    "    # Reduce batch size significantly compared to CLIP due to BLIP2's size\n",
    "    batch_size = 2  # START LOW (e.g., 4, 8, 16) and increase based on GPU memory\n",
    "    num_workers = 4  # Adjust based on system capability\n",
    "    # Learning rates for different components (tune these)\n",
    "    vision_encoder_lr = 1e-5\n",
    "    qformer_lr = 2e-5\n",
    "    language_model_lr = 1e-6 # Only relevant if freeze_language_model=False\n",
    "    projection_lr = 1e-4 # Projection head can often learn faster\n",
    "    weight_decay = 1e-3\n",
    "    patience = 3 # Scheduler patience\n",
    "    factor = 0.8 # Scheduler reduction factor\n",
    "    epochs = 1 # Adjust as needed\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Use mixed precision to save memory and potentially speed up training\n",
    "    use_amp = False\n",
    "\n",
    "    # --- Image/Text parameters (mostly handled by processor) ---\n",
    "    # Processor determines image size (usually 224 for BLIP2)\n",
    "    max_length = 200 # Max text sequence length for tokenizer\n",
    "\n",
    "    # --- Loss/Saving parameters ---\n",
    "    temperature = 0.07 # Initial temperature for scaling logits\n",
    "    learnable_temperature = True # Whether the logit_scale is learnable\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"avg_acc\" # Common retrieval metric ('avg_acc', 'i2t R@1', 't2i R@1', 'avg_R@5', etc.)\n",
    "    mode = \"max\" # Mode for scheduler/saving based on metric_to_track ('max' for recall/acc, 'min' for loss)\n",
    "    early_stopping_patience = 5 # Epochs with no improvement before stopping\n",
    "    early_stopping_min_delta = 0.001 # Minimum change to qualify as improvement\n",
    "\n",
    "    accumulation_steps = 8 \n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Selected BLIP2 Model: {config.blip_model_name}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: Seeding for Reproducibility ===\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # For multi-GPU\n",
    "        # torch.backends.cudnn.deterministic = True # Can impact performance\n",
    "        # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Metric Calculation Utilities ===\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        if torch.is_tensor(val):\n",
    "             val = val.item()\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    \"\"\"Calculates Recall@k for image-text retrieval.\"\"\"\n",
    "    n = similarity_matrix.shape[1-dim] # Number of samples (e.g., images if dim=1 for T2I)\n",
    "    k_eff = min(k, similarity_matrix.shape[dim]) # Effective k cannot be larger than candidate pool size\n",
    "    if k_eff == 0 or n == 0: return 0.0\n",
    "\n",
    "    top_k_indices = torch.topk(similarity_matrix, k_eff, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "\n",
    "    correct_count = 0\n",
    "    if dim == 0: # I2T: Find correct text (row index) for each image (column)\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]:\n",
    "                correct_count += 1\n",
    "    elif dim == 1: # T2I: Find correct image (column index) for each text (row)\n",
    "        for txt_idx in range(n):\n",
    "             if ground_truth[txt_idx] in top_k_indices[txt_idx, :]:\n",
    "                correct_count += 1\n",
    "    else:\n",
    "        raise ValueError(\"dim must be 0 or 1\")\n",
    "\n",
    "    return correct_count / n if n > 0 else 0.0\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    \"\"\"Computes retrieval metrics for a batch or validation set.\"\"\"\n",
    "    if image_embeddings.device != text_embeddings.device:\n",
    "        text_embeddings = text_embeddings.to(image_embeddings.device)\n",
    "\n",
    "    sim_matrix = text_embeddings @ image_embeddings.T\n",
    "    sim_matrix = sim_matrix.float()\n",
    "    n = sim_matrix.shape[0]\n",
    "    if n == 0:\n",
    "        return {\n",
    "            \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "            \"avg_cosine_sim\": 0.0,\n",
    "            \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"avg_R@1\": 0.0, \"avg_R@5\": 0.0, \"avg_R@10\": 0.0\n",
    "        }\n",
    "\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "    t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "    t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "    avg_acc = (i2t_acc + t2i_acc) / 2\n",
    "    avg_cosine_sim = torch.diagonal(sim_matrix).mean().item()\n",
    "\n",
    "    i2t_recall = {}\n",
    "    t2i_recall = {}\n",
    "    recall_k_values = [1, 5, 10]\n",
    "\n",
    "    for k in recall_k_values:\n",
    "        k_str = f\"R@{k}\"\n",
    "        i2t_recall[k_str] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "        t2i_recall[k_str] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "\n",
    "    avg_recall = {}\n",
    "    for k in recall_k_values:\n",
    "        k_str = f\"R@{k}\"\n",
    "        avg_recall[f\"avg_{k_str}\"] = (i2t_recall[k_str] + t2i_recall[k_str]) / 2\n",
    "\n",
    "    metrics = {\n",
    "        \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "        \"avg_cosine_sim\": avg_cosine_sim,\n",
    "        \"i2t_recall\": i2t_recall,\n",
    "        \"t2i_recall\": t2i_recall,\n",
    "        **avg_recall\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blip2ImageCaptionDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Dataset Class Definition ===\n",
    "class Blip2ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, processor, max_length):\n",
    "        super().__init__()\n",
    "        print(f\"Attempting to load data from: {os.path.abspath(json_path)}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                self.data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: JSON file not found at {json_path}\")\n",
    "            self.data = []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error: Could not decode JSON from {json_path}: {e}\")\n",
    "            self.data = []\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred loading {json_path}: {e}\")\n",
    "            self.data = []\n",
    "\n",
    "        print(f\"Found {len(self.data)} samples in {os.path.basename(json_path)}.\")\n",
    "        self.image_base_path = image_base_path\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Determine image size from processor\n",
    "        try:\n",
    "            # Try accessing size directly (newer transformers)\n",
    "            if isinstance(self.processor.image_processor.size, dict):\n",
    "                self.img_size = self.processor.image_processor.size['height'] # Or 'shortest_edge'\n",
    "            else: # Older style might be int or tuple\n",
    "                 self.img_size = self.processor.image_processor.size\n",
    "                 if isinstance(self.img_size, (tuple, list)): self.img_size = self.img_size[0]\n",
    "        except AttributeError:\n",
    "            print(\"Warning: Could not determine image size from processor, defaulting to 224.\")\n",
    "            self.img_size = 224\n",
    "        print(f\"Using image size: {self.img_size}x{self.img_size}\")\n",
    "\n",
    "        if not os.path.isdir(self.image_base_path):\n",
    "             print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data):\n",
    "             raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path')\n",
    "        captions = item.get('caption', [])\n",
    "        caption = captions[0] if captions else \"\" # Take the first caption\n",
    "\n",
    "        # Initialize with dummy data\n",
    "        dummy_image = Image.new('RGB', (self.img_size, self.img_size))\n",
    "        try:\n",
    "            pixel_values = self.processor(images=dummy_image, return_tensors=\"pt\")['pixel_values'].squeeze()\n",
    "        except Exception as e:\n",
    "             print(f\"Error processing dummy image: {e}\")\n",
    "             pixel_values = torch.zeros((3, self.img_size, self.img_size)) # Fallback tensor\n",
    "\n",
    "        image_loaded_successfully = False\n",
    "        if relative_image_path:\n",
    "            image_path = os.path.normpath(os.path.join(self.image_base_path, relative_image_path))\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                processed_output = self.processor(images=image, text=None, return_tensors=\"pt\") # Only process image here\n",
    "                pixel_values = processed_output['pixel_values'].squeeze()\n",
    "                image_loaded_successfully = True\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Img not found at {image_path}. Using dummy image for idx {idx}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading image {image_path}: {e}. Using dummy image for idx {idx}.\")\n",
    "        else:\n",
    "             print(f\"Warning: Missing 'image_path' for item at index {idx}. Using dummy image.\")\n",
    "\n",
    "        # Process text\n",
    "        try:\n",
    "            text_inputs = self.processor(\n",
    "                images=None, # Important: don't re-process image\n",
    "                text=caption,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = text_inputs['input_ids'].squeeze()\n",
    "            attention_mask = text_inputs['attention_mask'].squeeze()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text '{caption}' for idx {idx}: {e}\")\n",
    "            # Create dummy text inputs if error occurs\n",
    "            input_ids = torch.zeros(self.max_length, dtype=torch.long)\n",
    "            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n",
    "\n",
    "        # Ensure tensors are 1D after squeeze\n",
    "        if input_ids.dim() > 1: input_ids = input_ids.view(-1)\n",
    "        if attention_mask.dim() > 1: attention_mask = attention_mask.view(-1)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "print(\"Blip2ImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP2 Retrieval Model and Loss Function defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Model Definition (BLIP2 Retrieval Model, Loss) ===\n",
    "import bitsandbytes.optim as bnb_optim # Make sure this is imported if using 8-bit optimizer\n",
    "# from peft import prepare_model_for_kbit_training # Needed if using gradient checkpointing with 8-bit\n",
    "\n",
    "class Blip2RetrievalModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config_train = config # Store training config\n",
    "        print(f\"Initializing BLIP2 Model: {config.blip_model_name}\")\n",
    "\n",
    "        # Load the base BLIP2 model.\n",
    "        try:\n",
    "            load_kwargs = {}\n",
    "            if config.load_in_8bit:\n",
    "                 if not torch.cuda.is_available() or torch.cuda.get_device_capability()[0] < 7:\n",
    "                    print(\"Warning: 8-bit loading requested but not supported on this GPU or CUDA version. Loading in default precision.\")\n",
    "                 else:\n",
    "                    print(\"Attempting to load model in 8-bit.\")\n",
    "                    load_kwargs['load_in_8bit'] = True\n",
    "                    load_kwargs['device_map'] = 'auto' # device_map needed for 8-bit\n",
    "\n",
    "            self.blip_model = Blip2Model.from_pretrained(\n",
    "                config.blip_model_name,\n",
    "                **load_kwargs\n",
    "            )\n",
    "\n",
    "            # If not using device_map, explicitly move model parts if needed\n",
    "            if 'device_map' not in load_kwargs and config.device != torch.device('cpu'):\n",
    "                print(f\"Manually moving model components to {config.device}\")\n",
    "                self.blip_model.to(config.device)\n",
    "            elif 'device_map' in load_kwargs:\n",
    "                print(f\"Model loaded with device_map: {self.blip_model.hf_device_map}\")\n",
    "\n",
    "        except ImportError as e:\n",
    "             if 'bitsandbytes' in str(e):\n",
    "                 print(\"ERROR: bitsandbytes library not found. Please install it (`pip install bitsandbytes`) to use 8-bit loading.\")\n",
    "             else:\n",
    "                 print(f\"ERROR loading base BLIP2 model: {e}\")\n",
    "             raise e\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading base BLIP2 model: {e}\")\n",
    "            print(\"Check model name, internet connection, and available memory.\")\n",
    "            raise e\n",
    "\n",
    "        # --- Freeze Components ---\n",
    "        if config.freeze_vision_model:\n",
    "            print(\"  Freezing Vision Model parameters.\")\n",
    "            for param in self.blip_model.vision_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        if config.freeze_qformer:\n",
    "            print(\"  Freezing Q-Former parameters.\")\n",
    "            for param in self.blip_model.qformer.parameters():\n",
    "                param.requires_grad = False\n",
    "        if config.freeze_language_model:\n",
    "            if hasattr(self.blip_model, 'language_model') and self.blip_model.language_model is not None:\n",
    "                 print(\"  Freezing Language Model parameters.\")\n",
    "                 for param in self.blip_model.language_model.parameters():\n",
    "                    param.requires_grad = False\n",
    "                 # Also freeze the language projection if it exists (maps Q-Former to LM input)\n",
    "                 if hasattr(self.blip_model, 'language_projection') and self.blip_model.language_projection is not None:\n",
    "                     for param in self.blip_model.language_projection.parameters():\n",
    "                         param.requires_grad = False\n",
    "            else:\n",
    "                 print(\"  Language model component not found or is None, skipping freeze.\")\n",
    "\n",
    "        # Determine input dimension for projection heads (usually Q-Former output dim)\n",
    "        qformer_hidden_size = self.blip_model.config.qformer_config.hidden_size\n",
    "        print(f\"  Q-Former hidden size (input to projection): {qformer_hidden_size}\")\n",
    "\n",
    "        # --- Projection Heads ---\n",
    "        self.image_projection = nn.Linear(qformer_hidden_size, config.projection_dim, bias=False)\n",
    "        self.text_projection = nn.Linear(qformer_hidden_size, config.projection_dim, bias=False)\n",
    "        print(f\"  Added projection heads: {qformer_hidden_size} -> {config.projection_dim}\")\n",
    "\n",
    "        # --- Learnable Temperature ---\n",
    "        if config.learnable_temperature:\n",
    "            self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / config.temperature))\n",
    "            print(f\"  Using learnable temperature (logit scale), initialized to {self.logit_scale.exp().item():.4f}\")\n",
    "        else:\n",
    "            # Store as a buffer if not learnable, ensure it's moved to device later if needed\n",
    "            self.register_buffer('logit_scale_fixed', torch.tensor(np.log(1 / config.temperature)))\n",
    "            print(f\"  Using fixed temperature: {config.temperature}\")\n",
    "\n",
    "        # Move projections to the correct device if model wasn't loaded with device_map\n",
    "        model_device = config.device # Get device from config\n",
    "        if 'device_map' not in getattr(self.blip_model, 'hf_device_map', {}):\n",
    "            self.image_projection.to(model_device)\n",
    "            self.text_projection.to(model_device)\n",
    "        # Move logit_scale parameter explicitly IF it's a parameter and device_map wasn't used\n",
    "        if config.learnable_temperature and 'device_map' not in getattr(self.blip_model, 'hf_device_map', {}):\n",
    "            self.logit_scale.to(model_device)\n",
    "        elif not config.learnable_temperature: # Move the fixed buffer\n",
    "             self.logit_scale_fixed = self.logit_scale_fixed.to(model_device)\n",
    "\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # Use the main forward pass and provide dummy decoder inputs\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        device = pixel_values.device\n",
    "\n",
    "        decoder_start_token_id = self.blip_model.config.decoder_start_token_id\n",
    "        if decoder_start_token_id is None:\n",
    "             try: decoder_start_token_id = self.blip_model.config.text_config.bos_token_id\n",
    "             except AttributeError:\n",
    "                  print(\"Warning: Could not reliably determine decoder_start_token_id from config. Defaulting based on model type (e.g., 1 for OPT, 0 for T5). Check!\")\n",
    "                  # Heuristic: T5 uses 0 (pad), OPT uses 1 or 2 (bos)\n",
    "                  if \"t5\" in self.config_train.blip_model_name.lower():\n",
    "                      decoder_start_token_id = 0\n",
    "                  else:\n",
    "                      decoder_start_token_id = 1 # Assuming OPT-like\n",
    "\n",
    "        decoder_input_ids = torch.full((batch_size, 1), decoder_start_token_id, dtype=torch.long, device=device)\n",
    "        decoder_attention_mask = torch.ones((batch_size, 1), dtype=torch.long, device=device)\n",
    "\n",
    "        # Call the main forward method of Blip2Model\n",
    "        outputs = self.blip_model(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        # Extract Q-Former features\n",
    "        image_embeds_qformer = outputs.get(\"image_embeds\")\n",
    "        text_embeds_qformer = outputs.get(\"text_embeds\")\n",
    "\n",
    "        if image_embeds_qformer is None or text_embeds_qformer is None:\n",
    "             print(\"Warning: 'image_embeds' or 'text_embeds' not found in output. Trying qformer_outputs.last_hidden_state.\")\n",
    "             if outputs.qformer_outputs is None or outputs.qformer_outputs.last_hidden_state is None:\n",
    "                 raise ValueError(\"Cannot extract Q-Former features from model output.\")\n",
    "             qformer_last_hidden_state = outputs.qformer_outputs.last_hidden_state\n",
    "             image_features = qformer_last_hidden_state[:, 0, :]\n",
    "             text_features = qformer_last_hidden_state[:, 0, :] # Assumption if text_embeds missing\n",
    "        else:\n",
    "            image_features = image_embeds_qformer[:, 0, :]\n",
    "            text_features = text_embeds_qformer[:, 0, :]\n",
    "\n",
    "        # --- Projection and Normalization ---\n",
    "        image_embeds = self.image_projection(image_features)\n",
    "        text_embeds = self.text_projection(text_features)\n",
    "\n",
    "        image_embeds_norm = F.normalize(image_embeds, p=2, dim=-1)\n",
    "        text_embeds_norm = F.normalize(text_embeds, p=2, dim=-1)\n",
    "\n",
    "        # --- Logit Calculation ---\n",
    "        # Cast embeddings to FP32 before matmul for stability\n",
    "        image_embeds_norm = image_embeds_norm.float()\n",
    "        text_embeds_norm = text_embeds_norm.float()\n",
    "\n",
    "        if self.config_train.learnable_temperature:\n",
    "             current_logit_scale = self.logit_scale.exp()\n",
    "        else:\n",
    "             current_logit_scale = self.logit_scale_fixed.exp().to(image_embeds_norm.device)\n",
    "\n",
    "        # Ensure scale is also FP32\n",
    "        logits_per_image = current_logit_scale.float() * image_embeds_norm @ text_embeds_norm.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        # Return original dtype embeddings for potential metric use, but FP32 logits for loss\n",
    "        return logits_per_image, logits_per_text, image_embeds_norm.to(image_embeds.dtype), text_embeds_norm.to(text_embeds.dtype)\n",
    "\n",
    "\n",
    "# --- Loss Function (Contrastive Loss) ---\n",
    "# Explicitly cast inputs to float32\n",
    "def contrastive_loss(logits_per_image, logits_per_text):\n",
    "    logits_per_image = logits_per_image.float()\n",
    "    logits_per_text = logits_per_text.float()\n",
    "\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    if batch_size == 0: return torch.tensor(0.0, device=logits_per_image.device, requires_grad=True)\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "    loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "    total_loss = (loss_img + loss_txt) / 2.0\n",
    "    return total_loss\n",
    "\n",
    "print(\"BLIP2 Retrieval Model and Loss Function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Training and Validation Epoch Functions (Fix for precision mismatch) ===\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch_num):\n",
    "    model.train()\n",
    "    loss_meter = AvgMeter(f\"Train Loss E{epoch_num}\")\n",
    "    try:\n",
    "        from tqdm.notebook import tqdm as tqdm_notebook\n",
    "        progress_bar = tqdm_notebook(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    except ImportError:\n",
    "        from tqdm import tqdm\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "\n",
    "    optimizer.zero_grad() # Zero gradients before accumulation loop\n",
    "\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_size_actual = pixel_values.size(0)\n",
    "        if batch_size_actual == 0: continue\n",
    "\n",
    "        # Cast inputs to float32 to avoid precision issues with 8-bit models\n",
    "        pixel_values = pixel_values.to(torch.float32)\n",
    "        \n",
    "        # Disable autocast entirely\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            logits_per_image, logits_per_text, _, _ = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "        # Cast logits to FP32 before loss\n",
    "        logits_per_image_fp32 = logits_per_image.float()\n",
    "        logits_per_text_fp32 = logits_per_text.float()\n",
    "\n",
    "        # Calculate loss using FP32 logits and divide by accumulation steps\n",
    "        loss = contrastive_loss(logits_per_image_fp32, logits_per_text_fp32) / config.accumulation_steps\n",
    "\n",
    "        # Standard backward pass (No scaler needed)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer Step logic for gradient accumulation\n",
    "        if (i + 1) % config.accumulation_steps == 0 or (i + 1) == len(dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Update meter with the un-normalized loss for correct reporting\n",
    "        loss_meter.update(loss.item() * config.accumulation_steps, batch_size_actual)\n",
    "        progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    optimizer.zero_grad() # Clean up at end of epoch\n",
    "    return loss_meter.avg\n",
    "\n",
    "\n",
    "# Fixed validate_epoch function to handle precision issues\n",
    "def validate_epoch(model, dataloader, device, epoch_num):\n",
    "    model.eval()\n",
    "    loss_meter = AvgMeter(f\"Val Loss E{epoch_num}\")\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    try:\n",
    "        from tqdm.notebook import tqdm as tqdm_notebook\n",
    "        progress_bar = tqdm_notebook(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    except ImportError:\n",
    "        from tqdm import tqdm\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = pixel_values.size(0)\n",
    "            if batch_size == 0: continue\n",
    "\n",
    "            # Cast inputs to float32 to avoid precision issues \n",
    "            pixel_values = pixel_values.to(torch.float32)\n",
    "            \n",
    "            # Disable autocast entirely\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                logits_per_image, logits_per_text, image_embeds, text_embeds = model(\n",
    "                    pixel_values, input_ids, attention_mask)\n",
    "\n",
    "            # Cast logits before loss\n",
    "            logits_per_image_fp32 = logits_per_image.float()\n",
    "            logits_per_text_fp32 = logits_per_text.float()\n",
    "\n",
    "            # Calculate loss using FP32 logits\n",
    "            loss = contrastive_loss(logits_per_image_fp32, logits_per_text_fp32)\n",
    "\n",
    "            loss_meter.update(loss.item(), batch_size)\n",
    "\n",
    "            # Store original precision embeddings\n",
    "            all_image_embeddings.append(image_embeds.cpu())\n",
    "            all_text_embeddings.append(text_embeds.cpu())\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    # Concatenate all embeddings\n",
    "    if not all_image_embeddings or not all_text_embeddings:\n",
    "        print(\"Warning: No embeddings collected during validation.\")\n",
    "        zero_metrics = { \n",
    "            \"loss\": loss_meter.avg, \n",
    "            \"avg_acc\": 0.0, \n",
    "            \"avg_cosine_sim\": 0.0,\n",
    "            \"i2t R@1\": 0.0, \"i2t R@5\": 0.0, \"i2t R@10\": 0.0,\n",
    "            \"t2i R@1\": 0.0, \"t2i R@5\": 0.0, \"t2i R@10\": 0.0,\n",
    "            \"avg R@1\": 0.0, \"avg R@5\": 0.0, \"avg R@10\": 0.0 \n",
    "        }\n",
    "        return {k.replace('_', ' '): v for k,v in zero_metrics.items()}\n",
    "\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "\n",
    "    # Compute metrics over the entire validation set\n",
    "    print(f\"\\nComputing metrics over {all_image_embeddings.shape[0]} validation samples...\")\n",
    "    validation_metrics = compute_metrics(all_image_embeddings.to(device), all_text_embeddings.to(device))\n",
    "\n",
    "    # Combine loss with computed metrics and flatten recall dicts\n",
    "    final_results = {\"loss\": loss_meter.avg}\n",
    "    for k, v in validation_metrics.items():\n",
    "        if isinstance(v, dict):\n",
    "            for recall_k, recall_v in v.items():\n",
    "                final_results[f\"{k.replace('_', ' ')} {recall_k}\"] = recall_v\n",
    "        else:\n",
    "            final_results[k.replace('_', ' ')] = v\n",
    "\n",
    "    return final_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP2 Processor: Salesforce/blip2-flan-t5-xl\n",
      "Processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8: Setup - BLIP2 Processor ===\n",
    "print(f\"Loading BLIP2 Processor: {config.blip_processor_name}\")\n",
    "try:\n",
    "    processor = Blip2Processor.from_pretrained(config.blip_processor_name)\n",
    "    print(\"Processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading processor '{config.blip_processor_name}': {e}\")\n",
    "    processor = None # Ensure processor is None if loading fails\n",
    "    # raise e # Optionally stop execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/json_data/train.json\n",
      "Found 18899 samples in train.json.\n",
      "Using image size: 224x224\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/json_data/dev.json\n",
      "Found 2239 samples in dev.json.\n",
      "Using image size: 224x224\n",
      "\n",
      "Creating dataloaders...\n",
      "Using 12 workers for DataLoaders.\n",
      "Train loader created with 18899 batches.\n",
      "Validation loader created with 2239 batches.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 9: Setup - Datasets and DataLoaders ===\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "\n",
    "if processor:\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_json = os.path.join(config.data_path, \"train.json\")\n",
    "    dev_json = os.path.join(config.data_path, \"dev.json\")\n",
    "\n",
    "    train_dataset = Blip2ImageCaptionDataset(\n",
    "        json_path=train_json,\n",
    "        image_base_path=config.image_path,\n",
    "        processor=processor,\n",
    "        max_length=config.max_length\n",
    "    )\n",
    "    dev_dataset = Blip2ImageCaptionDataset(\n",
    "        json_path=dev_json,\n",
    "        image_base_path=config.image_path,\n",
    "        processor=processor,\n",
    "        max_length=config.max_length\n",
    "    )\n",
    "\n",
    "    if not train_dataset.data:\n",
    "        print(\"\\nERROR: Failed to load training data. Check 'train_json' path and format.\")\n",
    "    if not dev_dataset.data:\n",
    "         print(\"\\nWARNING: Failed to load validation data. Validation steps will be skipped or may error.\")\n",
    "\n",
    "    print(\"\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    if train_dataset.data:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=True # Drop last incomplete batch for more stable training steps\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "    else:\n",
    "        print(\"Skipping train loader creation due to missing training data.\")\n",
    "\n",
    "    if dev_dataset.data:\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False # Keep last batch for full validation\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "    else:\n",
    "        print(\"Skipping validation loader creation due to missing validation data.\")\n",
    "\n",
    "    if not train_loader:\n",
    "         print(\"\\nERROR: Train loader could not be created. Cannot proceed with training.\")\n",
    "\n",
    "else:\n",
    "     print(\"ERROR: BLIP2 Processor not loaded. Skipping dataset and dataloader creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model components...\n",
      "Initializing BLIP2 Model: Salesforce/blip2-flan-t5-xl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88717d0e2f7640e9a6af20ab8782da4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually moving model components to cuda\n",
      "  Freezing Language Model parameters.\n",
      "  Q-Former hidden size (input to projection): 768\n",
      "  Added projection heads: 768 -> 256\n",
      "  Using learnable temperature (logit scale), initialized to 14.2857\n",
      "\n",
      "Blip2RetrievalModel initialized.\n",
      "Total parameters: 3942.84 M\n",
      "Trainable parameters: 1091.51 M\n",
      "\n",
      "Setting up optimizer...\n",
      "  Param counts (Trainable): Vision=474, QFormer=254, LM=0 (FROZEN), Projection=3\n",
      "Optimizer AdamW initialized.\n",
      "LR Scheduler ReduceLROnPlateau initialized (mode='max', factor=0.8, patience=3)\n",
      "Early stopping initialized (patience=5, min_delta=0.001)\n",
      "Note: AMP/Mixed precision disabled to avoid precision mismatches.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10: Setup - Model, Optimizer, Scheduler, AMP Scaler (with precision fix) ===\n",
    "model = None\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "scaler = None\n",
    "\n",
    "print(\"\\nInitializing model components...\")\n",
    "try:\n",
    "    # Pass the config object to the model but ensure load_in_8bit is False\n",
    "    config.load_in_8bit = False  # Force disable 8-bit loading to avoid precision issues\n",
    "    model = Blip2RetrievalModel(config) # Model is moved to device inside its __init__ if device_map not used\n",
    "    print(f\"\\nBlip2RetrievalModel initialized.\")\n",
    "    # Calculate trainable params AFTER potential freezing\n",
    "    num_params_total = sum(p.numel() for p in model.parameters())\n",
    "    num_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {num_params_total / 1e6:.2f} M\")\n",
    "    print(f\"Trainable parameters: {num_params_trainable / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing BLIP2 model: {e}\")\n",
    "    model = None # Ensure model is None if init fails\n",
    "\n",
    "# --- Optimizer Setup ---\n",
    "if model: # Check if model was created\n",
    "    print(\"\\nSetting up optimizer...\")\n",
    "    # Get parameters based on their names/modules\n",
    "    vision_params = [p for n, p in model.blip_model.vision_model.named_parameters() if p.requires_grad]\n",
    "    qformer_params = [p for n, p in model.blip_model.qformer.named_parameters() if p.requires_grad]\n",
    "\n",
    "    language_params = []\n",
    "    if hasattr(model.blip_model, 'language_model') and model.blip_model.language_model is not None:\n",
    "        language_params.extend([p for n, p in model.blip_model.language_model.named_parameters() if p.requires_grad])\n",
    "    if hasattr(model.blip_model, 'language_projection') and model.blip_model.language_projection is not None:\n",
    "         language_params.extend([p for n, p in model.blip_model.language_projection.named_parameters() if p.requires_grad])\n",
    "\n",
    "    projection_params = [p for n, p in model.named_parameters() if ('image_projection' in n or 'text_projection' in n or 'logit_scale' in n) and p.requires_grad]\n",
    "\n",
    "    print(f\"  Param counts (Trainable): Vision={len(vision_params)}, QFormer={len(qformer_params)}, LM={len(language_params)}{' (FROZEN)' if config.freeze_language_model and not language_params else ''}, Projection={len(projection_params)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": vision_params, \"lr\": config.vision_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": qformer_params, \"lr\": config.qformer_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": language_params, \"lr\": config.language_model_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": projection_params, \"lr\": config.projection_lr, \"weight_decay\": config.weight_decay},\n",
    "    ]\n",
    "\n",
    "    # Filter out groups with zero parameters\n",
    "    optimizer_grouped_parameters = [g for g in optimizer_grouped_parameters if g['params']]\n",
    "\n",
    "    if not optimizer_grouped_parameters:\n",
    "         print(\"ERROR: No trainable parameters found for the optimizer. Check freezing flags and model structure.\")\n",
    "    else:\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "        print(f\"Optimizer AdamW initialized.\")\n",
    "\n",
    "        # --- LR Scheduler Setup ---\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=config.mode,\n",
    "            factor=config.factor,\n",
    "            patience=config.patience\n",
    "        )\n",
    "        print(f\"LR Scheduler ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.factor}, patience={config.patience})\")\n",
    "\n",
    "        # --- Early Stopping Setup ---\n",
    "        early_stopping_counter = 0\n",
    "        print(f\"Early stopping initialized (patience={config.early_stopping_patience}, min_delta={config.early_stopping_min_delta})\")\n",
    "\n",
    "        # --- Note: AMP GradScaler removed since it was causing precision issues ---\n",
    "        print(f\"Note: AMP/Mixed precision disabled to avoid precision mismatches.\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not initialized. Skipping optimizer/scheduler/scaler setup.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 1 epochs...\n",
      "Tracking metric: 'avg_acc' (mode: max)\n",
      "\n",
      "--- Epoch 1/1 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d3f71f97374804b6b41ca0beeb26de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E1:   0%|          | 0/18899 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153074/1106409422.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'image_embeds' or 'text_embeds' not found in output. Trying qformer_outputs.last_hidden_state.\n",
      "Warning: 'image_embeds' or 'text_embeds' not found in output. Trying qformer_outputs.last_hidden_state.\n",
      "Warning: 'image_embeds' or 'text_embeds' not found in output. Trying qformer_outputs.last_hidden_state.\n",
      "Warning: 'image_embeds' or 'text_embeds' not found in output. Trying qformer_outputs.last_hidden_state.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 21.31 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.60 GiB is allocated by PyTorch, and 412.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# --- Training ---\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# REMOVED scaler from the arguments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_loss)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device, epoch_num)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Optimizer Step logic for gradient accumulation\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % config.accumulation_steps == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (i + \u001b[32m1\u001b[39m) == \u001b[38;5;28mlen\u001b[39m(dataloader):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     optimizer.zero_grad()\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Update meter with the un-normalized loss for correct reporting\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/optim/adamw.py:209\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    206\u001b[39m     amsgrad: \u001b[38;5;28mbool\u001b[39m = group[\u001b[33m\"\u001b[39m\u001b[33mamsgrad\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    207\u001b[39m     beta1, beta2 = cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     has_complex = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m     adamw(\n\u001b[32m    221\u001b[39m         params_with_grad,\n\u001b[32m    222\u001b[39m         grads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m         has_complex=has_complex,\n\u001b[32m    241\u001b[39m     )\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/optim/adamw.py:148\u001b[39m, in \u001b[36mAdamW._init_group\u001b[39m\u001b[34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[39m\n\u001b[32m    138\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m] = (\n\u001b[32m    139\u001b[39m     torch.zeros(\n\u001b[32m    140\u001b[39m         (),\n\u001b[32m   (...)\u001b[39m\u001b[32m    145\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m torch.tensor(\u001b[32m0.0\u001b[39m, dtype=_get_scalar_dtype())\n\u001b[32m    146\u001b[39m )\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mexp_avg\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreserve_format\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[32m    152\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mexp_avg_sq\u001b[39m\u001b[33m\"\u001b[39m] = torch.zeros_like(\n\u001b[32m    153\u001b[39m     p, memory_format=torch.preserve_format\n\u001b[32m    154\u001b[39m )\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 21.31 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.60 GiB is allocated by PyTorch, and 412.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# === Cell 11: Training Loop (AMP Removed) ===\n",
    "\n",
    "# Check prerequisites exist before starting loop\n",
    "# REMOVED scaler from the check\n",
    "if model and train_loader and optimizer and lr_scheduler:\n",
    "    print(f\"\\nStarting training for {config.epochs} epochs...\")\n",
    "    print(f\"Tracking metric: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "    # print(f\"Using AMP: {config.use_amp}\") # REMOVED AMP print statement\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    history = {'train_loss': [], 'validation_results': []}\n",
    "    start_train_time = time.time()\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "\n",
    "        # --- Training ---\n",
    "        # REMOVED scaler from the arguments\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, config.device, epoch+1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_results = {\"loss\": float('inf'), config.metric_to_track.replace('_', ' '): (-float('inf') if config.mode == 'max' else float('inf'))} # Default with formatted key\n",
    "        if dev_loader:\n",
    "            # validate_epoch might need autocast removed internally if use_amp is False\n",
    "            val_results = validate_epoch(model, dev_loader, config.device, epoch+1)\n",
    "            history['validation_results'].append(val_results)\n",
    "            print(\"  Validation Metrics:\")\n",
    "            metric_log_str = \"  \"\n",
    "            # Sort keys for consistent printing\n",
    "            sorted_keys = sorted(val_results.keys(), key=lambda x: (x.split()[0], int(x.split('@')[-1]) if '@' in x else -1))\n",
    "            for name in sorted_keys:\n",
    "                 metric_log_str += f\"{name}: {val_results[name]:.4f} | \"\n",
    "            print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "\n",
    "            # --- Scheduler Step ---\n",
    "            current_val_metric_for_scheduler = val_results.get(config.metric_to_track.replace('_', ' '), None) # Use formatted key\n",
    "            if current_val_metric_for_scheduler is not None:\n",
    "                lr_scheduler.step(current_val_metric_for_scheduler)\n",
    "                current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "                # Map LRs back to components for printing (adjust indices based on actual groups)\n",
    "                lr_map = {config.vision_encoder_lr: 'Vision', config.qformer_lr: 'QF', config.language_model_lr: 'LM', config.projection_lr: 'Proj'}\n",
    "                lr_str = \"  Current LRs: \" + \", \".join([f\"{lr_map.get(group['lr'], f'Group{i}')}={group['lr']:.2e}\" for i, group in enumerate(optimizer.param_groups)])\n",
    "                print(lr_str)\n",
    "\n",
    "            else:\n",
    "                print(f\"  Warning: Metric '{config.metric_to_track}' not found in validation results. Scheduler not stepped.\")\n",
    "        else:\n",
    "            print(\"  Validation skipped (no dev_loader).\")\n",
    "            history['validation_results'].append(None)\n",
    "\n",
    "        # --- Save Checkpoint & Early Stopping Logic ---\n",
    "        current_val_metric = val_results.get(config.metric_to_track.replace('_', ' '), -float('inf') if config.mode == \"max\" else float('inf')) # Use formatted key\n",
    "        is_best = False\n",
    "        improved = False\n",
    "\n",
    "        if dev_loader:\n",
    "            if config.mode == \"max\":\n",
    "                if current_val_metric > best_val_metric + config.early_stopping_min_delta:\n",
    "                    is_best = True\n",
    "                    improved = True\n",
    "            else: # config.mode == \"min\"\n",
    "                if current_val_metric < best_val_metric - config.early_stopping_min_delta:\n",
    "                    is_best = True\n",
    "                    improved = True\n",
    "\n",
    "            if is_best:\n",
    "                print(f\"  Metric '{config.metric_to_track}' improved from {best_val_metric:.4f} to {current_val_metric:.4f}\")\n",
    "                best_val_metric = current_val_metric\n",
    "                no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                print(f\"  Metric '{config.metric_to_track}' did not improve. Best: {best_val_metric:.4f}. Counter: {no_improve_epochs}/{config.early_stopping_patience}\")\n",
    "\n",
    "        # Prepare save dictionary - saving state_dict is generally preferred for large models\n",
    "        save_dict = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            # 'scaler_state_dict': scaler.state_dict(), # REMOVED scaler state\n",
    "            'train_loss': train_loss,\n",
    "            'validation_results': val_results,\n",
    "            'best_val_metric': best_val_metric,\n",
    "            'metric_tracked': config.metric_to_track,\n",
    "            'blip_config_dict': model.blip_model.config.to_dict() # Save base model config\n",
    "        }\n",
    "\n",
    "        # Save logic\n",
    "        best_checkpoint_path = os.path.join(config.model_path, \"blip2_retrieval_best.pt\")\n",
    "        final_epoch_path = os.path.join(config.model_path, f\"blip2_retrieval_epoch_{epoch+1}.pt\")\n",
    "\n",
    "        if config.save_best_only and dev_loader:\n",
    "            if is_best:\n",
    "                torch.save(save_dict, best_checkpoint_path)\n",
    "                print(f\"  Saved Best Model (Epoch {epoch+1}) to {best_checkpoint_path}\")\n",
    "        else:\n",
    "            torch.save(save_dict, final_epoch_path)\n",
    "            print(f\"  Saved Epoch {epoch+1} Checkpoint to {final_epoch_path}\")\n",
    "            if is_best and dev_loader:\n",
    "                 torch.save(save_dict, best_checkpoint_path)\n",
    "                 print(f\"  (Also saved as best model)\")\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {epoch_end_time - epoch_start_time:.2f} seconds ---\")\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if dev_loader and no_improve_epochs >= config.early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {config.early_stopping_patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    # --- End of Training ---\n",
    "    end_train_time = time.time()\n",
    "    total_train_time = end_train_time - start_train_time\n",
    "    print(f\"\\n=============== Training Finished ================\")\n",
    "    print(f\"Total Training Time: {total_train_time:.2f} seconds ({total_train_time/60:.2f} minutes)\")\n",
    "\n",
    "    # Save the final epoch's state dictionary separately\n",
    "    final_model_path = os.path.join(config.model_path, 'blip2_retrieval_final_epoch.pt')\n",
    "    # Make sure save_dict has the state from the *last* completed epoch\n",
    "    torch.save(save_dict, final_model_path)\n",
    "    print(f\"Final epoch model state saved to {final_model_path}\")\n",
    "\n",
    "    best_model_file = os.path.join(config.model_path, \"blip2_retrieval_best.pt\")\n",
    "    if dev_loader and os.path.exists(best_model_file):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) is saved at: {best_model_file}\")\n",
    "    elif dev_loader:\n",
    "        print(\"Best model checkpoint file not found. The final epoch model is saved.\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "else:\n",
    "    # Updated error message to remove scaler\n",
    "    print(\"ERROR: Prerequisites for training (model, dataloader, optimizer, scheduler) not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Starting Test Set Evaluation ===============\n",
      "Loading test data from: ./json_data/test.json\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/json_data/test.json\n",
      "Found 2176 samples in test.json.\n",
      "Using image size: 224x224\n",
      "Test loader created with 2176 batches.\n",
      "\n",
      "WARNING: No saved model checkpoints ('best' or 'final') found in ./ViBLIP_vivqa.\n",
      "Evaluation skipped as no model weights were found to load.\n",
      "\n",
      "================= Evaluation Finished =================\n"
     ]
    }
   ],
   "source": [
    "# === Cell 12: Final Evaluation on Test Set ===\n",
    "print(\"\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_loader = None\n",
    "model_to_test = None\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "\n",
    "# 1. Check if test data and processor exist\n",
    "if os.path.exists(test_json_path) and 'processor' in globals() and processor:\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    try:\n",
    "        test_dataset = Blip2ImageCaptionDataset(\n",
    "            json_path=test_json_path,\n",
    "            image_base_path=config.image_path,\n",
    "            processor=processor,\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "\n",
    "        if test_dataset.data:\n",
    "            num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=config.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "        else:\n",
    "             print(\"Test dataset loaded but is empty. Skipping evaluation.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating test dataset/loader: {e}\")\n",
    "else:\n",
    "    print(\"Skipping test evaluation: Test JSON or Processor not found/loaded.\")\n",
    "\n",
    "# 2. Load Model for Testing if test_loader was created\n",
    "if test_loader:\n",
    "    try:\n",
    "        # Determine which model weights to load (best or final)\n",
    "        best_model_path = os.path.join(config.model_path, \"blip2_retrieval_best.pt\")\n",
    "        final_model_path = os.path.join(config.model_path, \"blip2_retrieval_final_epoch.pt\")\n",
    "\n",
    "        load_path = None\n",
    "        if os.path.exists(best_model_path):\n",
    "            load_path = best_model_path\n",
    "            print(f\"\\nAttempting to load best model weights from: {load_path}\")\n",
    "        elif os.path.exists(final_model_path):\n",
    "            load_path = final_model_path\n",
    "            print(f\"\\nBest model not found. Attempting to load final epoch weights from: {load_path}\")\n",
    "        else:\n",
    "            print(f\"\\nWARNING: No saved model checkpoints ('best' or 'final') found in {config.model_path}.\")\n",
    "\n",
    "        if load_path:\n",
    "            checkpoint = torch.load(load_path, map_location=config.device)\n",
    "\n",
    "            # Re-create model using saved config, then load state_dict\n",
    "            print(\"Re-creating model structure for testing...\")\n",
    "            if 'blip_config_dict' in checkpoint:\n",
    "                 saved_blip_config = Blip2Config.from_dict(checkpoint['blip_config_dict'])\n",
    "                 temp_config = config # Start with current config\n",
    "                 temp_config.blip_model_name = saved_blip_config._name_or_path\n",
    "                 print(f\"  Using base model config from checkpoint: {temp_config.blip_model_name}\")\n",
    "                 model_to_test = Blip2RetrievalModel(temp_config)\n",
    "            else:\n",
    "                 print(\"Warning: Blip config not found in checkpoint, using current CFG.\")\n",
    "                 model_to_test = Blip2RetrievalModel(config)\n",
    "\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            # Handle 'module.' prefix\n",
    "            if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "                print(\"Detected 'module.' prefix, removing for loading.\")\n",
    "                from collections import OrderedDict\n",
    "                new_state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "                state_dict = new_state_dict\n",
    "\n",
    "            load_result = model_to_test.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"  State dict loading result: {load_result}\")\n",
    "            if load_result.missing_keys:\n",
    "                 print(f\"  Warning: Missing keys: {load_result.missing_keys}\")\n",
    "            if load_result.unexpected_keys:\n",
    "                 print(f\"  Warning: Unexpected keys: {load_result.unexpected_keys}\")\n",
    "\n",
    "            # Ensure model is on device if not using device_map\n",
    "            if 'device_map' not in getattr(model_to_test.blip_model, 'hf_device_map', {}):\n",
    "                 model_to_test.to(config.device)\n",
    "\n",
    "            print(f\"Model weights loaded successfully from {load_path}\")\n",
    "\n",
    "            # --- Run Evaluation ---\n",
    "            print(\"\\nRunning evaluation on test set...\")\n",
    "            test_results = validate_epoch(model_to_test, test_loader, config.device, epoch_num=\"Test\")\n",
    "\n",
    "            print(\"\\n--- Test Set Results ---\")\n",
    "            metric_log_str = \"\"\n",
    "            sorted_keys = sorted(test_results.keys(), key=lambda x: (x.split()[0], int(x.split('@')[-1]) if '@' in x else -1))\n",
    "            for name in sorted_keys:\n",
    "                 value = test_results[name]\n",
    "                 metric_log_str += f\"  {name}: {value:.4f}\\n\"\n",
    "            print(metric_log_str.strip())\n",
    "            print(\"------------------------\")\n",
    "\n",
    "        else:\n",
    "             print(\"Evaluation skipped as no model weights were found to load.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR during test setup or evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n================= Evaluation Finished =================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot directory ensured at: /home/researcher/huypq69/TuningModels/train_plot\n",
      "No training history found or history is empty. Run training first to generate history.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Cell 13: Training Visualization (Adapted) ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plot_dir = \"train_plot\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "print(f\"Plot directory ensured at: {os.path.abspath(plot_dir)}\")\n",
    "\n",
    "def save_subplot_as_figure(subplot, save_path):\n",
    "    fig_new = plt.figure(figsize=(8, 6))\n",
    "    ax_new = fig_new.add_subplot(111)\n",
    "    lines = subplot.get_lines()\n",
    "    if not lines:\n",
    "        print(f\"Warning: No lines found in subplot for {save_path}\")\n",
    "        plt.close(fig_new)\n",
    "        return\n",
    "    labels = [line.get_label() for line in lines]\n",
    "    for line in lines:\n",
    "        ax_new.plot(line.get_xdata(), line.get_ydata(),\n",
    "                    color=line.get_color(),\n",
    "                    linestyle=line.get_linestyle(),\n",
    "                    marker=line.get_marker(),\n",
    "                    label=line.get_label())\n",
    "    ax_new.set_title(subplot.get_title())\n",
    "    ax_new.set_xlabel(subplot.get_xlabel())\n",
    "    ax_new.set_ylabel(subplot.get_ylabel())\n",
    "    ax_new.grid(True)\n",
    "    if any(label and not label.startswith('_') for label in labels):\n",
    "         ax_new.legend()\n",
    "    plt.tight_layout()\n",
    "    fig_new.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig_new)\n",
    "\n",
    "def plot_training_metrics(history):\n",
    "    if not history or not history.get('train_loss') or not history.get('validation_results'):\n",
    "        print(\"No training history available or history is incomplete.\")\n",
    "        return\n",
    "\n",
    "    valid_results = [res for res in history['validation_results'] if res is not None]\n",
    "    if not valid_results:\n",
    "        print(\"No valid validation results found. Plotting only training loss.\")\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "        ax.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')\n",
    "        ax.set_title('Training Loss over Epochs')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        save_path = os.path.join(plot_dir, f'training_loss.png')\n",
    "        fig.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Saved loss plot to: {save_path}\")\n",
    "        # plt.show() # Don't show in script mode\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    num_epochs_trained = len(history['train_loss'])\n",
    "    num_epochs_validated = len(valid_results)\n",
    "    epochs_train = range(1, num_epochs_trained + 1)\n",
    "    epochs_val = range(1, num_epochs_validated + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 13))\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16, y=1.02)\n",
    "\n",
    "    # --- Plot Loss ---\n",
    "    val_loss = [res.get('loss', float('nan')) for res in valid_results] # Use .get for safety\n",
    "    axes[0, 0].plot(epochs_train, history['train_loss'], 'b-o', label='Training Loss')\n",
    "    axes[0, 0].plot(epochs_val, val_loss, 'r-s', label='Validation Loss')\n",
    "    axes[0, 0].set_title('Loss over Epochs')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # --- Plot Accuracy (Average Accuracy) ---\n",
    "    metric_key_acc = 'avg acc' # Key from validate_epoch output\n",
    "    if metric_key_acc in valid_results[0]:\n",
    "        val_acc = [res[metric_key_acc] for res in valid_results]\n",
    "        axes[0, 1].plot(epochs_val, val_acc, 'g-^', label='Average Accuracy (Val)')\n",
    "        axes[0, 1].set_title('Validation Average Accuracy over Epochs')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "    else:\n",
    "        axes[0, 1].set_title(f'Validation Acc ({metric_key_acc}) (Not Found)')\n",
    "\n",
    "    # --- Plot Recall Metrics ---\n",
    "    has_recall = 'i2t recall R@1' in valid_results[0] # Check a representative key\n",
    "\n",
    "    if has_recall:\n",
    "        # I2T Recall\n",
    "        for k in [1, 5, 10]:\n",
    "            key = f'i2t recall R@{k}'\n",
    "            values = [res.get(key, float('nan')) for res in valid_results]\n",
    "            axes[1, 0].plot(epochs_val, values, marker='o', label=f'I2T R@{k}')\n",
    "        axes[1, 0].set_title('Image-to-Text Recall (Val)')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Recall')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "\n",
    "        # T2I Recall\n",
    "        for k in [1, 5, 10]:\n",
    "            key = f't2i recall R@{k}'\n",
    "            values = [res.get(key, float('nan')) for res in valid_results]\n",
    "            axes[1, 1].plot(epochs_val, values, marker='s', label=f'T2I R@{k}')\n",
    "        axes[1, 1].set_title('Text-to-Image Recall (Val)')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Recall')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "    else:\n",
    "        axes[1, 0].set_title('I2T Recall (Not Found)')\n",
    "        axes[1, 1].set_title('T2I Recall (Not Found)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    # Save individual plots\n",
    "    plot_names = ['loss', 'accuracy', 'i2t_recall', 't2i_recall']\n",
    "    for idx, name in enumerate(plot_names):\n",
    "        i, j = divmod(idx, 2)\n",
    "        save_path = os.path.join(plot_dir, f'training_{name}.png')\n",
    "        if axes[i, j].has_data():\n",
    "            save_subplot_as_figure(axes[i, j], save_path)\n",
    "            print(f\"Saved {name} plot to: {save_path}\")\n",
    "        else:\n",
    "            print(f\"Skipping save for {name} plot (no data).\")\n",
    "\n",
    "    # Save combined plot\n",
    "    combined_save_path = os.path.join(plot_dir, 'training_metrics_combined.png')\n",
    "    fig.savefig(combined_save_path, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved combined plot to: {combined_save_path}\")\n",
    "\n",
    "    # plt.show() # Avoid showing plots in a script run\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "# Check if the 'history' variable exists from the training loop before plotting\n",
    "if 'history' in locals() and isinstance(history, dict) and history.get('train_loss'):\n",
    "    plot_training_metrics(history)\n",
    "else:\n",
    "    print(\"No training history found or history is empty. Run training first to generate history.\")\n",
    "\n",
    "\n",
    "# --- END OF SCRIPT ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
