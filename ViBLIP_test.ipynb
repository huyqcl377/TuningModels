{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "Transformers Version: 4.50.0\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ViBLIP Fine-tuning for Vietnamese Image Retrieval by Text\n",
    "# Generated from Jupyter Notebook\n",
    "\n",
    "\n",
    "# === Cell 1: Installs and Imports ===\n",
    "# !pip install -q transformers torch torchvision torchaudio Pillow tqdm accelerate bitsandbytes sentencepiece\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Blip2Processor, Blip2Model, Blip2Config, AutoTokenizer # AutoTokenizer might be needed for processor loading edge cases\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm # Use standard tqdm if not in notebook: from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time # For timing epochs\n",
    "import transformers\n",
    "from torch.cuda.amp import GradScaler, autocast # For mixed precision\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model output path: ./ViBLIP_vivqa\n",
      "Selected BLIP2 Model: Salesforce/blip2-flan-t5-xl\n",
      "Image base path (for resolving paths in JSON): /home/researcher/huypq69/TuningModels/data/OpenViVQA-dataset\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Configuration Class (CFG) ===\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    # Base directory where your train.json, dev.json, test.json are located\n",
    "    data_path = \"./json_data/\"\n",
    "    image_path = \"./data/OpenViVQA-dataset/\"\n",
    "\n",
    "    # Output directory for saved models\n",
    "    model_path = \"./ViBLIP_vivqa\"\n",
    "\n",
    "    # --- BLIP Model Selection ---\n",
    "    # Common options: Salesforce/blip2-opt-2.7b, Salesforce/blip2-flan-t5-xl\n",
    "    # Choose based on available resources. OPT models might be slightly smaller.\n",
    "    selected_blip_model = \"Salesforce/blip2-flan-t5-xl\" # Smaller model to avoid CUDA OOM errors\n",
    "\n",
    "    # --- Model parameters ---\n",
    "    blip_model_name = selected_blip_model\n",
    "    blip_processor_name = selected_blip_model\n",
    "    projection_dim = 256 # Shared latent space dimension (e.g., 256, 512, 768)\n",
    "    # Freeze parts of the model? BLIP2's LLM is very large.\n",
    "    freeze_vision_model = False\n",
    "    freeze_language_model = True\n",
    "    freeze_qformer = False\n",
    "    # Quantization (Requires bitsandbytes). Reduces memory significantly.\n",
    "    load_in_8bit = False # Set to True if memory is tight\n",
    "\n",
    "    # --- Training parameters ---\n",
    "    seed = 42\n",
    "    # Reduce batch size significantly compared to CLIP due to BLIP2's size\n",
    "    batch_size = 8  # START LOW (e.g., 4, 8, 16) and increase based on GPU memory\n",
    "    num_workers = 8  # Adjust based on system capability\n",
    "    # Learning rates for different components (tune these)\n",
    "    vision_encoder_lr = 1e-5\n",
    "    qformer_lr = 2e-5\n",
    "    language_model_lr = 1e-6 # Only relevant if freeze_language_model=False\n",
    "    projection_lr = 1e-4 # Projection head can often learn faster\n",
    "    weight_decay = 1e-3\n",
    "    patience = 3 # Scheduler patience\n",
    "    factor = 0.8 # Scheduler reduction factor\n",
    "    epochs = 10 # Adjust as needed\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Use mixed precision to save memory and potentially speed up training\n",
    "    use_amp = True\n",
    "\n",
    "    # --- Image/Text parameters (mostly handled by processor) ---\n",
    "    # Processor determines image size (usually 224 for BLIP2)\n",
    "    max_length = 64 # Max text sequence length for tokenizer\n",
    "\n",
    "    # --- Loss/Saving parameters ---\n",
    "    temperature = 0.07 # Initial temperature for scaling logits\n",
    "    learnable_temperature = True # Whether the logit_scale is learnable\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"avg_acc\" # Common retrieval metric ('avg_acc', 'i2t R@1', 't2i R@1', 'avg_R@5', etc.)\n",
    "    mode = \"max\" # Mode for scheduler/saving based on metric_to_track ('max' for recall/acc, 'min' for loss)\n",
    "    early_stopping_patience = 5 # Epochs with no improvement before stopping\n",
    "    early_stopping_min_delta = 0.001 # Minimum change to qualify as improvement\n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Selected BLIP2 Model: {config.blip_model_name}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: Seeding for Reproducibility ===\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # For multi-GPU\n",
    "        # torch.backends.cudnn.deterministic = True # Can impact performance\n",
    "        # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Metric Calculation Utilities ===\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        if torch.is_tensor(val):\n",
    "             val = val.item()\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    \"\"\"Calculates Recall@k for image-text retrieval.\"\"\"\n",
    "    n = similarity_matrix.shape[1-dim] # Number of samples (e.g., images if dim=1 for T2I)\n",
    "    k_eff = min(k, similarity_matrix.shape[dim]) # Effective k cannot be larger than candidate pool size\n",
    "    if k_eff == 0 or n == 0: return 0.0\n",
    "\n",
    "    top_k_indices = torch.topk(similarity_matrix, k_eff, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "\n",
    "    correct_count = 0\n",
    "    if dim == 0: # I2T: Find correct text (row index) for each image (column)\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]:\n",
    "                correct_count += 1\n",
    "    elif dim == 1: # T2I: Find correct image (column index) for each text (row)\n",
    "        for txt_idx in range(n):\n",
    "             if ground_truth[txt_idx] in top_k_indices[txt_idx, :]:\n",
    "                correct_count += 1\n",
    "    else:\n",
    "        raise ValueError(\"dim must be 0 or 1\")\n",
    "\n",
    "    return correct_count / n if n > 0 else 0.0\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    \"\"\"Computes retrieval metrics for a batch or validation set.\"\"\"\n",
    "    if image_embeddings.device != text_embeddings.device:\n",
    "        text_embeddings = text_embeddings.to(image_embeddings.device)\n",
    "\n",
    "    sim_matrix = text_embeddings @ image_embeddings.T\n",
    "    sim_matrix = sim_matrix.float()\n",
    "    n = sim_matrix.shape[0]\n",
    "    if n == 0:\n",
    "        return {\n",
    "            \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "            \"avg_cosine_sim\": 0.0,\n",
    "            \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"avg_R@1\": 0.0, \"avg_R@5\": 0.0, \"avg_R@10\": 0.0\n",
    "        }\n",
    "\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "    t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "    t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "    avg_acc = (i2t_acc + t2i_acc) / 2\n",
    "    avg_cosine_sim = torch.diagonal(sim_matrix).mean().item()\n",
    "\n",
    "    i2t_recall = {}\n",
    "    t2i_recall = {}\n",
    "    recall_k_values = [1, 5, 10]\n",
    "\n",
    "    for k in recall_k_values:\n",
    "        k_str = f\"R@{k}\"\n",
    "        i2t_recall[k_str] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "        t2i_recall[k_str] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "\n",
    "    avg_recall = {}\n",
    "    for k in recall_k_values:\n",
    "        k_str = f\"R@{k}\"\n",
    "        avg_recall[f\"avg_{k_str}\"] = (i2t_recall[k_str] + t2i_recall[k_str]) / 2\n",
    "\n",
    "    metrics = {\n",
    "        \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "        \"avg_cosine_sim\": avg_cosine_sim,\n",
    "        \"i2t_recall\": i2t_recall,\n",
    "        \"t2i_recall\": t2i_recall,\n",
    "        **avg_recall\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blip2ImageCaptionDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Dataset Class Definition ===\n",
    "class Blip2ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, processor, max_length):\n",
    "        super().__init__()\n",
    "        print(f\"Attempting to load data from: {os.path.abspath(json_path)}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                self.data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: JSON file not found at {json_path}\")\n",
    "            self.data = []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error: Could not decode JSON from {json_path}: {e}\")\n",
    "            self.data = []\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred loading {json_path}: {e}\")\n",
    "            self.data = []\n",
    "\n",
    "        print(f\"Found {len(self.data)} samples in {os.path.basename(json_path)}.\")\n",
    "        self.image_base_path = image_base_path\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Determine image size from processor\n",
    "        try:\n",
    "            # Try accessing size directly (newer transformers)\n",
    "            if isinstance(self.processor.image_processor.size, dict):\n",
    "                self.img_size = self.processor.image_processor.size['height'] # Or 'shortest_edge'\n",
    "            else: # Older style might be int or tuple\n",
    "                 self.img_size = self.processor.image_processor.size\n",
    "                 if isinstance(self.img_size, (tuple, list)): self.img_size = self.img_size[0]\n",
    "        except AttributeError:\n",
    "            print(\"Warning: Could not determine image size from processor, defaulting to 224.\")\n",
    "            self.img_size = 224\n",
    "        print(f\"Using image size: {self.img_size}x{self.img_size}\")\n",
    "\n",
    "        if not os.path.isdir(self.image_base_path):\n",
    "             print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data):\n",
    "             raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path')\n",
    "        captions = item.get('caption', [])\n",
    "        caption = captions[0] if captions else \"\" # Take the first caption\n",
    "\n",
    "        # Initialize with dummy data\n",
    "        dummy_image = Image.new('RGB', (self.img_size, self.img_size))\n",
    "        try:\n",
    "            pixel_values = self.processor(images=dummy_image, return_tensors=\"pt\")['pixel_values'].squeeze()\n",
    "        except Exception as e:\n",
    "             print(f\"Error processing dummy image: {e}\")\n",
    "             pixel_values = torch.zeros((3, self.img_size, self.img_size)) # Fallback tensor\n",
    "\n",
    "        image_loaded_successfully = False\n",
    "        if relative_image_path:\n",
    "            image_path = os.path.normpath(os.path.join(self.image_base_path, relative_image_path))\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                processed_output = self.processor(images=image, text=None, return_tensors=\"pt\") # Only process image here\n",
    "                pixel_values = processed_output['pixel_values'].squeeze()\n",
    "                image_loaded_successfully = True\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Img not found at {image_path}. Using dummy image for idx {idx}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading image {image_path}: {e}. Using dummy image for idx {idx}.\")\n",
    "        else:\n",
    "             print(f\"Warning: Missing 'image_path' for item at index {idx}. Using dummy image.\")\n",
    "\n",
    "        # Process text\n",
    "        try:\n",
    "            text_inputs = self.processor(\n",
    "                images=None, # Important: don't re-process image\n",
    "                text=caption,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = text_inputs['input_ids'].squeeze()\n",
    "            attention_mask = text_inputs['attention_mask'].squeeze()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text '{caption}' for idx {idx}: {e}\")\n",
    "            # Create dummy text inputs if error occurs\n",
    "            input_ids = torch.zeros(self.max_length, dtype=torch.long)\n",
    "            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n",
    "\n",
    "        # Ensure tensors are 1D after squeeze\n",
    "        if input_ids.dim() > 1: input_ids = input_ids.view(-1)\n",
    "        if attention_mask.dim() > 1: attention_mask = attention_mask.view(-1)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "print(\"Blip2ImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP2 Retrieval Model and Loss Function defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Model Definition (BLIP2 Retrieval Model, Loss) ===\n",
    "class Blip2RetrievalModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config_train = config # Store training config\n",
    "        print(f\"Initializing BLIP2 Model: {config.blip_model_name}\")\n",
    "\n",
    "        # Load the base BLIP2 model.\n",
    "        # Use Blip2Model for feature extraction, not Blip2ForConditionalGeneration.\n",
    "        try:\n",
    "            load_kwargs = {}\n",
    "            if config.load_in_8bit:\n",
    "                 if not torch.cuda.is_available() or torch.cuda.get_device_capability()[0] < 7:\n",
    "                    print(\"Warning: 8-bit loading requested but not supported on this GPU or CUDA version. Loading in default precision.\")\n",
    "                 else:\n",
    "                    print(\"Attempting to load model in 8-bit.\")\n",
    "                    load_kwargs['load_in_8bit'] = True\n",
    "                    load_kwargs['device_map'] = 'auto' # device_map needed for 8-bit\n",
    "\n",
    "            self.blip_model = Blip2Model.from_pretrained(\n",
    "                config.blip_model_name,\n",
    "                **load_kwargs\n",
    "            )\n",
    "\n",
    "            # If not using device_map, explicitly move model parts if needed\n",
    "            if 'device_map' not in load_kwargs and config.device != torch.device('cpu'):\n",
    "                print(f\"Manually moving model components to {config.device}\")\n",
    "                self.blip_model.to(config.device)\n",
    "            elif 'device_map' in load_kwargs:\n",
    "                print(f\"Model loaded with device_map: {self.blip_model.hf_device_map}\")\n",
    "\n",
    "        except ImportError as e:\n",
    "             if 'bitsandbytes' in str(e):\n",
    "                 print(\"ERROR: bitsandbytes library not found. Please install it (`pip install bitsandbytes`) to use 8-bit loading.\")\n",
    "             else:\n",
    "                 print(f\"ERROR loading base BLIP2 model: {e}\")\n",
    "             raise e\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading base BLIP2 model: {e}\")\n",
    "            print(\"Check model name, internet connection, and available memory.\")\n",
    "            raise e\n",
    "\n",
    "        # --- Freeze Components ---\n",
    "        if config.freeze_vision_model:\n",
    "            print(\"  Freezing Vision Model parameters.\")\n",
    "            for param in self.blip_model.vision_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        if config.freeze_qformer:\n",
    "            print(\"  Freezing Q-Former parameters.\")\n",
    "            for param in self.blip_model.qformer.parameters():\n",
    "                param.requires_grad = False\n",
    "        if config.freeze_language_model:\n",
    "            if hasattr(self.blip_model, 'language_model') and self.blip_model.language_model is not None:\n",
    "                 print(\"  Freezing Language Model parameters.\")\n",
    "                 for param in self.blip_model.language_model.parameters():\n",
    "                    param.requires_grad = False\n",
    "                 # Also freeze the language projection if it exists (maps Q-Former to LM input)\n",
    "                 if hasattr(self.blip_model, 'language_projection') and self.blip_model.language_projection is not None:\n",
    "                     for param in self.blip_model.language_projection.parameters():\n",
    "                         param.requires_grad = False\n",
    "            else:\n",
    "                 print(\"  Language model component not found or is None, skipping freeze.\")\n",
    "\n",
    "        # Determine input dimension for projection heads (usually Q-Former output dim)\n",
    "        qformer_hidden_size = self.blip_model.config.qformer_config.hidden_size\n",
    "        print(f\"  Q-Former hidden size (input to projection): {qformer_hidden_size}\")\n",
    "\n",
    "        # --- Projection Heads ---\n",
    "        # Use separate projections for image and text features coming from Q-Former\n",
    "        self.image_projection = nn.Linear(qformer_hidden_size, config.projection_dim, bias=False)\n",
    "        self.text_projection = nn.Linear(qformer_hidden_size, config.projection_dim, bias=False)\n",
    "        print(f\"  Added projection heads: {qformer_hidden_size} -> {config.projection_dim}\")\n",
    "\n",
    "        # --- Learnable Temperature ---\n",
    "        if config.learnable_temperature:\n",
    "            self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / config.temperature))\n",
    "            print(f\"  Using learnable temperature (logit scale), initialized to {self.logit_scale.exp().item():.4f}\")\n",
    "        else:\n",
    "            self.logit_scale = torch.tensor(np.log(1 / config.temperature)) # Keep on CPU initially, move in forward\n",
    "            print(f\"  Using fixed temperature: {config.temperature}\")\n",
    "\n",
    "        # Move projections to the correct device if model wasn't loaded with device_map\n",
    "        if 'device_map' not in getattr(self.blip_model, 'hf_device_map', {}):\n",
    "            model_device = config.device # Get device from config\n",
    "            self.image_projection.to(model_device)\n",
    "            self.text_projection.to(model_device)\n",
    "            if isinstance(self.logit_scale, nn.Parameter):\n",
    "                 self.logit_scale.to(model_device)\n",
    "            else:\n",
    "                 self.logit_scale = self.logit_scale.to(model_device) # Move tensor\n",
    "\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # Extract features - QFormer output is used for multimodal understanding\n",
    "        image_outputs = self.blip_model.get_image_features(pixel_values=pixel_values, return_dict=True)\n",
    "        text_outputs = self.blip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "\n",
    "        # Use the pooled output (representation of [CLS] query token)\n",
    "        image_features = image_outputs.last_hidden_state[:, 0, :] # Shape: [batch_size, qformer_hidden_size]\n",
    "        text_features = text_outputs.last_hidden_state[:, 0, :]   # Shape: [batch_size, qformer_hidden_size]\n",
    "\n",
    "        # Project features into the shared embedding space\n",
    "        image_embeds = self.image_projection(image_features)\n",
    "        text_embeds = self.text_projection(text_features)\n",
    "\n",
    "        # Normalize embeddings\n",
    "        image_embeds_norm = F.normalize(image_embeds, p=2, dim=-1)\n",
    "        text_embeds_norm = F.normalize(text_embeds, p=2, dim=-1)\n",
    "\n",
    "        # Cosine similarity scaled by temperature\n",
    "        # Ensure logit_scale is on the same device as embeddings\n",
    "        current_logit_scale = self.logit_scale.exp().to(image_embeds_norm.device)\n",
    "        logits_per_image = current_logit_scale * image_embeds_norm @ text_embeds_norm.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text, image_embeds_norm, text_embeds_norm\n",
    "\n",
    "# --- Loss Function (Contrastive Loss) ---\n",
    "def contrastive_loss(logits_per_image, logits_per_text):\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    if batch_size == 0: return torch.tensor(0.0, device=logits_per_image.device, requires_grad=True)\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "    loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "    total_loss = (loss_img + loss_txt) / 2.0\n",
    "    return total_loss\n",
    "\n",
    "print(\"BLIP2 Retrieval Model and Loss Function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation epoch functions defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 7: Training and Validation Epoch Functions ===\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch_num, scaler):\n",
    "    model.train()\n",
    "    loss_meter = AvgMeter(f\"Train Loss E{epoch_num}\")\n",
    "    # Use standard tqdm if not in notebook\n",
    "    try:\n",
    "        from tqdm.notebook import tqdm as tqdm_notebook\n",
    "        progress_bar = tqdm_notebook(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    except ImportError:\n",
    "        from tqdm import tqdm\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move batch to device\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_size = pixel_values.size(0)\n",
    "        if batch_size == 0: continue\n",
    "\n",
    "        # Automatic Mixed Precision\n",
    "        with autocast(enabled=config.use_amp):\n",
    "            logits_per_image, logits_per_text, _, _ = model(pixel_values, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(logits_per_image, logits_per_text)\n",
    "\n",
    "        # Gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_meter.update(loss.item(), batch_size)\n",
    "        progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    return loss_meter.avg\n",
    "\n",
    "def validate_epoch(model, dataloader, device, epoch_num):\n",
    "    model.eval()\n",
    "    loss_meter = AvgMeter(f\"Val Loss E{epoch_num}\")\n",
    "    # Initialize metric accumulators\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "\n",
    "    # Use standard tqdm if not in notebook\n",
    "    try:\n",
    "        from tqdm.notebook import tqdm as tqdm_notebook\n",
    "        progress_bar = tqdm_notebook(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    except ImportError:\n",
    "        from tqdm import tqdm\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = pixel_values.size(0)\n",
    "            if batch_size == 0: continue\n",
    "\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                logits_per_image, logits_per_text, image_embeds, text_embeds = model(pixel_values, input_ids, attention_mask)\n",
    "                loss = contrastive_loss(logits_per_image, logits_per_text)\n",
    "\n",
    "            loss_meter.update(loss.item(), batch_size)\n",
    "\n",
    "            # Store embeddings (move to CPU to conserve GPU memory during validation)\n",
    "            all_image_embeddings.append(image_embeds.cpu())\n",
    "            all_text_embeddings.append(text_embeds.cpu())\n",
    "\n",
    "            # Display running loss in progress bar\n",
    "            progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    # Concatenate all embeddings\n",
    "    if not all_image_embeddings or not all_text_embeddings:\n",
    "         print(\"Warning: No embeddings collected during validation.\")\n",
    "         # Return zero/default metrics if no data processed\n",
    "         zero_metrics = { \"loss\": loss_meter.avg, \"avg_acc\": 0.0, \"avg_cosine_sim\": 0.0,\n",
    "                           \"i2t R@1\": 0.0, \"i2t R@5\": 0.0, \"i2t R@10\": 0.0,\n",
    "                           \"t2i R@1\": 0.0, \"t2i R@5\": 0.0, \"t2i R@10\": 0.0,\n",
    "                           \"avg R@1\": 0.0, \"avg R@5\": 0.0, \"avg R@10\": 0.0 }\n",
    "         # Reformat keys for consistency with normal return\n",
    "         return {k.replace('_', ' '): v for k,v in zero_metrics.items()}\n",
    "\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "\n",
    "    # Compute metrics over the entire validation set on the specified device\n",
    "    print(f\"\\nComputing metrics over {all_image_embeddings.shape[0]} validation samples...\")\n",
    "    validation_metrics = compute_metrics(all_image_embeddings.to(device), all_text_embeddings.to(device))\n",
    "\n",
    "    # Combine loss with computed metrics\n",
    "    final_results = {\"loss\": loss_meter.avg}\n",
    "    # Flatten the recall dictionaries for easier logging/history tracking\n",
    "    for k, v in validation_metrics.items():\n",
    "        if isinstance(v, dict):\n",
    "            for recall_k, recall_v in v.items():\n",
    "                final_results[f\"{k.replace('_', ' ')} {recall_k}\"] = recall_v\n",
    "        else:\n",
    "            final_results[k.replace('_', ' ')] = v # Replace underscores for keys like avg_acc\n",
    "\n",
    "    return final_results\n",
    "\n",
    "print(\"Training and Validation epoch functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP2 Processor: Salesforce/blip2-flan-t5-xl\n",
      "Processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8: Setup - BLIP2 Processor ===\n",
    "print(f\"Loading BLIP2 Processor: {config.blip_processor_name}\")\n",
    "try:\n",
    "    processor = Blip2Processor.from_pretrained(config.blip_processor_name)\n",
    "    print(\"Processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading processor '{config.blip_processor_name}': {e}\")\n",
    "    processor = None # Ensure processor is None if loading fails\n",
    "    # raise e # Optionally stop execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/json_data/train.json\n",
      "Found 18899 samples in train.json.\n",
      "Using image size: 224x224\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/json_data/dev.json\n",
      "Found 2239 samples in dev.json.\n",
      "Using image size: 224x224\n",
      "\n",
      "Creating dataloaders...\n",
      "Using 8 workers for DataLoaders.\n",
      "Train loader created with 2362 batches.\n",
      "Validation loader created with 280 batches.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 9: Setup - Datasets and DataLoaders ===\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "\n",
    "if processor:\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_json = os.path.join(config.data_path, \"train.json\")\n",
    "    dev_json = os.path.join(config.data_path, \"dev.json\")\n",
    "\n",
    "    train_dataset = Blip2ImageCaptionDataset(\n",
    "        json_path=train_json,\n",
    "        image_base_path=config.image_path,\n",
    "        processor=processor,\n",
    "        max_length=config.max_length\n",
    "    )\n",
    "    dev_dataset = Blip2ImageCaptionDataset(\n",
    "        json_path=dev_json,\n",
    "        image_base_path=config.image_path,\n",
    "        processor=processor,\n",
    "        max_length=config.max_length\n",
    "    )\n",
    "\n",
    "    if not train_dataset.data:\n",
    "        print(\"\\nERROR: Failed to load training data. Check 'train_json' path and format.\")\n",
    "    if not dev_dataset.data:\n",
    "         print(\"\\nWARNING: Failed to load validation data. Validation steps will be skipped or may error.\")\n",
    "\n",
    "    print(\"\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    if train_dataset.data:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=True # Drop last incomplete batch for more stable training steps\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "    else:\n",
    "        print(\"Skipping train loader creation due to missing training data.\")\n",
    "\n",
    "    if dev_dataset.data:\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False # Keep last batch for full validation\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "    else:\n",
    "        print(\"Skipping validation loader creation due to missing validation data.\")\n",
    "\n",
    "    if not train_loader:\n",
    "         print(\"\\nERROR: Train loader could not be created. Cannot proceed with training.\")\n",
    "\n",
    "else:\n",
    "     print(\"ERROR: BLIP2 Processor not loaded. Skipping dataset and dataloader creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model components...\n",
      "Initializing BLIP2 Model: Salesforce/blip2-flan-t5-xl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7cc576e6214bc6be686dd29752862d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually moving model components to cuda\n",
      "  Freezing Language Model parameters.\n",
      "  Q-Former hidden size (input to projection): 768\n",
      "  Added projection heads: 768 -> 256\n",
      "  Using learnable temperature (logit scale), initialized to 14.2857\n",
      "\n",
      "Blip2RetrievalModel initialized.\n",
      "Total parameters: 3942.84 M\n",
      "Trainable parameters: 1091.51 M\n",
      "\n",
      "Setting up optimizer...\n",
      "  Param counts (Trainable): Vision=474, QFormer=254, LM=0 (FROZEN), Projection=3\n",
      "Optimizer AdamW initialized.\n",
      "LR Scheduler ReduceLROnPlateau initialized (mode='max', factor=0.8, patience=3)\n",
      "Early stopping initialized (patience=5, min_delta=0.001)\n",
      "AMP GradScaler initialized (enabled).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87807/1791359304.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=config.use_amp)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10: Setup - Model, Optimizer, Scheduler, AMP Scaler ===\n",
    "model = None\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "scaler = None\n",
    "\n",
    "print(\"\\nInitializing model components...\")\n",
    "try:\n",
    "    # Pass the config object to the model\n",
    "    model = Blip2RetrievalModel(config) # Model is moved to device inside its __init__ if device_map not used\n",
    "    print(f\"\\nBlip2RetrievalModel initialized.\")\n",
    "    # Calculate trainable params AFTER potential freezing\n",
    "    num_params_total = sum(p.numel() for p in model.parameters())\n",
    "    num_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {num_params_total / 1e6:.2f} M\")\n",
    "    print(f\"Trainable parameters: {num_params_trainable / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing BLIP2 model: {e}\")\n",
    "    model = None # Ensure model is None if init fails\n",
    "\n",
    "# --- Optimizer Setup ---\n",
    "if model: # Check if model was created\n",
    "    print(\"\\nSetting up optimizer...\")\n",
    "    # Get parameters based on their names/modules\n",
    "    vision_params = [p for n, p in model.blip_model.vision_model.named_parameters() if p.requires_grad]\n",
    "    qformer_params = [p for n, p in model.blip_model.qformer.named_parameters() if p.requires_grad]\n",
    "\n",
    "    language_params = []\n",
    "    if hasattr(model.blip_model, 'language_model') and model.blip_model.language_model is not None:\n",
    "        language_params.extend([p for n, p in model.blip_model.language_model.named_parameters() if p.requires_grad])\n",
    "    if hasattr(model.blip_model, 'language_projection') and model.blip_model.language_projection is not None:\n",
    "         language_params.extend([p for n, p in model.blip_model.language_projection.named_parameters() if p.requires_grad])\n",
    "\n",
    "    projection_params = [p for n, p in model.named_parameters() if ('image_projection' in n or 'text_projection' in n or 'logit_scale' in n) and p.requires_grad]\n",
    "\n",
    "    print(f\"  Param counts (Trainable): Vision={len(vision_params)}, QFormer={len(qformer_params)}, LM={len(language_params)}{' (FROZEN)' if config.freeze_language_model and not language_params else ''}, Projection={len(projection_params)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": vision_params, \"lr\": config.vision_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": qformer_params, \"lr\": config.qformer_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": language_params, \"lr\": config.language_model_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": projection_params, \"lr\": config.projection_lr, \"weight_decay\": config.weight_decay},\n",
    "    ]\n",
    "\n",
    "    # Filter out groups with zero parameters\n",
    "    optimizer_grouped_parameters = [g for g in optimizer_grouped_parameters if g['params']]\n",
    "\n",
    "    if not optimizer_grouped_parameters:\n",
    "         print(\"ERROR: No trainable parameters found for the optimizer. Check freezing flags and model structure.\")\n",
    "    else:\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "        print(f\"Optimizer AdamW initialized.\")\n",
    "\n",
    "        # --- LR Scheduler Setup ---\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=config.mode,\n",
    "            factor=config.factor,\n",
    "            patience=config.patience\n",
    "        )\n",
    "        print(f\"LR Scheduler ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.factor}, patience={config.patience})\")\n",
    "\n",
    "        # --- Early Stopping Setup ---\n",
    "        early_stopping_counter = 0\n",
    "        print(f\"Early stopping initialized (patience={config.early_stopping_patience}, min_delta={config.early_stopping_min_delta})\")\n",
    "\n",
    "        # --- AMP GradScaler Setup ---\n",
    "        scaler = GradScaler(enabled=config.use_amp)\n",
    "        print(f\"AMP GradScaler initialized ({'enabled' if config.use_amp else 'disabled'}).\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not initialized. Skipping optimizer/scheduler/scaler setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 10 epochs...\n",
      "Tracking metric: 'avg_acc' (mode: max)\n",
      "Using AMP: True\n",
      "\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdcf0f6257e4a329c60f49051431cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E1:   0%|          | 0/2362 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87807/1304350631.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=config.use_amp):\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 13.31 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 848.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# --- Training ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_loss)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device, epoch_num, scaler)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Automatic Mixed Precision\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled=config.use_amp):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     logits_per_image, logits_per_text, _, _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     loss = contrastive_loss(logits_per_image, logits_per_text)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Gradient scaling\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mBlip2RetrievalModel.forward\u001b[39m\u001b[34m(self, pixel_values, input_ids, attention_mask)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values, input_ids, attention_mask):\n\u001b[32m     94\u001b[39m     \u001b[38;5;66;03m# Extract features - QFormer output is used for multimodal understanding\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     image_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblip_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     text_outputs = \u001b[38;5;28mself\u001b[39m.blip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     98\u001b[39m     \u001b[38;5;66;03m# Use the pooled output (representation of [CLS] query token)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/transformers/models/blip_2/modeling_blip_2.py:1593\u001b[39m, in \u001b[36mBlip2Model.get_image_features\u001b[39m\u001b[34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[39m\n\u001b[32m   1588\u001b[39m output_hidden_states = (\n\u001b[32m   1589\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m   1590\u001b[39m )\n\u001b[32m   1591\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1593\u001b[39m vision_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1595\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1598\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1599\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vision_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/transformers/models/blip_2/modeling_blip_2.py:753\u001b[39m, in \u001b[36mBlip2VisionModel.forward\u001b[39m\u001b[34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[39m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to specify pixel_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    751\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m last_hidden_state = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    761\u001b[39m last_hidden_state = \u001b[38;5;28mself\u001b[39m.post_layernorm(last_hidden_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/transformers/models/blip_2/modeling_blip_2.py:691\u001b[39m, in \u001b[36mBlip2Encoder.forward\u001b[39m\u001b[34m(self, inputs_embeds, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         encoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m    687\u001b[39m         attention_mask,\n\u001b[32m    688\u001b[39m         output_attentions,\n\u001b[32m    689\u001b[39m     )\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m     layer_outputs = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/transformers/models/blip_2/modeling_blip_2.py:384\u001b[39m, in \u001b[36mBlip2EncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions)\u001b[39m\n\u001b[32m    381\u001b[39m residual = hidden_states\n\u001b[32m    383\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm1(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m hidden_states, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    389\u001b[39m hidden_states = hidden_states + residual\n\u001b[32m    390\u001b[39m residual = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/TuningModels/.venv/lib/python3.12/site-packages/transformers/models/blip_2/modeling_blip_2.py:327\u001b[39m, in \u001b[36mBlip2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    325\u001b[39m     attention_probs = attention_probs * head_mask\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m context_layer = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m)\u001b[49m.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m)\n\u001b[32m    329\u001b[39m new_context_layer_shape = context_layer.size()[:-\u001b[32m2\u001b[39m] + (\u001b[38;5;28mself\u001b[39m.embed_dim,)\n\u001b[32m    330\u001b[39m context_layer = context_layer.reshape(new_context_layer_shape)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 13.31 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 22.19 GiB is allocated by PyTorch, and 848.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# === Cell 11: Training Loop ===\n",
    "# Check prerequisites exist before starting loop\n",
    "if model and train_loader and optimizer and lr_scheduler and scaler:\n",
    "    print(f\"\\nStarting training for {config.epochs} epochs...\")\n",
    "    print(f\"Tracking metric: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "    print(f\"Using AMP: {config.use_amp}\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    history = {'train_loss': [], 'validation_results': []}\n",
    "    start_train_time = time.time()\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "\n",
    "        # --- Training ---\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, config.device, epoch+1, scaler)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_results = {\"loss\": float('inf'), config.metric_to_track.replace('_', ' '): (-float('inf') if config.mode == 'max' else float('inf'))} # Default with formatted key\n",
    "        if dev_loader:\n",
    "            val_results = validate_epoch(model, dev_loader, config.device, epoch+1)\n",
    "            history['validation_results'].append(val_results)\n",
    "            print(\"  Validation Metrics:\")\n",
    "            metric_log_str = \"  \"\n",
    "            # Sort keys for consistent printing\n",
    "            sorted_keys = sorted(val_results.keys(), key=lambda x: (x.split()[0], int(x.split('@')[-1]) if '@' in x else -1))\n",
    "            for name in sorted_keys:\n",
    "                 metric_log_str += f\"{name}: {val_results[name]:.4f} | \"\n",
    "            print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "\n",
    "            # --- Scheduler Step ---\n",
    "            current_val_metric_for_scheduler = val_results.get(config.metric_to_track.replace('_', ' '), None) # Use formatted key\n",
    "            if current_val_metric_for_scheduler is not None:\n",
    "                lr_scheduler.step(current_val_metric_for_scheduler)\n",
    "                current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "                # Map LRs back to components for printing (adjust indices based on actual groups)\n",
    "                lr_map = {config.vision_encoder_lr: 'Vision', config.qformer_lr: 'QF', config.language_model_lr: 'LM', config.projection_lr: 'Proj'}\n",
    "                lr_str = \"  Current LRs: \" + \", \".join([f\"{lr_map.get(group['lr'], f'Group{i}')}={group['lr']:.2e}\" for i, group in enumerate(optimizer.param_groups)])\n",
    "                print(lr_str)\n",
    "\n",
    "            else:\n",
    "                print(f\"  Warning: Metric '{config.metric_to_track}' not found in validation results. Scheduler not stepped.\")\n",
    "        else:\n",
    "            print(\"  Validation skipped (no dev_loader).\")\n",
    "            history['validation_results'].append(None)\n",
    "\n",
    "        # --- Save Checkpoint & Early Stopping Logic ---\n",
    "        current_val_metric = val_results.get(config.metric_to_track.replace('_', ' '), -float('inf') if config.mode == \"max\" else float('inf')) # Use formatted key\n",
    "        is_best = False\n",
    "        improved = False\n",
    "\n",
    "        if dev_loader:\n",
    "            if config.mode == \"max\":\n",
    "                if current_val_metric > best_val_metric + config.early_stopping_min_delta:\n",
    "                    is_best = True\n",
    "                    improved = True\n",
    "            else: # config.mode == \"min\"\n",
    "                if current_val_metric < best_val_metric - config.early_stopping_min_delta:\n",
    "                    is_best = True\n",
    "                    improved = True\n",
    "\n",
    "            if is_best:\n",
    "                print(f\"  Metric '{config.metric_to_track}' improved from {best_val_metric:.4f} to {current_val_metric:.4f}\")\n",
    "                best_val_metric = current_val_metric\n",
    "                no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                print(f\"  Metric '{config.metric_to_track}' did not improve. Best: {best_val_metric:.4f}. Counter: {no_improve_epochs}/{config.early_stopping_patience}\")\n",
    "\n",
    "        # Prepare save dictionary - saving state_dict is generally preferred for large models\n",
    "        save_dict = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'validation_results': val_results,\n",
    "            'best_val_metric': best_val_metric,\n",
    "            'metric_tracked': config.metric_to_track,\n",
    "            'blip_config_dict': model.blip_model.config.to_dict() # Save base model config\n",
    "        }\n",
    "\n",
    "        # Save logic\n",
    "        best_checkpoint_path = os.path.join(config.model_path, \"blip2_retrieval_best.pt\")\n",
    "        final_epoch_path = os.path.join(config.model_path, f\"blip2_retrieval_epoch_{epoch+1}.pt\")\n",
    "\n",
    "        if config.save_best_only and dev_loader:\n",
    "            if is_best:\n",
    "                torch.save(save_dict, best_checkpoint_path)\n",
    "                print(f\"  Saved Best Model (Epoch {epoch+1}) to {best_checkpoint_path}\")\n",
    "        else:\n",
    "            torch.save(save_dict, final_epoch_path)\n",
    "            print(f\"  Saved Epoch {epoch+1} Checkpoint to {final_epoch_path}\")\n",
    "            if is_best and dev_loader:\n",
    "                 torch.save(save_dict, best_checkpoint_path)\n",
    "                 print(f\"  (Also saved as best model)\")\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {epoch_end_time - epoch_start_time:.2f} seconds ---\")\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if dev_loader and no_improve_epochs >= config.early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {config.early_stopping_patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    # --- End of Training ---\n",
    "    end_train_time = time.time()\n",
    "    total_train_time = end_train_time - start_train_time\n",
    "    print(f\"\\n=============== Training Finished ================\")\n",
    "    print(f\"Total Training Time: {total_train_time:.2f} seconds ({total_train_time/60:.2f} minutes)\")\n",
    "\n",
    "    # Save the final epoch's state dictionary separately\n",
    "    final_model_path = os.path.join(config.model_path, 'blip2_retrieval_final_epoch.pt')\n",
    "    # Make sure save_dict has the state from the *last* completed epoch\n",
    "    torch.save(save_dict, final_model_path)\n",
    "    print(f\"Final epoch model state saved to {final_model_path}\")\n",
    "\n",
    "    best_model_file = os.path.join(config.model_path, \"blip2_retrieval_best.pt\")\n",
    "    if dev_loader and os.path.exists(best_model_file):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) is saved at: {best_model_file}\")\n",
    "    elif dev_loader:\n",
    "        print(\"Best model checkpoint file not found. The final epoch model is saved.\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training (model, dataloader, optimizer, scheduler, scaler) not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Starting Test Set Evaluation ===============\n",
      "Skipping test evaluation: Test JSON or Processor not found/loaded.\n",
      "\n",
      "================= Evaluation Finished =================\n"
     ]
    }
   ],
   "source": [
    "# === Cell 12: Final Evaluation on Test Set ===\n",
    "print(\"\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_loader = None\n",
    "model_to_test = None\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "\n",
    "# 1. Check if test data and processor exist\n",
    "if os.path.exists(test_json_path) and 'processor' in globals() and processor:\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    try:\n",
    "        test_dataset = Blip2ImageCaptionDataset(\n",
    "            json_path=test_json_path,\n",
    "            image_base_path=config.image_path,\n",
    "            processor=processor,\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "\n",
    "        if test_dataset.data:\n",
    "            num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=config.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "        else:\n",
    "             print(\"Test dataset loaded but is empty. Skipping evaluation.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating test dataset/loader: {e}\")\n",
    "else:\n",
    "    print(\"Skipping test evaluation: Test JSON or Processor not found/loaded.\")\n",
    "\n",
    "# 2. Load Model for Testing if test_loader was created\n",
    "if test_loader:\n",
    "    try:\n",
    "        # Determine which model weights to load (best or final)\n",
    "        best_model_path = os.path.join(config.model_path, \"blip2_retrieval_best.pt\")\n",
    "        final_model_path = os.path.join(config.model_path, \"blip2_retrieval_final_epoch.pt\")\n",
    "\n",
    "        load_path = None\n",
    "        if os.path.exists(best_model_path):\n",
    "            load_path = best_model_path\n",
    "            print(f\"\\nAttempting to load best model weights from: {load_path}\")\n",
    "        elif os.path.exists(final_model_path):\n",
    "            load_path = final_model_path\n",
    "            print(f\"\\nBest model not found. Attempting to load final epoch weights from: {load_path}\")\n",
    "        else:\n",
    "            print(f\"\\nWARNING: No saved model checkpoints ('best' or 'final') found in {config.model_path}.\")\n",
    "\n",
    "        if load_path:\n",
    "            checkpoint = torch.load(load_path, map_location=config.device)\n",
    "\n",
    "            # Re-create model using saved config, then load state_dict\n",
    "            print(\"Re-creating model structure for testing...\")\n",
    "            if 'blip_config_dict' in checkpoint:\n",
    "                 saved_blip_config = Blip2Config.from_dict(checkpoint['blip_config_dict'])\n",
    "                 temp_config = config # Start with current config\n",
    "                 temp_config.blip_model_name = saved_blip_config._name_or_path\n",
    "                 print(f\"  Using base model config from checkpoint: {temp_config.blip_model_name}\")\n",
    "                 model_to_test = Blip2RetrievalModel(temp_config)\n",
    "            else:\n",
    "                 print(\"Warning: Blip config not found in checkpoint, using current CFG.\")\n",
    "                 model_to_test = Blip2RetrievalModel(config)\n",
    "\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            # Handle 'module.' prefix\n",
    "            if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "                print(\"Detected 'module.' prefix, removing for loading.\")\n",
    "                from collections import OrderedDict\n",
    "                new_state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "                state_dict = new_state_dict\n",
    "\n",
    "            load_result = model_to_test.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"  State dict loading result: {load_result}\")\n",
    "            if load_result.missing_keys:\n",
    "                 print(f\"  Warning: Missing keys: {load_result.missing_keys}\")\n",
    "            if load_result.unexpected_keys:\n",
    "                 print(f\"  Warning: Unexpected keys: {load_result.unexpected_keys}\")\n",
    "\n",
    "            # Ensure model is on device if not using device_map\n",
    "            if 'device_map' not in getattr(model_to_test.blip_model, 'hf_device_map', {}):\n",
    "                 model_to_test.to(config.device)\n",
    "\n",
    "            print(f\"Model weights loaded successfully from {load_path}\")\n",
    "\n",
    "            # --- Run Evaluation ---\n",
    "            print(\"\\nRunning evaluation on test set...\")\n",
    "            test_results = validate_epoch(model_to_test, test_loader, config.device, epoch_num=\"Test\")\n",
    "\n",
    "            print(\"\\n--- Test Set Results ---\")\n",
    "            metric_log_str = \"\"\n",
    "            sorted_keys = sorted(test_results.keys(), key=lambda x: (x.split()[0], int(x.split('@')[-1]) if '@' in x else -1))\n",
    "            for name in sorted_keys:\n",
    "                 value = test_results[name]\n",
    "                 metric_log_str += f\"  {name}: {value:.4f}\\n\"\n",
    "            print(metric_log_str.strip())\n",
    "            print(\"------------------------\")\n",
    "\n",
    "        else:\n",
    "             print(\"Evaluation skipped as no model weights were found to load.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR during test setup or evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n================= Evaluation Finished =================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot directory ensured at: /home/researcher/huypq69/TuningModels/train_plot\n",
      "No training history found or history is empty. Run training first to generate history.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Cell 13: Training Visualization (Adapted) ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plot_dir = \"train_plot\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "print(f\"Plot directory ensured at: {os.path.abspath(plot_dir)}\")\n",
    "\n",
    "def save_subplot_as_figure(subplot, save_path):\n",
    "    fig_new = plt.figure(figsize=(8, 6))\n",
    "    ax_new = fig_new.add_subplot(111)\n",
    "    lines = subplot.get_lines()\n",
    "    if not lines:\n",
    "        print(f\"Warning: No lines found in subplot for {save_path}\")\n",
    "        plt.close(fig_new)\n",
    "        return\n",
    "    labels = [line.get_label() for line in lines]\n",
    "    for line in lines:\n",
    "        ax_new.plot(line.get_xdata(), line.get_ydata(),\n",
    "                    color=line.get_color(),\n",
    "                    linestyle=line.get_linestyle(),\n",
    "                    marker=line.get_marker(),\n",
    "                    label=line.get_label())\n",
    "    ax_new.set_title(subplot.get_title())\n",
    "    ax_new.set_xlabel(subplot.get_xlabel())\n",
    "    ax_new.set_ylabel(subplot.get_ylabel())\n",
    "    ax_new.grid(True)\n",
    "    if any(label and not label.startswith('_') for label in labels):\n",
    "         ax_new.legend()\n",
    "    plt.tight_layout()\n",
    "    fig_new.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig_new)\n",
    "\n",
    "def plot_training_metrics(history):\n",
    "    if not history or not history.get('train_loss') or not history.get('validation_results'):\n",
    "        print(\"No training history available or history is incomplete.\")\n",
    "        return\n",
    "\n",
    "    valid_results = [res for res in history['validation_results'] if res is not None]\n",
    "    if not valid_results:\n",
    "        print(\"No valid validation results found. Plotting only training loss.\")\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "        ax.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')\n",
    "        ax.set_title('Training Loss over Epochs')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        save_path = os.path.join(plot_dir, f'training_loss.png')\n",
    "        fig.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Saved loss plot to: {save_path}\")\n",
    "        # plt.show() # Don't show in script mode\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    num_epochs_trained = len(history['train_loss'])\n",
    "    num_epochs_validated = len(valid_results)\n",
    "    epochs_train = range(1, num_epochs_trained + 1)\n",
    "    epochs_val = range(1, num_epochs_validated + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 13))\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16, y=1.02)\n",
    "\n",
    "    # --- Plot Loss ---\n",
    "    val_loss = [res.get('loss', float('nan')) for res in valid_results] # Use .get for safety\n",
    "    axes[0, 0].plot(epochs_train, history['train_loss'], 'b-o', label='Training Loss')\n",
    "    axes[0, 0].plot(epochs_val, val_loss, 'r-s', label='Validation Loss')\n",
    "    axes[0, 0].set_title('Loss over Epochs')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # --- Plot Accuracy (Average Accuracy) ---\n",
    "    metric_key_acc = 'avg acc' # Key from validate_epoch output\n",
    "    if metric_key_acc in valid_results[0]:\n",
    "        val_acc = [res[metric_key_acc] for res in valid_results]\n",
    "        axes[0, 1].plot(epochs_val, val_acc, 'g-^', label='Average Accuracy (Val)')\n",
    "        axes[0, 1].set_title('Validation Average Accuracy over Epochs')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "    else:\n",
    "        axes[0, 1].set_title(f'Validation Acc ({metric_key_acc}) (Not Found)')\n",
    "\n",
    "    # --- Plot Recall Metrics ---\n",
    "    has_recall = 'i2t recall R@1' in valid_results[0] # Check a representative key\n",
    "\n",
    "    if has_recall:\n",
    "        # I2T Recall\n",
    "        for k in [1, 5, 10]:\n",
    "            key = f'i2t recall R@{k}'\n",
    "            values = [res.get(key, float('nan')) for res in valid_results]\n",
    "            axes[1, 0].plot(epochs_val, values, marker='o', label=f'I2T R@{k}')\n",
    "        axes[1, 0].set_title('Image-to-Text Recall (Val)')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Recall')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "\n",
    "        # T2I Recall\n",
    "        for k in [1, 5, 10]:\n",
    "            key = f't2i recall R@{k}'\n",
    "            values = [res.get(key, float('nan')) for res in valid_results]\n",
    "            axes[1, 1].plot(epochs_val, values, marker='s', label=f'T2I R@{k}')\n",
    "        axes[1, 1].set_title('Text-to-Image Recall (Val)')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Recall')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "    else:\n",
    "        axes[1, 0].set_title('I2T Recall (Not Found)')\n",
    "        axes[1, 1].set_title('T2I Recall (Not Found)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    # Save individual plots\n",
    "    plot_names = ['loss', 'accuracy', 'i2t_recall', 't2i_recall']\n",
    "    for idx, name in enumerate(plot_names):\n",
    "        i, j = divmod(idx, 2)\n",
    "        save_path = os.path.join(plot_dir, f'training_{name}.png')\n",
    "        if axes[i, j].has_data():\n",
    "            save_subplot_as_figure(axes[i, j], save_path)\n",
    "            print(f\"Saved {name} plot to: {save_path}\")\n",
    "        else:\n",
    "            print(f\"Skipping save for {name} plot (no data).\")\n",
    "\n",
    "    # Save combined plot\n",
    "    combined_save_path = os.path.join(plot_dir, 'training_metrics_combined.png')\n",
    "    fig.savefig(combined_save_path, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved combined plot to: {combined_save_path}\")\n",
    "\n",
    "    # plt.show() # Avoid showing plots in a script run\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "# Check if the 'history' variable exists from the training loop before plotting\n",
    "if 'history' in locals() and isinstance(history, dict) and history.get('train_loss'):\n",
    "    plot_training_metrics(history)\n",
    "else:\n",
    "    print(\"No training history found or history is empty. Run training first to generate history.\")\n",
    "\n",
    "\n",
    "# --- END OF SCRIPT ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
