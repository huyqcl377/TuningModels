{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ViBLIP Fine-tuning for Vietnamese Image Retrieval by Text\n",
    "# Generated from Jupyter Notebook\n",
    "\n",
    "\n",
    "# === Cell 1: Installs and Imports ===\n",
    "# !pip install -q transformers torch torchvision torchaudio Pillow tqdm accelerate bitsandbytes sentencepiece\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Blip2Processor, Blip2Model, Blip2Config, AutoTokenizer # AutoTokenizer might be needed for processor loading edge cases\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm # Use standard tqdm if not in notebook: from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time # For timing epochs\n",
    "import transformers\n",
    "from torch.cuda.amp import GradScaler, autocast # For mixed precision\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "\n",
    "# === Cell 2: Configuration Class (CFG) ===\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    # Base directory where your train.json, dev.json, test.json are located\n",
    "    data_path = \"./json_data/\" # ADJUST THIS PATH\n",
    "    # Base directory for images referenced in JSON files (relative to data_path or absolute)\n",
    "    image_path = \"./data/\" # ADJUST THIS PATH\n",
    "\n",
    "    # Output directory for saved models\n",
    "    model_path = \"./ViBLIP_retrieval\"\n",
    "\n",
    "    # --- BLIP Model Selection ---\n",
    "    # Common options: Salesforce/blip2-opt-2.7b, Salesforce/blip2-flan-t5-xl\n",
    "    # Choose based on available resources. OPT models might be slightly smaller.\n",
    "    selected_blip_model = \"Salesforce/blip2-opt-2.7b\" # ADJUST IF NEEDED\n",
    "\n",
    "    # --- Model parameters ---\n",
    "    blip_model_name = selected_blip_model\n",
    "    blip_processor_name = selected_blip_model\n",
    "    projection_dim = 256 # Shared latent space dimension (e.g., 256, 512, 768)\n",
    "    # Freeze parts of the model? BLIP2's LLM is very large.\n",
    "    freeze_vision_model = False\n",
    "    freeze_language_model = True # HIGHLY RECOMMENDED to freeze LLM unless you have >> 40GB VRAM\n",
    "    freeze_qformer = False\n",
    "    # Quantization (Requires bitsandbytes). Reduces memory significantly.\n",
    "    load_in_8bit = False # Set to True if memory is tight\n",
    "\n",
    "    # --- Training parameters ---\n",
    "    seed = 42\n",
    "    # Reduce batch size significantly compared to CLIP due to BLIP2's size\n",
    "    batch_size = 16  # START LOW (e.g., 4, 8, 16) and increase based on GPU memory\n",
    "    num_workers = 4  # Adjust based on system capability\n",
    "    # Learning rates for different components (tune these)\n",
    "    vision_encoder_lr = 1e-5\n",
    "    qformer_lr = 2e-5\n",
    "    language_model_lr = 1e-6 # Only relevant if freeze_language_model=False\n",
    "    projection_lr = 1e-4 # Projection head can often learn faster\n",
    "    weight_decay = 1e-3\n",
    "    patience = 3 # Scheduler patience\n",
    "    factor = 0.8 # Scheduler reduction factor\n",
    "    epochs = 20 # Adjust as needed\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Use mixed precision to save memory and potentially speed up training\n",
    "    use_amp = True\n",
    "\n",
    "    # --- Image/Text parameters (mostly handled by processor) ---\n",
    "    # Processor determines image size (usually 224 for BLIP2)\n",
    "    max_length = 64 # Max text sequence length for tokenizer\n",
    "\n",
    "    # --- Loss/Saving parameters ---\n",
    "    temperature = 0.07 # Initial temperature for scaling logits\n",
    "    learnable_temperature = True # Whether the logit_scale is learnable\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"avg_R@1\" # Common retrieval metric ('avg_acc', 'i2t R@1', 't2i R@1', 'avg_R@5', etc.)\n",
    "    mode = \"max\" # Mode for scheduler/saving based on metric_to_track ('max' for recall/acc, 'min' for loss)\n",
    "    early_stopping_patience = 5 # Epochs with no improvement before stopping\n",
    "    early_stopping_min_delta = 0.001 # Minimum change to qualify as improvement\n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Selected BLIP2 Model: {config.blip_model_name}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_path)}\")\n",
    "\n",
    "\n",
    "# === Cell 3: Seeding for Reproducibility ===\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # For multi-GPU\n",
    "        # torch.backends.cudnn.deterministic = True # Can impact performance\n",
    "        # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "\n",
    "# === Cell 4: Metric Calculation Utilities ===\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        if torch.is_tensor(val):\n",
    "             val = val.item()\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    \"\"\"Calculates Recall@k for image-text retrieval.\"\"\"\n",
    "    n = similarity_matrix.shape[1-dim] # Number of samples (e.g., images if dim=1 for T2I)\n",
    "    k_eff = min(k, similarity_matrix.shape[dim]) # Effective k cannot be larger than candidate pool size\n",
    "    if k_eff == 0 or n == 0: return 0.0\n",
    "\n",
    "    top_k_indices = torch.topk(similarity_matrix, k_eff, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "\n",
    "    correct_count = 0\n",
    "    if dim == 0: # I2T: Find correct text (row index) for each image (column)\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]:\n",
    "                correct_count += 1\n",
    "    elif dim == 1: # T2I: Find correct image (column index) for each text (row)\n",
    "        for txt_idx in range(n):\n",
    "             if ground_truth[txt_idx] in top_k_indices[txt_idx, :]:\n",
    "                correct_count += 1\n",
    "    else:\n",
    "        raise ValueError(\"dim must be 0 or 1\")\n",
    "\n",
    "    return correct_count / n if n > 0 else 0.0\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    \"\"\"Computes retrieval metrics for a batch or validation set.\"\"\"\n",
    "    if image_embeddings.device != text_embeddings.device:\n",
    "        text_embeddings = text_embeddings.to(image_embeddings.device)\n",
    "\n",
    "    sim_matrix = text_embeddings @ image_embeddings.T\n",
    "    sim_matrix = sim_matrix.float()\n",
    "    n = sim_matrix.shape[0]\n",
    "    if n == 0:\n",
    "        return {\n",
    "            \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "            \"avg_cosine_sim\": 0.0,\n",
    "            \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"avg_R@1\": 0.0, \"avg_R@5\": 0.0, \"avg_R@10\": 0.0\n",
    "        }\n",
    "\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "    t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "    t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "    avg_acc = (i2t_acc + t2i_acc) / 2\n",
    "    avg_cosine_sim = torch.diagonal(sim_matrix).mean().item()\n",
    "\n",
    "    i2t_recall = {}\n",
    "    t2i_recall = {}\n",
    "    recall_k_values = [1, 5, 10]\n",
    "\n",
    "    for k in recall_k_values:\n",
    "        k_str = f\"R@{k}\"\n",
    "        i2t_recall[k_str] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "        t2i_recall[k_str] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "\n",
    "    avg_recall = {}\n",
    "    for k in recall_k_values:\n",
    "        k_str = f\"R@{k}\"\n",
    "        avg_recall[f\"avg_{k_str}\"] = (i2t_recall[k_str] + t2i_recall[k_str]) / 2\n",
    "\n",
    "    metrics = {\n",
    "        \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "        \"avg_cosine_sim\": avg_cosine_sim,\n",
    "        \"i2t_recall\": i2t_recall,\n",
    "        \"t2i_recall\": t2i_recall,\n",
    "        **avg_recall\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")\n",
    "\n",
    "\n",
    "# === Cell 5: Dataset Class Definition ===\n",
    "class Blip2ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, processor, max_length):\n",
    "        super().__init__()\n",
    "        print(f\"Attempting to load data from: {os.path.abspath(json_path)}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                self.data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: JSON file not found at {json_path}\")\n",
    "            self.data = []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error: Could not decode JSON from {json_path}: {e}\")\n",
    "            self.data = []\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred loading {json_path}: {e}\")\n",
    "            self.data = []\n",
    "\n",
    "        print(f\"Found {len(self.data)} samples in {os.path.basename(json_path)}.\")\n",
    "        self.image_base_path = image_base_path\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Determine image size from processor\n",
    "        try:\n",
    "            # Try accessing size directly (newer transformers)\n",
    "            if isinstance(self.processor.image_processor.size, dict):\n",
    "                self.img_size = self.processor.image_processor.size['height'] # Or 'shortest_edge'\n",
    "            else: # Older style might be int or tuple\n",
    "                 self.img_size = self.processor.image_processor.size\n",
    "                 if isinstance(self.img_size, (tuple, list)): self.img_size = self.img_size[0]\n",
    "        except AttributeError:\n",
    "            print(\"Warning: Could not determine image size from processor, defaulting to 224.\")\n",
    "            self.img_size = 224\n",
    "        print(f\"Using image size: {self.img_size}x{self.img_size}\")\n",
    "\n",
    "        if not os.path.isdir(self.image_base_path):\n",
    "             print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data):\n",
    "             raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path')\n",
    "        captions = item.get('caption', [])\n",
    "        caption = captions[0] if captions else \"\" # Take the first caption\n",
    "\n",
    "        # Initialize with dummy data\n",
    "        dummy_image = Image.new('RGB', (self.img_size, self.img_size))\n",
    "        try:\n",
    "            pixel_values = self.processor(images=dummy_image, return_tensors=\"pt\")['pixel_values'].squeeze()\n",
    "        except Exception as e:\n",
    "             print(f\"Error processing dummy image: {e}\")\n",
    "             pixel_values = torch.zeros((3, self.img_size, self.img_size)) # Fallback tensor\n",
    "\n",
    "        image_loaded_successfully = False\n",
    "        if relative_image_path:\n",
    "            image_path = os.path.normpath(os.path.join(self.image_base_path, relative_image_path))\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                processed_output = self.processor(images=image, text=None, return_tensors=\"pt\") # Only process image here\n",
    "                pixel_values = processed_output['pixel_values'].squeeze()\n",
    "                image_loaded_successfully = True\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Img not found at {image_path}. Using dummy image for idx {idx}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading image {image_path}: {e}. Using dummy image for idx {idx}.\")\n",
    "        else:\n",
    "             print(f\"Warning: Missing 'image_path' for item at index {idx}. Using dummy image.\")\n",
    "\n",
    "        # Process text\n",
    "        try:\n",
    "            text_inputs = self.processor(\n",
    "                images=None, # Important: don't re-process image\n",
    "                text=caption,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = text_inputs['input_ids'].squeeze()\n",
    "            attention_mask = text_inputs['attention_mask'].squeeze()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text '{caption}' for idx {idx}: {e}\")\n",
    "            # Create dummy text inputs if error occurs\n",
    "            input_ids = torch.zeros(self.max_length, dtype=torch.long)\n",
    "            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n",
    "\n",
    "        # Ensure tensors are 1D after squeeze\n",
    "        if input_ids.dim() > 1: input_ids = input_ids.view(-1)\n",
    "        if attention_mask.dim() > 1: attention_mask = attention_mask.view(-1)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "print(\"Blip2ImageCaptionDataset class defined.\")\n",
    "\n",
    "\n",
    "# === Cell 6: Model Definition (BLIP2 Retrieval Model, Loss) ===\n",
    "class Blip2RetrievalModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config_train = config # Store training config\n",
    "        print(f\"Initializing BLIP2 Model: {config.blip_model_name}\")\n",
    "\n",
    "        # Load the base BLIP2 model.\n",
    "        # Use Blip2Model for feature extraction, not Blip2ForConditionalGeneration.\n",
    "        try:\n",
    "            load_kwargs = {}\n",
    "            if config.load_in_8bit:\n",
    "                 if not torch.cuda.is_available() or torch.cuda.get_device_capability()[0] < 7:\n",
    "                    print(\"Warning: 8-bit loading requested but not supported on this GPU or CUDA version. Loading in default precision.\")\n",
    "                 else:\n",
    "                    print(\"Attempting to load model in 8-bit.\")\n",
    "                    load_kwargs['load_in_8bit'] = True\n",
    "                    load_kwargs['device_map'] = 'auto' # device_map needed for 8-bit\n",
    "\n",
    "            self.blip_model = Blip2Model.from_pretrained(\n",
    "                config.blip_model_name,\n",
    "                **load_kwargs\n",
    "            )\n",
    "\n",
    "            # If not using device_map, explicitly move model parts if needed\n",
    "            if 'device_map' not in load_kwargs and config.device != torch.device('cpu'):\n",
    "                print(f\"Manually moving model components to {config.device}\")\n",
    "                self.blip_model.to(config.device)\n",
    "            elif 'device_map' in load_kwargs:\n",
    "                print(f\"Model loaded with device_map: {self.blip_model.hf_device_map}\")\n",
    "\n",
    "        except ImportError as e:\n",
    "             if 'bitsandbytes' in str(e):\n",
    "                 print(\"ERROR: bitsandbytes library not found. Please install it (`pip install bitsandbytes`) to use 8-bit loading.\")\n",
    "             else:\n",
    "                 print(f\"ERROR loading base BLIP2 model: {e}\")\n",
    "             raise e\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading base BLIP2 model: {e}\")\n",
    "            print(\"Check model name, internet connection, and available memory.\")\n",
    "            raise e\n",
    "\n",
    "        # --- Freeze Components ---\n",
    "        if config.freeze_vision_model:\n",
    "            print(\"  Freezing Vision Model parameters.\")\n",
    "            for param in self.blip_model.vision_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        if config.freeze_qformer:\n",
    "            print(\"  Freezing Q-Former parameters.\")\n",
    "            for param in self.blip_model.qformer.parameters():\n",
    "                param.requires_grad = False\n",
    "        if config.freeze_language_model:\n",
    "            if hasattr(self.blip_model, 'language_model') and self.blip_model.language_model is not None:\n",
    "                 print(\"  Freezing Language Model parameters.\")\n",
    "                 for param in self.blip_model.language_model.parameters():\n",
    "                    param.requires_grad = False\n",
    "                 # Also freeze the language projection if it exists (maps Q-Former to LM input)\n",
    "                 if hasattr(self.blip_model, 'language_projection') and self.blip_model.language_projection is not None:\n",
    "                     for param in self.blip_model.language_projection.parameters():\n",
    "                         param.requires_grad = False\n",
    "            else:\n",
    "                 print(\"  Language model component not found or is None, skipping freeze.\")\n",
    "\n",
    "        # Determine input dimension for projection heads (usually Q-Former output dim)\n",
    "        qformer_hidden_size = self.blip_model.config.qformer_config.hidden_size\n",
    "        print(f\"  Q-Former hidden size (input to projection): {qformer_hidden_size}\")\n",
    "\n",
    "        # --- Projection Heads ---\n",
    "        # Use separate projections for image and text features coming from Q-Former\n",
    "        self.image_projection = nn.Linear(qformer_hidden_size, config.projection_dim, bias=False)\n",
    "        self.text_projection = nn.Linear(qformer_hidden_size, config.projection_dim, bias=False)\n",
    "        print(f\"  Added projection heads: {qformer_hidden_size} -> {config.projection_dim}\")\n",
    "\n",
    "        # --- Learnable Temperature ---\n",
    "        if config.learnable_temperature:\n",
    "            self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / config.temperature))\n",
    "            print(f\"  Using learnable temperature (logit scale), initialized to {self.logit_scale.exp().item():.4f}\")\n",
    "        else:\n",
    "            self.logit_scale = torch.tensor(np.log(1 / config.temperature)) # Keep on CPU initially, move in forward\n",
    "            print(f\"  Using fixed temperature: {config.temperature}\")\n",
    "\n",
    "        # Move projections to the correct device if model wasn't loaded with device_map\n",
    "        if 'device_map' not in getattr(self.blip_model, 'hf_device_map', {}):\n",
    "            model_device = config.device # Get device from config\n",
    "            self.image_projection.to(model_device)\n",
    "            self.text_projection.to(model_device)\n",
    "            if isinstance(self.logit_scale, nn.Parameter):\n",
    "                 self.logit_scale.to(model_device)\n",
    "            else:\n",
    "                 self.logit_scale = self.logit_scale.to(model_device) # Move tensor\n",
    "\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # Extract features - QFormer output is used for multimodal understanding\n",
    "        image_outputs = self.blip_model.get_image_features(pixel_values=pixel_values, return_dict=True)\n",
    "        text_outputs = self.blip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "\n",
    "        # Use the pooled output (representation of [CLS] query token)\n",
    "        image_features = image_outputs.last_hidden_state[:, 0, :] # Shape: [batch_size, qformer_hidden_size]\n",
    "        text_features = text_outputs.last_hidden_state[:, 0, :]   # Shape: [batch_size, qformer_hidden_size]\n",
    "\n",
    "        # Project features into the shared embedding space\n",
    "        image_embeds = self.image_projection(image_features)\n",
    "        text_embeds = self.text_projection(text_features)\n",
    "\n",
    "        # Normalize embeddings\n",
    "        image_embeds_norm = F.normalize(image_embeds, p=2, dim=-1)\n",
    "        text_embeds_norm = F.normalize(text_embeds, p=2, dim=-1)\n",
    "\n",
    "        # Cosine similarity scaled by temperature\n",
    "        # Ensure logit_scale is on the same device as embeddings\n",
    "        current_logit_scale = self.logit_scale.exp().to(image_embeds_norm.device)\n",
    "        logits_per_image = current_logit_scale * image_embeds_norm @ text_embeds_norm.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text, image_embeds_norm, text_embeds_norm\n",
    "\n",
    "# --- Loss Function (Contrastive Loss) ---\n",
    "def contrastive_loss(logits_per_image, logits_per_text):\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    if batch_size == 0: return torch.tensor(0.0, device=logits_per_image.device, requires_grad=True)\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "    loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "    total_loss = (loss_img + loss_txt) / 2.0\n",
    "    return total_loss\n",
    "\n",
    "print(\"BLIP2 Retrieval Model and Loss Function defined.\")\n",
    "\n",
    "\n",
    "# === Cell 7: Training and Validation Epoch Functions ===\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch_num, scaler):\n",
    "    model.train()\n",
    "    loss_meter = AvgMeter(f\"Train Loss E{epoch_num}\")\n",
    "    # Use standard tqdm if not in notebook\n",
    "    try:\n",
    "        from tqdm.notebook import tqdm as tqdm_notebook\n",
    "        progress_bar = tqdm_notebook(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    except ImportError:\n",
    "        from tqdm import tqdm\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move batch to device\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_size = pixel_values.size(0)\n",
    "        if batch_size == 0: continue\n",
    "\n",
    "        # Automatic Mixed Precision\n",
    "        with autocast(enabled=config.use_amp):\n",
    "            logits_per_image, logits_per_text, _, _ = model(pixel_values, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(logits_per_image, logits_per_text)\n",
    "\n",
    "        # Gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_meter.update(loss.item(), batch_size)\n",
    "        progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    return loss_meter.avg\n",
    "\n",
    "def validate_epoch(model, dataloader, device, epoch_num):\n",
    "    model.eval()\n",
    "    loss_meter = AvgMeter(f\"Val Loss E{epoch_num}\")\n",
    "    # Initialize metric accumulators\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "\n",
    "    # Use standard tqdm if not in notebook\n",
    "    try:\n",
    "        from tqdm.notebook import tqdm as tqdm_notebook\n",
    "        progress_bar = tqdm_notebook(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    except ImportError:\n",
    "        from tqdm import tqdm\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = pixel_values.size(0)\n",
    "            if batch_size == 0: continue\n",
    "\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                logits_per_image, logits_per_text, image_embeds, text_embeds = model(pixel_values, input_ids, attention_mask)\n",
    "                loss = contrastive_loss(logits_per_image, logits_per_text)\n",
    "\n",
    "            loss_meter.update(loss.item(), batch_size)\n",
    "\n",
    "            # Store embeddings (move to CPU to conserve GPU memory during validation)\n",
    "            all_image_embeddings.append(image_embeds.cpu())\n",
    "            all_text_embeddings.append(text_embeds.cpu())\n",
    "\n",
    "            # Display running loss in progress bar\n",
    "            progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    # Concatenate all embeddings\n",
    "    if not all_image_embeddings or not all_text_embeddings:\n",
    "         print(\"Warning: No embeddings collected during validation.\")\n",
    "         # Return zero/default metrics if no data processed\n",
    "         zero_metrics = { \"loss\": loss_meter.avg, \"avg_acc\": 0.0, \"avg_cosine_sim\": 0.0,\n",
    "                           \"i2t R@1\": 0.0, \"i2t R@5\": 0.0, \"i2t R@10\": 0.0,\n",
    "                           \"t2i R@1\": 0.0, \"t2i R@5\": 0.0, \"t2i R@10\": 0.0,\n",
    "                           \"avg R@1\": 0.0, \"avg R@5\": 0.0, \"avg R@10\": 0.0 }\n",
    "         # Reformat keys for consistency with normal return\n",
    "         return {k.replace('_', ' '): v for k,v in zero_metrics.items()}\n",
    "\n",
    "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "\n",
    "    # Compute metrics over the entire validation set on the specified device\n",
    "    print(f\"\\nComputing metrics over {all_image_embeddings.shape[0]} validation samples...\")\n",
    "    validation_metrics = compute_metrics(all_image_embeddings.to(device), all_text_embeddings.to(device))\n",
    "\n",
    "    # Combine loss with computed metrics\n",
    "    final_results = {\"loss\": loss_meter.avg}\n",
    "    # Flatten the recall dictionaries for easier logging/history tracking\n",
    "    for k, v in validation_metrics.items():\n",
    "        if isinstance(v, dict):\n",
    "            for recall_k, recall_v in v.items():\n",
    "                final_results[f\"{k.replace('_', ' ')} {recall_k}\"] = recall_v\n",
    "        else:\n",
    "            final_results[k.replace('_', ' ')] = v # Replace underscores for keys like avg_acc\n",
    "\n",
    "    return final_results\n",
    "\n",
    "print(\"Training and Validation epoch functions defined.\")\n",
    "\n",
    "\n",
    "# === Cell 8: Setup - BLIP2 Processor ===\n",
    "print(f\"Loading BLIP2 Processor: {config.blip_processor_name}\")\n",
    "try:\n",
    "    processor = Blip2Processor.from_pretrained(config.blip_processor_name)\n",
    "    print(\"Processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading processor '{config.blip_processor_name}': {e}\")\n",
    "    processor = None # Ensure processor is None if loading fails\n",
    "    # raise e # Optionally stop execution\n",
    "\n",
    "\n",
    "# === Cell 9: Setup - Datasets and DataLoaders ===\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "\n",
    "if processor:\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_json = os.path.join(config.data_path, \"train.json\")\n",
    "    dev_json = os.path.join(config.data_path, \"dev.json\")\n",
    "\n",
    "    train_dataset = Blip2ImageCaptionDataset(\n",
    "        json_path=train_json,\n",
    "        image_base_path=config.image_path,\n",
    "        processor=processor,\n",
    "        max_length=config.max_length\n",
    "    )\n",
    "    dev_dataset = Blip2ImageCaptionDataset(\n",
    "        json_path=dev_json,\n",
    "        image_base_path=config.image_path,\n",
    "        processor=processor,\n",
    "        max_length=config.max_length\n",
    "    )\n",
    "\n",
    "    if not train_dataset.data:\n",
    "        print(\"\\nERROR: Failed to load training data. Check 'train_json' path and format.\")\n",
    "    if not dev_dataset.data:\n",
    "         print(\"\\nWARNING: Failed to load validation data. Validation steps will be skipped or may error.\")\n",
    "\n",
    "    print(\"\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    if train_dataset.data:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=True # Drop last incomplete batch for more stable training steps\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "    else:\n",
    "        print(\"Skipping train loader creation due to missing training data.\")\n",
    "\n",
    "    if dev_dataset.data:\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False # Keep last batch for full validation\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "    else:\n",
    "        print(\"Skipping validation loader creation due to missing validation data.\")\n",
    "\n",
    "    if not train_loader:\n",
    "         print(\"\\nERROR: Train loader could not be created. Cannot proceed with training.\")\n",
    "\n",
    "else:\n",
    "     print(\"ERROR: BLIP2 Processor not loaded. Skipping dataset and dataloader creation.\")\n",
    "\n",
    "\n",
    "# === Cell 10: Setup - Model, Optimizer, Scheduler, AMP Scaler ===\n",
    "model = None\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "scaler = None\n",
    "\n",
    "print(\"\\nInitializing model components...\")\n",
    "try:\n",
    "    # Pass the config object to the model\n",
    "    model = Blip2RetrievalModel(config) # Model is moved to device inside its __init__ if device_map not used\n",
    "    print(f\"\\nBlip2RetrievalModel initialized.\")\n",
    "    # Calculate trainable params AFTER potential freezing\n",
    "    num_params_total = sum(p.numel() for p in model.parameters())\n",
    "    num_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {num_params_total / 1e6:.2f} M\")\n",
    "    print(f\"Trainable parameters: {num_params_trainable / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing BLIP2 model: {e}\")\n",
    "    model = None # Ensure model is None if init fails\n",
    "\n",
    "# --- Optimizer Setup ---\n",
    "if model: # Check if model was created\n",
    "    print(\"\\nSetting up optimizer...\")\n",
    "    # Get parameters based on their names/modules\n",
    "    vision_params = [p for n, p in model.blip_model.vision_model.named_parameters() if p.requires_grad]\n",
    "    qformer_params = [p for n, p in model.blip_model.qformer.named_parameters() if p.requires_grad]\n",
    "\n",
    "    language_params = []\n",
    "    if hasattr(model.blip_model, 'language_model') and model.blip_model.language_model is not None:\n",
    "        language_params.extend([p for n, p in model.blip_model.language_model.named_parameters() if p.requires_grad])\n",
    "    if hasattr(model.blip_model, 'language_projection') and model.blip_model.language_projection is not None:\n",
    "         language_params.extend([p for n, p in model.blip_model.language_projection.named_parameters() if p.requires_grad])\n",
    "\n",
    "    projection_params = [p for n, p in model.named_parameters() if ('image_projection' in n or 'text_projection' in n or 'logit_scale' in n) and p.requires_grad]\n",
    "\n",
    "    print(f\"  Param counts (Trainable): Vision={len(vision_params)}, QFormer={len(qformer_params)}, LM={len(language_params)}{' (FROZEN)' if config.freeze_language_model and not language_params else ''}, Projection={len(projection_params)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": vision_params, \"lr\": config.vision_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": qformer_params, \"lr\": config.qformer_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": language_params, \"lr\": config.language_model_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": projection_params, \"lr\": config.projection_lr, \"weight_decay\": config.weight_decay},\n",
    "    ]\n",
    "\n",
    "    # Filter out groups with zero parameters\n",
    "    optimizer_grouped_parameters = [g for g in optimizer_grouped_parameters if g['params']]\n",
    "\n",
    "    if not optimizer_grouped_parameters:\n",
    "         print(\"ERROR: No trainable parameters found for the optimizer. Check freezing flags and model structure.\")\n",
    "    else:\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "        print(f\"Optimizer AdamW initialized.\")\n",
    "\n",
    "        # --- LR Scheduler Setup ---\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=config.mode,\n",
    "            factor=config.factor,\n",
    "            patience=config.patience\n",
    "        )\n",
    "        print(f\"LR Scheduler ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.factor}, patience={config.patience})\")\n",
    "\n",
    "        # --- Early Stopping Setup ---\n",
    "        early_stopping_counter = 0\n",
    "        print(f\"Early stopping initialized (patience={config.early_stopping_patience}, min_delta={config.early_stopping_min_delta})\")\n",
    "\n",
    "        # --- AMP GradScaler Setup ---\n",
    "        scaler = GradScaler(enabled=config.use_amp)\n",
    "        print(f\"AMP GradScaler initialized ({'enabled' if config.use_amp else 'disabled'}).\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not initialized. Skipping optimizer/scheduler/scaler setup.\")\n",
    "\n",
    "\n",
    "# === Cell 11: Training Loop ===\n",
    "# Check prerequisites exist before starting loop\n",
    "if model and train_loader and optimizer and lr_scheduler and scaler:\n",
    "    print(f\"\\nStarting training for {config.epochs} epochs...\")\n",
    "    print(f\"Tracking metric: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "    print(f\"Using AMP: {config.use_amp}\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    history = {'train_loss': [], 'validation_results': []}\n",
    "    start_train_time = time.time()\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "\n",
    "        # --- Training ---\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, config.device, epoch+1, scaler)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_results = {\"loss\": float('inf'), config.metric_to_track.replace('_', ' '): (-float('inf') if config.mode == 'max' else float('inf'))} # Default with formatted key\n",
    "        if dev_loader:\n",
    "            val_results = validate_epoch(model, dev_loader, config.device, epoch+1)\n",
    "            history['validation_results'].append(val_results)\n",
    "            print(\"  Validation Metrics:\")\n",
    "            metric_log_str = \"  \"\n",
    "            # Sort keys for consistent printing\n",
    "            sorted_keys = sorted(val_results.keys(), key=lambda x: (x.split()[0], int(x.split('@')[-1]) if '@' in x else -1))\n",
    "            for name in sorted_keys:\n",
    "                 metric_log_str += f\"{name}: {val_results[name]:.4f} | \"\n",
    "            print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "\n",
    "            # --- Scheduler Step ---\n",
    "            current_val_metric_for_scheduler = val_results.get(config.metric_to_track.replace('_', ' '), None) # Use formatted key\n",
    "            if current_val_metric_for_scheduler is not None:\n",
    "                lr_scheduler.step(current_val_metric_for_scheduler)\n",
    "                current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "                # Map LRs back to components for printing (adjust indices based on actual groups)\n",
    "                lr_map = {config.vision_encoder_lr: 'Vision', config.qformer_lr: 'QF', config.language_model_lr: 'LM', config.projection_lr: 'Proj'}\n",
    "                lr_str = \"  Current LRs: \" + \", \".join([f\"{lr_map.get(group['lr'], f'Group{i}')}={group['lr']:.2e}\" for i, group in enumerate(optimizer.param_groups)])\n",
    "                print(lr_str)\n",
    "\n",
    "            else:\n",
    "                print(f\"  Warning: Metric '{config.metric_to_track}' not found in validation results. Scheduler not stepped.\")\n",
    "        else:\n",
    "            print(\"  Validation skipped (no dev_loader).\")\n",
    "            history['validation_results'].append(None)\n",
    "\n",
    "        # --- Save Checkpoint & Early Stopping Logic ---\n",
    "        current_val_metric = val_results.get(config.metric_to_track.replace('_', ' '), -float('inf') if config.mode == \"max\" else float('inf')) # Use formatted key\n",
    "        is_best = False\n",
    "        improved = False\n",
    "\n",
    "        if dev_loader:\n",
    "            if config.mode == \"max\":\n",
    "                if current_val_metric > best_val_metric + config.early_stopping_min_delta:\n",
    "                    is_best = True\n",
    "                    improved = True\n",
    "            else: # config.mode == \"min\"\n",
    "                if current_val_metric < best_val_metric - config.early_stopping_min_delta:\n",
    "                    is_best = True\n",
    "                    improved = True\n",
    "\n",
    "            if is_best:\n",
    "                print(f\"  Metric '{config.metric_to_track}' improved from {best_val_metric:.4f} to {current_val_metric:.4f}\")\n",
    "                best_val_metric = current_val_metric\n",
    "                no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                print(f\"  Metric '{config.metric_to_track}' did not improve. Best: {best_val_metric:.4f}. Counter: {no_improve_epochs}/{config.early_stopping_patience}\")\n",
    "\n",
    "        # Prepare save dictionary - saving state_dict is generally preferred for large models\n",
    "        save_dict = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'validation_results': val_results,\n",
    "            'best_val_metric': best_val_metric,\n",
    "            'metric_tracked': config.metric_to_track,\n",
    "            'blip_config_dict': model.blip_model.config.to_dict() # Save base model config\n",
    "        }\n",
    "\n",
    "        # Save logic\n",
    "        best_checkpoint_path = os.path.join(config.model_path, \"blip2_retrieval_best.pt\")\n",
    "        final_epoch_path = os.path.join(config.model_path, f\"blip2_retrieval_epoch_{epoch+1}.pt\")\n",
    "\n",
    "        if config.save_best_only and dev_loader:\n",
    "            if is_best:\n",
    "                torch.save(save_dict, best_checkpoint_path)\n",
    "                print(f\"  Saved Best Model (Epoch {epoch+1}) to {best_checkpoint_path}\")\n",
    "        else:\n",
    "            torch.save(save_dict, final_epoch_path)\n",
    "            print(f\"  Saved Epoch {epoch+1} Checkpoint to {final_epoch_path}\")\n",
    "            if is_best and dev_loader:\n",
    "                 torch.save(save_dict, best_checkpoint_path)\n",
    "                 print(f\"  (Also saved as best model)\")\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {epoch_end_time - epoch_start_time:.2f} seconds ---\")\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if dev_loader and no_improve_epochs >= config.early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {config.early_stopping_patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    # --- End of Training ---\n",
    "    end_train_time = time.time()\n",
    "    total_train_time = end_train_time - start_train_time\n",
    "    print(f\"\\n=============== Training Finished ================\")\n",
    "    print(f\"Total Training Time: {total_train_time:.2f} seconds ({total_train_time/60:.2f} minutes)\")\n",
    "\n",
    "    # Save the final epoch's state dictionary separately\n",
    "    final_model_path = os.path.join(config.model_path, 'blip2_retrieval_final_epoch.pt')\n",
    "    # Make sure save_dict has the state from the *last* completed epoch\n",
    "    torch.save(save_dict, final_model_path)\n",
    "    print(f\"Final epoch model state saved to {final_model_path}\")\n",
    "\n",
    "    best_model_file = os.path.join(config.model_path, \"blip2_retrieval_best.pt\")\n",
    "    if dev_loader and os.path.exists(best_model_file):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) is saved at: {best_model_file}\")\n",
    "    elif dev_loader:\n",
    "        print(\"Best model checkpoint file not found. The final epoch model is saved.\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training (model, dataloader, optimizer, scheduler, scaler) not met. Training loop skipped.\")\n",
    "\n",
    "\n",
    "# === Cell 12: Final Evaluation on Test Set ===\n",
    "print(\"\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_loader = None\n",
    "model_to_test = None\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "\n",
    "# 1. Check if test data and processor exist\n",
    "if os.path.exists(test_json_path) and 'processor' in globals() and processor:\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    try:\n",
    "        test_dataset = Blip2ImageCaptionDataset(\n",
    "            json_path=test_json_path,\n",
    "            image_base_path=config.image_path,\n",
    "            processor=processor,\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "\n",
    "        if test_dataset.data:\n",
    "            num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=config.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "                drop_last=False\n",
    "            )\n",
    "            print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "        else:\n",
    "             print(\"Test dataset loaded but is empty. Skipping evaluation.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating test dataset/loader: {e}\")\n",
    "else:\n",
    "    print(\"Skipping test evaluation: Test JSON or Processor not found/loaded.\")\n",
    "\n",
    "# 2. Load Model for Testing if test_loader was created\n",
    "if test_loader:\n",
    "    try:\n",
    "        # Determine which model weights to load (best or final)\n",
    "        best_model_path = os.path.join(config.model_path, \"blip2_retrieval_best.pt\")\n",
    "        final_model_path = os.path.join(config.model_path, \"blip2_retrieval_final_epoch.pt\")\n",
    "\n",
    "        load_path = None\n",
    "        if os.path.exists(best_model_path):\n",
    "            load_path = best_model_path\n",
    "            print(f\"\\nAttempting to load best model weights from: {load_path}\")\n",
    "        elif os.path.exists(final_model_path):\n",
    "            load_path = final_model_path\n",
    "            print(f\"\\nBest model not found. Attempting to load final epoch weights from: {load_path}\")\n",
    "        else:\n",
    "            print(f\"\\nWARNING: No saved model checkpoints ('best' or 'final') found in {config.model_path}.\")\n",
    "\n",
    "        if load_path:\n",
    "            checkpoint = torch.load(load_path, map_location=config.device)\n",
    "\n",
    "            # Re-create model using saved config, then load state_dict\n",
    "            print(\"Re-creating model structure for testing...\")\n",
    "            if 'blip_config_dict' in checkpoint:\n",
    "                 saved_blip_config = Blip2Config.from_dict(checkpoint['blip_config_dict'])\n",
    "                 temp_config = config # Start with current config\n",
    "                 temp_config.blip_model_name = saved_blip_config._name_or_path\n",
    "                 print(f\"  Using base model config from checkpoint: {temp_config.blip_model_name}\")\n",
    "                 model_to_test = Blip2RetrievalModel(temp_config)\n",
    "            else:\n",
    "                 print(\"Warning: Blip config not found in checkpoint, using current CFG.\")\n",
    "                 model_to_test = Blip2RetrievalModel(config)\n",
    "\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            # Handle 'module.' prefix\n",
    "            if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "                print(\"Detected 'module.' prefix, removing for loading.\")\n",
    "                from collections import OrderedDict\n",
    "                new_state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "                state_dict = new_state_dict\n",
    "\n",
    "            load_result = model_to_test.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"  State dict loading result: {load_result}\")\n",
    "            if load_result.missing_keys:\n",
    "                 print(f\"  Warning: Missing keys: {load_result.missing_keys}\")\n",
    "            if load_result.unexpected_keys:\n",
    "                 print(f\"  Warning: Unexpected keys: {load_result.unexpected_keys}\")\n",
    "\n",
    "            # Ensure model is on device if not using device_map\n",
    "            if 'device_map' not in getattr(model_to_test.blip_model, 'hf_device_map', {}):\n",
    "                 model_to_test.to(config.device)\n",
    "\n",
    "            print(f\"Model weights loaded successfully from {load_path}\")\n",
    "\n",
    "            # --- Run Evaluation ---\n",
    "            print(\"\\nRunning evaluation on test set...\")\n",
    "            test_results = validate_epoch(model_to_test, test_loader, config.device, epoch_num=\"Test\")\n",
    "\n",
    "            print(\"\\n--- Test Set Results ---\")\n",
    "            metric_log_str = \"\"\n",
    "            sorted_keys = sorted(test_results.keys(), key=lambda x: (x.split()[0], int(x.split('@')[-1]) if '@' in x else -1))\n",
    "            for name in sorted_keys:\n",
    "                 value = test_results[name]\n",
    "                 metric_log_str += f\"  {name}: {value:.4f}\\n\"\n",
    "            print(metric_log_str.strip())\n",
    "            print(\"------------------------\")\n",
    "\n",
    "        else:\n",
    "             print(\"Evaluation skipped as no model weights were found to load.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR during test setup or evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n================= Evaluation Finished =================\")\n",
    "\n",
    "\n",
    "# === Cell 13: Training Visualization (Adapted) ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plot_dir = \"train_plot\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "print(f\"Plot directory ensured at: {os.path.abspath(plot_dir)}\")\n",
    "\n",
    "def save_subplot_as_figure(subplot, save_path):\n",
    "    fig_new = plt.figure(figsize=(8, 6))\n",
    "    ax_new = fig_new.add_subplot(111)\n",
    "    lines = subplot.get_lines()\n",
    "    if not lines:\n",
    "        print(f\"Warning: No lines found in subplot for {save_path}\")\n",
    "        plt.close(fig_new)\n",
    "        return\n",
    "    labels = [line.get_label() for line in lines]\n",
    "    for line in lines:\n",
    "        ax_new.plot(line.get_xdata(), line.get_ydata(),\n",
    "                    color=line.get_color(),\n",
    "                    linestyle=line.get_linestyle(),\n",
    "                    marker=line.get_marker(),\n",
    "                    label=line.get_label())\n",
    "    ax_new.set_title(subplot.get_title())\n",
    "    ax_new.set_xlabel(subplot.get_xlabel())\n",
    "    ax_new.set_ylabel(subplot.get_ylabel())\n",
    "    ax_new.grid(True)\n",
    "    if any(label and not label.startswith('_') for label in labels):\n",
    "         ax_new.legend()\n",
    "    plt.tight_layout()\n",
    "    fig_new.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig_new)\n",
    "\n",
    "def plot_training_metrics(history):\n",
    "    if not history or not history.get('train_loss') or not history.get('validation_results'):\n",
    "        print(\"No training history available or history is incomplete.\")\n",
    "        return\n",
    "\n",
    "    valid_results = [res for res in history['validation_results'] if res is not None]\n",
    "    if not valid_results:\n",
    "        print(\"No valid validation results found. Plotting only training loss.\")\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "        ax.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')\n",
    "        ax.set_title('Training Loss over Epochs')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        save_path = os.path.join(plot_dir, f'training_loss.png')\n",
    "        fig.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Saved loss plot to: {save_path}\")\n",
    "        # plt.show() # Don't show in script mode\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    num_epochs_trained = len(history['train_loss'])\n",
    "    num_epochs_validated = len(valid_results)\n",
    "    epochs_train = range(1, num_epochs_trained + 1)\n",
    "    epochs_val = range(1, num_epochs_validated + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 13))\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16, y=1.02)\n",
    "\n",
    "    # --- Plot Loss ---\n",
    "    val_loss = [res.get('loss', float('nan')) for res in valid_results] # Use .get for safety\n",
    "    axes[0, 0].plot(epochs_train, history['train_loss'], 'b-o', label='Training Loss')\n",
    "    axes[0, 0].plot(epochs_val, val_loss, 'r-s', label='Validation Loss')\n",
    "    axes[0, 0].set_title('Loss over Epochs')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # --- Plot Accuracy (Average Accuracy) ---\n",
    "    metric_key_acc = 'avg acc' # Key from validate_epoch output\n",
    "    if metric_key_acc in valid_results[0]:\n",
    "        val_acc = [res[metric_key_acc] for res in valid_results]\n",
    "        axes[0, 1].plot(epochs_val, val_acc, 'g-^', label='Average Accuracy (Val)')\n",
    "        axes[0, 1].set_title('Validation Average Accuracy over Epochs')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "    else:\n",
    "        axes[0, 1].set_title(f'Validation Acc ({metric_key_acc}) (Not Found)')\n",
    "\n",
    "    # --- Plot Recall Metrics ---\n",
    "    has_recall = 'i2t recall R@1' in valid_results[0] # Check a representative key\n",
    "\n",
    "    if has_recall:\n",
    "        # I2T Recall\n",
    "        for k in [1, 5, 10]:\n",
    "            key = f'i2t recall R@{k}'\n",
    "            values = [res.get(key, float('nan')) for res in valid_results]\n",
    "            axes[1, 0].plot(epochs_val, values, marker='o', label=f'I2T R@{k}')\n",
    "        axes[1, 0].set_title('Image-to-Text Recall (Val)')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Recall')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "\n",
    "        # T2I Recall\n",
    "        for k in [1, 5, 10]:\n",
    "            key = f't2i recall R@{k}'\n",
    "            values = [res.get(key, float('nan')) for res in valid_results]\n",
    "            axes[1, 1].plot(epochs_val, values, marker='s', label=f'T2I R@{k}')\n",
    "        axes[1, 1].set_title('Text-to-Image Recall (Val)')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Recall')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "    else:\n",
    "        axes[1, 0].set_title('I2T Recall (Not Found)')\n",
    "        axes[1, 1].set_title('T2I Recall (Not Found)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    # Save individual plots\n",
    "    plot_names = ['loss', 'accuracy', 'i2t_recall', 't2i_recall']\n",
    "    for idx, name in enumerate(plot_names):\n",
    "        i, j = divmod(idx, 2)\n",
    "        save_path = os.path.join(plot_dir, f'training_{name}.png')\n",
    "        if axes[i, j].has_data():\n",
    "            save_subplot_as_figure(axes[i, j], save_path)\n",
    "            print(f\"Saved {name} plot to: {save_path}\")\n",
    "        else:\n",
    "            print(f\"Skipping save for {name} plot (no data).\")\n",
    "\n",
    "    # Save combined plot\n",
    "    combined_save_path = os.path.join(plot_dir, 'training_metrics_combined.png')\n",
    "    fig.savefig(combined_save_path, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved combined plot to: {combined_save_path}\")\n",
    "\n",
    "    # plt.show() # Avoid showing plots in a script run\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- Plotting ---\n",
    "# Check if the 'history' variable exists from the training loop before plotting\n",
    "if 'history' in locals() and isinstance(history, dict) and history.get('train_loss'):\n",
    "    plot_training_metrics(history)\n",
    "else:\n",
    "    print(\"No training history found or history is empty. Run training first to generate history.\")\n",
    "\n",
    "\n",
    "# --- END OF SCRIPT ---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
