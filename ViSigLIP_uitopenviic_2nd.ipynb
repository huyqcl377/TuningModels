{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Imports ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# --- Import SigLIP Vision model and its Processor ---\n",
    "from transformers import SiglipVisionModel, SiglipConfig, SiglipImageProcessor\n",
    "# --- Keep PhoBERT parts ---\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import transformers\n",
    "import gc\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device_idx = 1 # Or 0\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(device_idx)}\")\n",
    "    torch.cuda.set_device(device_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23828e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Configuration Class (CFG) - Configured for SigLIP Loss ===\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    data_path = \"./json_data/\"\n",
    "    image_base_path = \"./data/UIT-OpenViIC-dataset/\"\n",
    "    model_path = \"./trained_models/ViSiglip_uitopenviic_SigmoidLoss\"\n",
    "\n",
    "    # --- Model Selection ---\n",
    "    selected_vision_source = \"google/siglip-base-patch16-224\"\n",
    "    selected_text_model = \"vinai/phobert-base\"\n",
    "    text_tokenizer_name = selected_text_model\n",
    "\n",
    "    # --- Model parameters ---\n",
    "    vision_model_name = selected_vision_source\n",
    "    text_model_name = selected_text_model\n",
    "    image_processor_name = selected_vision_source\n",
    "\n",
    "    @property\n",
    "    def text_embedding(self): return 768\n",
    "    @property\n",
    "    def vision_embedding(self): return 768\n",
    "\n",
    "    projection_dim = 768 # Match SigLIP paper's projection dim\n",
    "\n",
    "    # --- SigLIP Loss specific Parameters ---\n",
    "    learnable_temperature = True\n",
    "    temperature_init = 10.0 # SigLIP often initializes temperature higher\n",
    "    learnable_bias = True\n",
    "    bias_init = -10.0 # SigLIP initializes bias negative\n",
    "\n",
    "    # --- Training parameters ---\n",
    "    seed = 42\n",
    "    # SigLIP benefits from large batches, use accumulation\n",
    "    batch_size = 32   # Keep reduced per-device batch size for 24GB VRAM\n",
    "    accumulation_steps = 64 # Increase accumulation (Effective 2048)\n",
    "    num_workers = 20\n",
    "\n",
    "    # --- Learning Rates (May need adjustment for SigLIP loss) ---\n",
    "    # Might use a single LR for simplicity when using SigLIP loss\n",
    "    learning_rate = 1e-4 # Base LR (AdamW default)\n",
    "    projection_lr = 1e-4 # LR for projection, temp, bias\n",
    "    vision_encoder_lr = 1e-5 # Lower LR for backbones\n",
    "    text_encoder_lr = 2e-5\n",
    "    weight_decay = 0.1 # Higher WD often used with SigLIP pretraining\n",
    "\n",
    "    # --- Scheduler ---\n",
    "    scheduler_type = \"cosine\" # Cosine often used for longer training\n",
    "    warmup_steps = 1000 # Number of warmup steps for Cosine scheduler\n",
    "    rop_patience = 3  # ReduceLROnPlateau parameters (if used)\n",
    "    rop_factor = 0.8\n",
    "\n",
    "    epochs = 50 # Increase epochs for more training steps\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_amp = True\n",
    "\n",
    "    # --- Image/Text parameters ---\n",
    "    max_length = 77 # Standard CLIP length\n",
    "\n",
    "    # --- Saving parameters ---\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"avg_acc\" # Still track retrieval accuracy for validation\n",
    "    mode = \"max\"\n",
    "    save_interval_steps = 2000 # Save checkpoints periodically\n",
    "    validation_interval_steps = 1000 # Validate periodically\n",
    "    log_interval_steps = 50\n",
    "\n",
    "    early_stopping_patience = 10 # Allow more patience if convergence is slower\n",
    "    early_stopping_min_delta = 0.001\n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Per-Device Batch Size: {config.batch_size}\")\n",
    "print(f\"Accumulation Steps: {config.accumulation_steps}\")\n",
    "print(f\"Effective Batch Size (per optimizer step): {config.batch_size * config.accumulation_steps}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Selected Vision Source: {config.selected_vision_source}\")\n",
    "print(f\"Selected Text Model: {config.selected_text_model}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_base_path)}\")\n",
    "print(f\"AMP Enabled: {config.use_amp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ba369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Seeding ===\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368965f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Metric & AvgMeter Utilities ===\n",
    "# (Keep AvgMeter, compute_recall_at_k, compute_metrics as before)\n",
    "# These metrics are used for validation, even if training loss is different\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"): self.name = name; self.reset()\n",
    "    def reset(self): self.sum = 0; self.count = 0; self.avg = 0.0\n",
    "    def update(self, val, count=1):\n",
    "        if torch.is_tensor(val): val = val.item()\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count; self.count += count\n",
    "            self.avg = float(self.sum) / self.count if self.count != 0 else 0.0\n",
    "    def __repr__(self): return f\"{self.name}: {self.avg:.4f}\"\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    n = similarity_matrix.shape[1-dim]; correct_count = 0\n",
    "    if n == 0: return 0.0\n",
    "    actual_k = min(k, similarity_matrix.shape[dim])\n",
    "    if actual_k == 0: return 0.0\n",
    "    top_k_indices = torch.topk(similarity_matrix, actual_k, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "    if dim == 0: # I2T\n",
    "        for i in range(n): correct_count += ground_truth[i] in top_k_indices[:, i]\n",
    "    elif dim == 1: # T2I\n",
    "        for i in range(n): correct_count += ground_truth[i] in top_k_indices[i, :]\n",
    "    else: raise ValueError(\"dim must be 0 or 1\")\n",
    "    return float(correct_count) / n\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    sim_matrix = text_embeddings.float() @ image_embeddings.float().T; n = sim_matrix.shape[0]\n",
    "    if n == 0: return {\"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0, \"avg_cosine_sim\": 0.0, \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}, \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}}\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device); i2t_preds = torch.argmax(sim_matrix, dim=0); t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item(); t2i_acc = (t2i_preds == ground_truth).float().mean().item(); avg_acc = (i2t_acc + t2i_acc) / 2.0\n",
    "    avg_cosine_sim = torch.diag(sim_matrix).mean().item(); i2t_recall = {}; t2i_recall = {}\n",
    "    recall_k_values = [k for k in [1, 5, 10] if k <= n]\n",
    "    for k in recall_k_values: i2t_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=0); t2i_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "    for k in [1, 5, 10]: k_str = f\"R@{k}\"; i2t_recall.setdefault(k_str, 0.0); t2i_recall.setdefault(k_str, 0.0)\n",
    "    return {\"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc, \"avg_cosine_sim\": avg_cosine_sim, \"i2t_recall\": i2t_recall, \"t2i_recall\": t2i_recall}\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Dataset Class Definition ===\n",
    "# (No changes needed from the previous corrected version)\n",
    "\n",
    "import traceback\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class CustomImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path_or_list, image_base_path, tokenizer, image_processor, max_length):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        if isinstance(json_path_or_list, str) and os.path.isdir(json_path_or_list):\n",
    "             json_files = [os.path.join(json_path_or_list, f) for f in os.listdir(json_path_or_list) if f.endswith('.json')]\n",
    "             print(f\"Found {len(json_files)} JSON files in {json_path_or_list}\")\n",
    "        elif isinstance(json_path_or_list, str) and os.path.isfile(json_path_or_list):\n",
    "            json_files = [json_path_or_list]\n",
    "        elif isinstance(json_path_or_list, list):\n",
    "            json_files = json_path_or_list\n",
    "        else:\n",
    "            raise ValueError(\"json_path_or_list must be a directory, a single JSON file, or a list of JSON files.\")\n",
    "\n",
    "        print(\"Loading JSON metadata...\")\n",
    "        total_loaded_count = 0\n",
    "        for json_path in tqdm(json_files, desc=\"Loading JSONs\"):\n",
    "            try:\n",
    "                with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        file_data = json.load(f)\n",
    "                        if isinstance(file_data, list):\n",
    "                             self.data.extend(file_data)\n",
    "                             total_loaded_count += len(file_data)\n",
    "                        else:\n",
    "                             self.data.append(file_data)\n",
    "                             total_loaded_count += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"  Info: Failed to load {json_path} as single JSON. Attempting JSON-per-line format...\")\n",
    "                        f.seek(0)\n",
    "                        count_line_by_line = 0\n",
    "                        for line in f:\n",
    "                            line = line.strip()\n",
    "                            if line:\n",
    "                                try:\n",
    "                                    line_data = json.loads(line)\n",
    "                                    self.data.append(line_data)\n",
    "                                    count_line_by_line += 1\n",
    "                                except json.JSONDecodeError as line_err:\n",
    "                                     print(f\"  ERROR parsing line in {json_path}: {line_err}. Line content (partial): {line[:100]}...\")\n",
    "                        total_loaded_count += count_line_by_line\n",
    "                        if count_line_by_line == 0: print(f\"  Failed to load any data using JSON-per-line format from {json_path} either.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR opening or processing file {json_path}: {e}\")\n",
    "\n",
    "        print(f\"Loaded {total_loaded_count} samples total from {len(json_files)} file(s).\")\n",
    "        self.data = [item for item in self.data if item]\n",
    "        print(f\"Dataset size after potential cleaning: {len(self.data)}\")\n",
    "\n",
    "        if not self.data: print(\"WARNING: No data loaded!\")\n",
    "\n",
    "        self.image_base_path = image_base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "        try:\n",
    "            self.img_size = image_processor.config.image_size\n",
    "        except AttributeError:\n",
    "            try: self.img_size = image_processor.size['height']\n",
    "            except (AttributeError, TypeError, KeyError): self.img_size = 224; print(\"Warning: Defaulting image size to 224.\")\n",
    "        print(f\"Using image target size: {self.img_size}x{self.img_size}\")\n",
    "        if not os.path.isdir(self.image_base_path): print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data): raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path', item.get('url', item.get('filename')))\n",
    "        caption_data = item.get('caption', item.get('text', item.get('title', '')))\n",
    "        caption = caption_data[0] if isinstance(caption_data, list) and caption_data else caption_data if isinstance(caption_data, str) else \"\"\n",
    "        if not relative_image_path or not caption: return self._get_dummy_item()\n",
    "        try:\n",
    "            image_path = os.path.join(self.image_base_path, relative_image_path)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_inputs = self.image_processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = image_inputs['pixel_values'].squeeze(0)\n",
    "        except Exception: return self._get_dummy_item()\n",
    "        try:\n",
    "            text_inputs = self.tokenizer(caption, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "            input_ids = text_inputs['input_ids'].squeeze(0)\n",
    "            attention_mask = text_inputs['attention_mask'].squeeze(0)\n",
    "        except Exception: return self._get_dummy_item()\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "    def _get_dummy_item(self): return {\"pixel_values\": torch.zeros((3, self.img_size, self.img_size), dtype=torch.float), \"input_ids\": torch.zeros(self.max_length, dtype=torch.long), \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long)}\n",
    "\n",
    "print(\"CustomImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Model Definition (SigLIP Vision + PhoBERT Text + SigLIP Params) ===\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Encodes images using SigLIP's Vision Model.\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        print(f\"Initializing SigLIP Vision Encoder from: {config_train.vision_model_name}\")\n",
    "        if pretrained:\n",
    "            self.vision_model = SiglipVisionModel.from_pretrained(config_train.vision_model_name)\n",
    "        else:\n",
    "            siglip_vision_config = SiglipConfig.from_pretrained(config_train.vision_model_name).vision_config\n",
    "            self.vision_model = SiglipVisionModel(siglip_vision_config)\n",
    "        self.input_features = self.vision_model.config.hidden_size\n",
    "        print(f\"  Confirmed/Using vision model hidden size: {self.input_features}\")\n",
    "        self.projection = nn.Linear(self.input_features, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        vision_outputs = self.vision_model(pixel_values=pixel_values, return_dict=True)\n",
    "        image_embed = vision_outputs.pooler_output\n",
    "        projected_features = self.projection(image_embed)\n",
    "        # Normalization in main model\n",
    "        return projected_features\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Encodes text using PhoBERT-Base.\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        print(f\"Initializing Text Encoder: {config_train.text_model_name}\")\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config_train.text_model_name)\n",
    "        else:\n",
    "            model_config = AutoConfig.from_pretrained(config_train.text_model_name)\n",
    "            self.model = AutoModel.from_config(model_config)\n",
    "        self.input_features = self.model.config.hidden_size\n",
    "        print(f\"  Confirmed text model hidden size: {self.input_features}\")\n",
    "        self.projection = nn.Linear(self.input_features, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        text_features = outputs.last_hidden_state[:, 0, :]\n",
    "        projected_features = self.projection(text_features)\n",
    "        # Normalization in main model\n",
    "        return projected_features\n",
    "\n",
    "# --- Combined Model (Using SigLIP temperature & bias) ---\n",
    "class ViPhobertSiglipLossModel(nn.Module): # New name\n",
    "    \"\"\"Combines SigLIP Vision, PhoBERT Text, and SigLIP temp/bias.\"\"\"\n",
    "    def __init__(self, image_encoder, text_encoder, config_train):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.config_train = config_train\n",
    "\n",
    "        if config_train.learnable_temperature:\n",
    "            init_val_t = torch.tensor(config_train.temperature_init, dtype=torch.float)\n",
    "            self.logit_scale = nn.Parameter(init_val_t)\n",
    "            print(f\"Using learnable temperature, initialized to {self.logit_scale.item():.4f}\")\n",
    "        else:\n",
    "            temp_tensor = torch.tensor(config_train.temperature_init, dtype=torch.float)\n",
    "            self.register_buffer('logit_scale', temp_tensor)\n",
    "            print(f\"Using fixed temperature: {self.logit_scale.item():.4f}\")\n",
    "\n",
    "        if config_train.learnable_bias:\n",
    "            init_val_b = torch.tensor(config_train.bias_init, dtype=torch.float)\n",
    "            self.logit_bias = nn.Parameter(init_val_b)\n",
    "            print(f\"Using learnable bias, initialized to {self.logit_bias.item():.4f}\")\n",
    "        else:\n",
    "            bias_tensor = torch.tensor(config_train.bias_init, dtype=torch.float)\n",
    "            self.register_buffer('logit_bias', bias_tensor)\n",
    "            print(f\"Using fixed bias: {self.logit_bias.item():.4f}\")\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        pixel_values = pixel_values.to(self.config_train.device)\n",
    "        input_ids = input_ids.to(self.config_train.device)\n",
    "        attention_mask = attention_mask.to(self.config_train.device)\n",
    "\n",
    "        image_embed = self.image_encoder(pixel_values)\n",
    "        text_embed = self.text_encoder(input_ids, attention_mask)\n",
    "\n",
    "        image_features = F.normalize(image_embed, p=2, dim=-1)\n",
    "        text_features = F.normalize(text_embed, p=2, dim=-1)\n",
    "\n",
    "        # Temperature and bias are returned for the SigLIP loss function\n",
    "        current_temp = self.logit_scale.to(image_features.device)\n",
    "        current_bias = self.logit_bias.to(image_features.device)\n",
    "\n",
    "        return image_features, text_features, current_temp, current_bias\n",
    "\n",
    "print(\"ViPhobertSiglipLoss Model components defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: SigLIP Loss Function ===\n",
    "def siglip_loss(image_features, text_features, logit_scale, logit_bias):\n",
    "    \"\"\" Computes the SigLIP loss. \"\"\"\n",
    "    image_features = image_features.float()\n",
    "    text_features = text_features.float()\n",
    "    logit_scale = logit_scale.float()\n",
    "    logit_bias = logit_bias.float()\n",
    "\n",
    "    n = text_features.shape[0]\n",
    "    if n == 0:\n",
    "        return torch.tensor(0.0, device=image_features.device, requires_grad=True)\n",
    "\n",
    "    # Calculate similarity with scaling and bias\n",
    "    logits = image_features @ text_features.t() * logit_scale + logit_bias\n",
    "\n",
    "    # Labels: 1 for positive pairs (diagonal), 0 for negative pairs\n",
    "    labels = torch.eye(n, device=logits.device, dtype=logits.dtype)\n",
    "\n",
    "    # Pairwise sigmoid loss using BCEWithLogitsLoss\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='mean')\n",
    "\n",
    "    return loss\n",
    "\n",
    "print(\"SigLIP loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Setup - Tokenizer and Image Processor (Using SigLIP Processor) ===\n",
    "from transformers import AutoTokenizer, SiglipImageProcessor\n",
    "\n",
    "tokenizer = None\n",
    "image_processor = None\n",
    "\n",
    "print(f\"Loading Tokenizer: {config.text_tokenizer_name}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer_name)\n",
    "    print(\"PhoBERT Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading tokenizer '{config.text_tokenizer_name}': {e}\")\n",
    "\n",
    "print(f\"Loading Image Processor from: {config.image_processor_name}\")\n",
    "try:\n",
    "    image_processor = SiglipImageProcessor.from_pretrained(config.image_processor_name)\n",
    "    print(\"SigLIP Image Processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading image processor '{config.image_processor_name}': {e}\")\n",
    "\n",
    "# Image transforms are implicitly handled by the processor during dataset __getitem__\n",
    "# No separate transforms object needed if using the processor directly\n",
    "print(\"Image transforms handled by SiglipImageProcessor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9: Setup - Datasets and DataLoaders (FIXED Validation Path) ===\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "\n",
    "validation_json_path = os.path.join(config.data_path, \"dev.json\")\n",
    "train_json_path = os.path.join(config.data_path, \"train.json\")\n",
    "\n",
    "if tokenizer and image_processor:\n",
    "    print(\"\\\\nCreating datasets...\")\n",
    "    # --- Training Dataset ---\n",
    "    try:\n",
    "        print(f\"Attempting to load training data from: {train_json_path}\")\n",
    "        train_dataset = CustomImageCaptionDataset(\n",
    "            json_path_or_list=train_json_path,\n",
    "            image_base_path=config.image_base_path,\n",
    "            tokenizer=tokenizer,\n",
    "            image_processor=image_processor, # Pass the loaded processor\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "        if not train_dataset.data: print(\"\\\\nERROR: Failed to load training data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating training dataset: {e}\")\n",
    "        train_dataset = None\n",
    "\n",
    "    # --- Validation Dataset ---\n",
    "    if os.path.exists(validation_json_path):\n",
    "         try:\n",
    "             print(f\"Attempting to load validation data from: {validation_json_path}\")\n",
    "             dev_dataset = CustomImageCaptionDataset(\n",
    "                 json_path_or_list=validation_json_path,\n",
    "                 image_base_path=config.image_base_path,\n",
    "                 tokenizer=tokenizer,\n",
    "                 image_processor=image_processor, # Use same processor\n",
    "                 max_length=config.max_length\n",
    "             )\n",
    "             if not dev_dataset.data: print(\"\\\\nWARNING: Failed to load validation data.\")\n",
    "         except Exception as e:\n",
    "             print(f\"ERROR creating validation dataset: {e}\")\n",
    "             dev_dataset = None\n",
    "    else:\n",
    "        print(f\"Validation JSON file not found at {validation_json_path}, skipping validation set creation.\")\n",
    "        dev_dataset = None\n",
    "\n",
    "    print(\"\\\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    if train_dataset and train_dataset.data:\n",
    "        persist_workers = (num_workers > 0)\n",
    "        try: _ = DataLoader(train_dataset, num_workers=num_workers, persistent_workers=persist_workers)\n",
    "        except TypeError: persist_workers = False;# print(\"Note: `persistent_workers=True` not supported.\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=True,\n",
    "            persistent_workers=persist_workers\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "        if config.scheduler_type == \"cosine\":\n",
    "            config.total_training_steps = len(train_loader) * config.epochs // config.accumulation_steps\n",
    "            print(f\"Total estimated training steps for Cosine Scheduler: {config.total_training_steps}\")\n",
    "    else:\n",
    "        print(\"Skipping train loader creation (no data).\")\n",
    "        config.total_training_steps = 0\n",
    "\n",
    "    if dev_dataset and dev_dataset.data:\n",
    "        persist_workers_dev = (num_workers > 0)\n",
    "        try: _ = DataLoader(dev_dataset, num_workers=num_workers, persistent_workers=persist_workers_dev)\n",
    "        except TypeError: persist_workers_dev = False\n",
    "\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset, batch_size=config.batch_size * 2, shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False,\n",
    "            persistent_workers=persist_workers_dev\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "    else: print(\"Skipping validation loader creation.\")\n",
    "\n",
    "    if not train_loader: print(\"\\\\nERROR: Train loader could not be created.\")\n",
    "else:\n",
    "     print(\"ERROR: Tokenizer or Image Processor not loaded. Skipping dataset/loader creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10: Setup - Model, Optimizer, Scheduler (SigLIP Params) ===\n",
    "\n",
    "import traceback\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "scaler = None # For AMP\n",
    "\n",
    "print(\"\\\\nInitializing ViPhobertSiglipLoss model components...\")\n",
    "try:\n",
    "    image_encoder = ImageEncoder(config).to(config.device)\n",
    "    text_encoder = TextEncoder(config).to(config.device)\n",
    "    # --- Instantiate the CORRECT model class ---\n",
    "    model = ViPhobertSiglipLossModel(image_encoder, text_encoder, config).to(config.device)\n",
    "\n",
    "    print(f\"\\\\nViPhobertSiglipLoss Model initialized successfully on {config.device}.\")\n",
    "    num_params_total = sum(p.numel() for p in model.parameters())\n",
    "    num_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {num_params_total / 1e6:.2f} M\")\n",
    "    print(f\"Trainable parameters: {num_params_trainable / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing model components: {e}\")\n",
    "    traceback.print_exc()\n",
    "    model = None\n",
    "\n",
    "if model and train_loader:\n",
    "    print(\"\\\\nSetting up optimizer...\")\n",
    "    # --- Optimizer Grouping (Include logit_scale and logit_bias if learnable) ---\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\", \"logit_scale\", \"logit_bias\"] # Exclude temp/bias from WD\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "         'lr': config.learning_rate, 'weight_decay': config.weight_decay}, # Base LR for most params\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "         'lr': config.learning_rate, 'weight_decay': 0.0}, # No WD for bias/norm/temp/bias\n",
    "        # Optionally assign different LRs if needed (e.g., higher for projection/temp/bias)\n",
    "        # {'params': model.image_encoder.projection.parameters(), 'lr': config.projection_lr, 'weight_decay': 0.0},\n",
    "        # {'params': model.text_encoder.projection.parameters(), 'lr': config.projection_lr, 'weight_decay': 0.0},\n",
    "        # {'params': [p for n, p in param_optimizer if \"logit_scale\" in n and p.requires_grad], 'lr': config.projection_lr, 'weight_decay': 0.0},\n",
    "        # {'params': [p for n, p in param_optimizer if \"logit_bias\" in n and p.requires_grad], 'lr': config.projection_lr, 'weight_decay': 0.0},\n",
    "    ]\n",
    "    # Filter empty groups\n",
    "    optimizer_grouped_parameters = [g for g in optimizer_grouped_parameters if g['params']]\n",
    "\n",
    "    if not optimizer_grouped_parameters:\n",
    "        print(\"ERROR: No trainable parameters found.\")\n",
    "        optimizer = None\n",
    "    else:\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters, lr=config.learning_rate) # Base LR is default\n",
    "        print(f\"Optimizer AdamW initialized with base LR: {config.learning_rate}, weight decay: {config.weight_decay}\")\n",
    "\n",
    "        # --- LR Scheduler ---\n",
    "        if config.scheduler_type == \"cosine\":\n",
    "            if hasattr(config, 'total_training_steps') and config.total_training_steps > 0:\n",
    "                 lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "                     optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=config.total_training_steps\n",
    "                 )\n",
    "                 print(f\"LR Scheduler: Cosine with Warmup ({config.warmup_steps} steps) initialized.\")\n",
    "            else: lr_scheduler = None; print(\"ERROR: Cannot init Cosine scheduler.\")\n",
    "        elif config.scheduler_type == \"reduce_on_plateau\":\n",
    "            lr_scheduler = ReduceLROnPlateau(optimizer, mode=config.mode, factor=config.rop_factor, patience=config.rop_patience)\n",
    "            print(f\"LR Scheduler: ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.rop_factor}, patience={config.rop_patience})\")\n",
    "        else: lr_scheduler = None; print(\"No LR Scheduler specified.\")\n",
    "\n",
    "        # --- AMP Scaler ---\n",
    "        if config.use_amp:\n",
    "            scaler = torch.amp.GradScaler('cuda')\n",
    "            print(\"AMP GradScaler initialized.\")\n",
    "        else: scaler = None\n",
    "\n",
    "        # Early stopping setup\n",
    "        early_stopping_counter = 0\n",
    "        best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "        print(f\"Early stopping enabled with patience: {config.early_stopping_patience}\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model or train_loader not available. Skipping optimizer/scheduler setup.\")\n",
    "    optimizer = None; lr_scheduler = None; scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14503a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 11: Training and Validation Functions (Using SigLIP Loss) ===\n",
    "import traceback\n",
    "\n",
    "def train_step(model, batch, optimizer, scaler, device, use_amp):\n",
    "    \"\"\" Performs a single training step with SigLIP loss and optional AMP \"\"\"\n",
    "    model.train()\n",
    "    pixel_values = batch['pixel_values']\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "\n",
    "    with torch.amp.autocast(device_type=device.type, enabled=use_amp):\n",
    "        # Model returns normalized features, temp, bias\n",
    "        image_features, text_features, temp, bias = model(pixel_values, input_ids, attention_mask)\n",
    "        loss = siglip_loss(image_features, text_features, temp, bias) # <<< USE SigLIP LOSS\n",
    "\n",
    "    if use_amp:\n",
    "        scaler.scale(loss).backward()\n",
    "    else:\n",
    "        loss.backward()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def validate_epoch(model, dataloader, device):\n",
    "    \"\"\" Performs validation, returning standard retrieval metrics \"\"\"\n",
    "    model.eval()\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    progress_bar = tqdm(dataloader, desc=\"Validation\", leave=False, unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values']\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=config.use_amp):\n",
    "                # Get normalized features, ignore temp/bias for standard metrics\n",
    "                image_embeds_norm, text_embeds_norm, _, _ = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "            all_image_embeddings.append(image_embeds_norm.cpu())\n",
    "            all_text_embeddings.append(text_embeds_norm.cpu())\n",
    "\n",
    "    if not all_image_embeddings or not all_text_embeddings:\n",
    "         print(\"Warning: No embeddings collected during validation.\")\n",
    "         return {\"loss\": float('inf'), \"avg_acc\": 0.0, \"avg_cosine_sim\": 0.0, \"i2t recall R@1\": 0.0, \"i2t recall R@5\": 0.0, \"i2t recall R@10\": 0.0, \"t2i recall R@1\": 0.0, \"t2i recall R@5\": 0.0, \"t2i recall R@10\": 0.0 }\n",
    "\n",
    "    try:\n",
    "        all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "        all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error concatenating embeddings: {e}\")\n",
    "        return {\"loss\": float('inf'), \"avg_acc\": 0.0, \"avg_cosine_sim\": 0.0, \"i2t recall R@1\": 0.0, \"i2t recall R@5\": 0.0, \"i2t recall R@10\": 0.0, \"t2i recall R@1\": 0.0, \"t2i recall R@5\": 0.0, \"t2i recall R@10\": 0.0 }\n",
    "\n",
    "    # Log Temperature and Bias values\n",
    "    if hasattr(model, 'logit_scale'):\n",
    "        current_temp_val = model.logit_scale.item() if isinstance(model.logit_scale, nn.Parameter) else model.logit_scale.item()\n",
    "        print(f\"DEBUG: Validation - Current Temp: {current_temp_val:.4f}\")\n",
    "    if hasattr(model, 'logit_bias'):\n",
    "        current_bias_val = model.logit_bias.item() if isinstance(model.logit_bias, nn.Parameter) else model.logit_bias.item()\n",
    "        print(f\"DEBUG: Validation - Current Bias: {current_bias_val:.4f}\")\n",
    "\n",
    "    print(f\"\\\\nComputing metrics over {all_image_embeddings.shape[0]} validation samples...\")\n",
    "    validation_metrics = compute_metrics(all_image_embeddings.to(device), all_text_embeddings.to(device))\n",
    "\n",
    "    # Format results (no validation loss calculated/averaged here)\n",
    "    final_results = {}\n",
    "    for k, v in validation_metrics.items():\n",
    "        if isinstance(v, dict):\n",
    "            for recall_k, recall_v in v.items(): final_results[f\"{k.replace('_', ' ')} {recall_k}\"] = recall_v\n",
    "        else: final_results[k.replace('_', ' ')] = v\n",
    "\n",
    "    del all_image_embeddings, all_text_embeddings\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "    return final_results\n",
    "\n",
    "print(\"Training step (SigLIP loss) and validation epoch functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e8142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 12: Training Loop (with Early Stopping, SigLIP Loss) ===\n",
    "import datetime\n",
    "\n",
    "if model and train_loader and optimizer:\n",
    "    print(f\"\\\\nStarting ViPhobertSiglip fine-tuning (SigLIP Loss) for {config.epochs} epochs...\")\n",
    "    print(f\"Target metric for saving best model: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "\n",
    "    # Use variables initialized in Cell 10\n",
    "    # best_val_metric, early_stopping_counter\n",
    "\n",
    "    global_step = 0\n",
    "    total_loss_since_log = 0.0\n",
    "    steps_since_log = 0\n",
    "    start_train_time = time.time()\n",
    "\n",
    "    history = {'steps': [], 'train_loss': [], 'val_metrics': {}}\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Training E{epoch+1}\", leave=True, unit=\"batch\")\n",
    "        epoch_loss_meter = AvgMeter(f\"Train Loss E{epoch+1}\")\n",
    "\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            if 'pixel_values' not in batch or batch['pixel_values'].shape[0] < config.batch_size and torch.all(batch['pixel_values'] == 0):\n",
    "                continue\n",
    "\n",
    "            # --- Training Step using siglip_loss ---\n",
    "            loss = train_step(model, batch, optimizer, scaler, config.device, config.use_amp)\n",
    "            epoch_loss_meter.update(loss * config.accumulation_steps, batch['pixel_values'].shape[0])\n",
    "\n",
    "            loss_normalized = loss / config.accumulation_steps\n",
    "            total_loss_since_log += loss_normalized\n",
    "            steps_since_log += 1\n",
    "\n",
    "            # --- Gradient Accumulation & Optimizer Step ---\n",
    "            is_update_step = (i + 1) % config.accumulation_steps == 0 or (i + 1) == len(train_loader)\n",
    "            if is_update_step:\n",
    "                if config.use_amp:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                if lr_scheduler and config.scheduler_type == \"cosine\":\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # --- Logging ---\n",
    "            if global_step % config.log_interval_steps == 0 and steps_since_log > 0:\n",
    "                avg_loss = total_loss_since_log / steps_since_log\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\", lr=f\"{current_lr:.2e}\", step=f\"{global_step}\")\n",
    "                history['steps'].append(global_step)\n",
    "                history['train_loss'].append(avg_loss)\n",
    "                total_loss_since_log = 0.0\n",
    "                steps_since_log = 0\n",
    "\n",
    "            # --- Validation & Checkpointing ---\n",
    "            if dev_loader and config.validation_interval_steps > 0 and global_step % config.validation_interval_steps == 0 and global_step > 0:\n",
    "                print(f\"\\\\nRunning validation at step {global_step}...\")\n",
    "                val_start_time = time.time()\n",
    "                val_results = validate_epoch(model, dev_loader, config.device)\n",
    "                val_end_time = time.time()\n",
    "                print(f\"Validation finished in {val_end_time - val_start_time:.2f}s\")\n",
    "\n",
    "                metric_log_str = f\"  Validation Step {global_step}: \"\n",
    "                history['val_metrics'][global_step] = val_results\n",
    "                sorted_keys = sorted(val_results.keys())\n",
    "                for name in sorted_keys: metric_log_str += f\"{name}: {val_results[name]:.4f} | \"\n",
    "                print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "                current_val_metric_for_scheduler = val_results.get(config.metric_to_track.replace('_', ' '), None)\n",
    "                if lr_scheduler and config.scheduler_type == \"reduce_on_plateau\":\n",
    "                     if current_val_metric_for_scheduler is not None:\n",
    "                         lr_scheduler.step(current_val_metric_for_scheduler)\n",
    "                         print(f\"  RoP Scheduler step called with {config.metric_to_track}={current_val_metric_for_scheduler:.4f}\")\n",
    "                     else:\n",
    "                         print(f\"  Warning: Metric '{config.metric_to_track}' not found. RoP Scheduler not stepped.\")\n",
    "\n",
    "                current_val_metric = val_results.get(config.metric_to_track.replace('_', ' '), None)\n",
    "                is_best = False\n",
    "                save_path = None\n",
    "                save_path_periodic = None\n",
    "\n",
    "                if current_val_metric is not None:\n",
    "                    improvement_threshold = best_val_metric + config.early_stopping_min_delta if config.mode == \"max\" else best_val_metric - config.early_stopping_min_delta\n",
    "                    if config.mode == \"max\": is_best = current_val_metric > improvement_threshold\n",
    "                    else: is_best = current_val_metric < improvement_threshold\n",
    "\n",
    "                    if is_best:\n",
    "                        print(f\"  Metric '{config.metric_to_track}' improved from {best_val_metric:.4f} to {current_val_metric:.4f}. Saving best model.\")\n",
    "                        best_val_metric = current_val_metric\n",
    "                        early_stopping_counter = 0\n",
    "                        # --- Use SigLIP Loss specific name ---\n",
    "                        save_path = os.path.join(config.model_path, \"phobert_sigliploss_best.pt\")\n",
    "                    else:\n",
    "                        early_stopping_counter += 1\n",
    "                        print(f\"  Metric '{config.metric_to_track}' did not improve. Best: {best_val_metric:.4f}. Counter: {early_stopping_counter}/{config.early_stopping_patience}\")\n",
    "\n",
    "                    if config.save_interval_steps > 0 and global_step % config.save_interval_steps == 0:\n",
    "                        # --- Use SigLIP Loss specific name ---\n",
    "                        periodic_save_path = os.path.join(config.model_path, f\"phobert_sigliploss_step_{global_step}.pt\")\n",
    "                        if save_path != periodic_save_path:\n",
    "                            print(f\"  Saving periodic checkpoint to {periodic_save_path}\")\n",
    "                            save_path_periodic = periodic_save_path\n",
    "\n",
    "                    if save_path or save_path_periodic:\n",
    "                        save_dict = { # Populate with relevant info\n",
    "                            'step': global_step, 'epoch': epoch + 1,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_metric': best_val_metric,\n",
    "                            'metric_tracked': config.metric_to_track,\n",
    "                            'current_val_metrics': val_results,\n",
    "                            'vision_model_name': config.vision_model_name,\n",
    "                            'text_model_name': config.text_model_name,\n",
    "                            'projection_dim': config.projection_dim,\n",
    "                            'learnable_temperature': config.learnable_temperature,\n",
    "                            'temperature_init': config.temperature_init,\n",
    "                            'learnable_bias': config.learnable_bias,\n",
    "                            'bias_init': config.bias_init,\n",
    "                            'max_length': config.max_length,\n",
    "                        }\n",
    "                        if lr_scheduler: save_dict['scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "                        if scaler: save_dict['scaler_state_dict'] = scaler.state_dict()\n",
    "                        if save_path: torch.save(save_dict, save_path)\n",
    "                        if save_path_periodic: torch.save(save_dict, save_path_periodic)\n",
    "                else:\n",
    "                    print(f\"  Warning: Metric '{config.metric_to_track}' not found. Cannot save best or check early stopping.\")\n",
    "                    early_stopping_counter += 1\n",
    "\n",
    "                if early_stopping_counter >= config.early_stopping_patience:\n",
    "                    print(f\"\\\\nEarly stopping triggered after {early_stopping_counter} validation checks without improvement.\")\n",
    "                    break # Break INNER loop\n",
    "\n",
    "                model.train()\n",
    "\n",
    "        # --- End of Epoch ---\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {datetime.timedelta(seconds=epoch_end_time - epoch_start_time)} ---\")\n",
    "        print(f\"--- Average Train Loss for Epoch {epoch+1}: {epoch_loss_meter.avg:.4f} ---\")\n",
    "\n",
    "        # --- Epoch-based Validation (if validation_interval_steps <= 0) ---\n",
    "        # (Keep the epoch-based validation block from previous clean version if you want that option)\n",
    "        # ...\n",
    "\n",
    "        if early_stopping_counter >= config.early_stopping_patience:\n",
    "           break\n",
    "\n",
    "    # --- End of Training ---\n",
    "    end_train_time = time.time()\n",
    "    total_duration = datetime.timedelta(seconds=end_train_time - start_train_time)\n",
    "    print(f\"=============== Fine-tuning (SigLIP Loss) Finished ================\")\n",
    "    print(f\"Total Training Time: {total_duration}\")\n",
    "\n",
    "    # --- Use SigLIP Loss specific name ---\n",
    "    final_model_path = os.path.join(config.model_path, 'phobert_sigliploss_final.pt')\n",
    "    final_save_dict = { # Populate final save dict\n",
    "        'step': global_step, 'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_metric': best_val_metric,\n",
    "        'metric_tracked': config.metric_to_track,\n",
    "        'vision_model_name': config.vision_model_name,\n",
    "        'text_model_name': config.text_model_name,\n",
    "        'projection_dim': config.projection_dim,\n",
    "        'learnable_temperature': config.learnable_temperature,\n",
    "        'temperature_init': config.temperature_init,\n",
    "        'learnable_bias': config.learnable_bias,\n",
    "        'bias_init': config.bias_init,\n",
    "        'max_length': config.max_length,\n",
    "    }\n",
    "    if lr_scheduler: final_save_dict['scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "    if scaler: final_save_dict['scaler_state_dict'] = scaler.state_dict()\n",
    "    torch.save(final_save_dict, final_model_path)\n",
    "    print(f\"Final model state saved to {final_model_path}\")\n",
    "\n",
    "    # --- Use SigLIP Loss specific name ---\n",
    "    best_model_file = os.path.join(config.model_path, \"phobert_sigliploss_best.pt\")\n",
    "    if dev_loader and os.path.exists(best_model_file):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) is saved at: {best_model_file}\")\n",
    "    elif dev_loader: print(\"Best model checkpoint file not found.\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 13: Final Evaluation on Test Set (Updated for SigLIP Loss Model) ===\n",
    "import traceback\n",
    "from types import SimpleNamespace\n",
    "from collections import OrderedDict\n",
    "\n",
    "print(\"\\\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_loader = None\n",
    "model_to_test = None\n",
    "\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "test_image_path = config.image_base_path\n",
    "\n",
    "# 1. Check prerequisites & Create Test Loader\n",
    "# ... (Keep data loading logic as before) ...\n",
    "if os.path.exists(test_json_path) and 'tokenizer' in globals() and tokenizer and 'image_processor' in globals() and image_processor:\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    try:\n",
    "        test_dataset = CustomImageCaptionDataset(\n",
    "            json_path=test_json_path, image_base_path=test_image_path,\n",
    "            tokenizer=tokenizer, image_processor=image_processor,\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "        if test_dataset.data:\n",
    "            num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            persist_workers_test = (num_workers > 0)\n",
    "            try: _ = DataLoader(test_dataset, num_workers=num_workers, persistent_workers=persist_workers_test)\n",
    "            except TypeError: persist_workers_test = False\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=config.batch_size * 2, shuffle=False,\n",
    "                num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "                drop_last=False, persistent_workers=persist_workers_test\n",
    "            )\n",
    "            print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "        else: print(\"Test dataset loaded but is empty.\")\n",
    "    except Exception as e: print(f\"Error creating test dataset/loader: {e}\")\n",
    "else: print(\"Skipping test evaluation: Test JSON, Tokenizer or Image Processor not found/loaded.\")\n",
    "\n",
    "# 2. Load Model for Testing\n",
    "if test_loader:\n",
    "    try:\n",
    "        # --- Use SigLIP Loss specific names ---\n",
    "        best_model_path = os.path.join(config.model_path, \"phobert_sigliploss_best.pt\")\n",
    "        final_model_path = os.path.join(config.model_path, \"phobert_sigliploss_final.pt\")\n",
    "        load_path = None\n",
    "\n",
    "        if os.path.exists(best_model_path): load_path = best_model_path; print(f\"\\\\nLoading best model: {load_path}\")\n",
    "        elif os.path.exists(final_model_path): load_path = final_model_path; print(f\"\\\\nLoading final model: {load_path}\")\n",
    "        else: print(f\"\\\\nWARNING: No checkpoints found in {config.model_path} to evaluate.\")\n",
    "\n",
    "        if load_path:\n",
    "            checkpoint = torch.load(load_path, map_location=config.device)\n",
    "            print(\"Re-creating model structure for testing...\")\n",
    "\n",
    "            # --- Create temp config based on saved checkpoint ---\n",
    "            temp_config_dict = {\n",
    "                'device': config.device,\n",
    "                'vision_model_name': checkpoint.get('vision_model_name', config.selected_vision_source),\n",
    "                'text_model_name': checkpoint.get('text_model_name', config.selected_text_model),\n",
    "                'vision_embedding': config.vision_embedding,\n",
    "                'text_embedding': config.text_embedding,\n",
    "                'projection_dim': checkpoint.get('projection_dim', config.projection_dim),\n",
    "                # Load SigLIP loss params correctly\n",
    "                'learnable_temperature': checkpoint.get('learnable_temperature', config.learnable_temperature),\n",
    "                'temperature_init': checkpoint.get('temperature_init', config.temperature_init),\n",
    "                'learnable_bias': checkpoint.get('learnable_bias', config.learnable_bias),\n",
    "                'bias_init': checkpoint.get('bias_init', config.bias_init),\n",
    "            }\n",
    "            temp_config = SimpleNamespace(**temp_config_dict)\n",
    "\n",
    "            print(f\"  Using Vision Source: {temp_config.vision_model_name}\")\n",
    "            print(f\"  Using Text Model: {temp_config.text_model_name}\")\n",
    "\n",
    "            # --- Instantiate the CORRECT model class ---\n",
    "            test_image_encoder = ImageEncoder(temp_config, pretrained=False).to(config.device)\n",
    "            test_text_encoder = TextEncoder(temp_config, pretrained=False).to(config.device)\n",
    "            # Use the model class that includes bias\n",
    "            model_to_test = ViPhobertSiglipLossModel(test_image_encoder, test_text_encoder, temp_config).to(config.device)\n",
    "\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "                print(\"Detected 'module.' prefix, removing.\")\n",
    "                state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "\n",
    "            load_result = model_to_test.load_state_dict(state_dict, strict=True)\n",
    "            print(f\"  State dict loading result: {load_result}\")\n",
    "            print(f\"Model weights loaded successfully.\")\n",
    "\n",
    "            print(\"\\\\nRunning evaluation on test set...\")\n",
    "            test_results = validate_epoch(model_to_test, test_loader, config.device) # Use same val function\n",
    "\n",
    "            print(\"\\\\n--- Test Set Results ---\")\n",
    "            metric_log_str = \"\"\n",
    "            sorted_keys = sorted(test_results.keys())\n",
    "            for name in sorted_keys: metric_log_str += f\"  {name}: {test_results[name]:.4f}\\\\n\"\n",
    "            print(metric_log_str.strip())\n",
    "            print(\"------------------------\")\n",
    "        else:\n",
    "            print(\"Evaluation skipped (no weights found).\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\nERROR during test setup/evaluation: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\\\n================= Evaluation Finished ==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3055fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 14: Training Visualization (Adapted for Steps/Epochs) ===\n",
    "# (No changes needed here, plotting logic is adaptable)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def plot_training_metrics(history, plot_dir, plot_by='epoch'):\n",
    "    \"\"\"Plots training and validation metrics.\"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    print(f\"Plot directory ensured at: {os.path.abspath(plot_dir)}\")\n",
    "    if not history: print(\"No history data provided.\"); return\n",
    "\n",
    "    if plot_by == 'step' and history.get('steps') and history.get('train_loss'):\n",
    "        x_axis_train = history['steps']; x_label = 'Global Steps'\n",
    "        x_axis_val = sorted(history.get('val_metrics', {}).keys()) if history.get('val_metrics') else []\n",
    "    elif history.get('train_loss') and history.get('validation_results'):\n",
    "         num_epochs_trained = len(history['train_loss']); num_epochs_validated = len([res for res in history['validation_results'] if res is not None])\n",
    "         x_axis_train = range(1, num_epochs_trained + 1); x_axis_val = range(1, num_epochs_validated + 1); x_label = 'Epoch'; plot_by = 'epoch'\n",
    "    else: print(\"Insufficient history data.\"); return\n",
    "\n",
    "    val_metrics_data = history.get('val_metrics', {}) if plot_by == 'step' else history.get('validation_results', [])\n",
    "    valid_val_results = [res for res in val_metrics_data if res is not None] if plot_by=='epoch' else \\\n",
    "                        [val_metrics_data.get(step) for step in x_axis_val if val_metrics_data.get(step)]\n",
    "\n",
    "    # Training Loss\n",
    "    if history.get('train_loss'):\n",
    "        plt.figure(figsize=(10, 6)); plt.plot(x_axis_train, history['train_loss'], 'b-', label=f'Training Loss (Avg per {\"Log Interval\" if plot_by==\"step\" else \"Epoch\"})'); plt.xlabel(x_label); plt.ylabel('Loss'); plt.title(f'Training Loss over {x_label.capitalize()}')\n",
    "        if valid_val_results:\n",
    "            try:\n",
    "                val_loss = [val_metrics_data[step].get('loss', float('nan')) for step in x_axis_val] if plot_by == 'step' else [res.get('loss', float('nan')) for res in valid_val_results]\n",
    "                x_axis_val_loss = x_axis_val if len(val_loss) == len(x_axis_val) else range(1, len(val_loss) + 1) if plot_by=='epoch' else x_axis_val[:len(val_loss)]\n",
    "                if any(not math.isnan(vl) for vl in val_loss): plt.plot(x_axis_val_loss, val_loss, 'r-', label='Validation Loss')\n",
    "            except (KeyError, TypeError, IndexError): print(\"Validation loss not found or incorrectly formatted.\")\n",
    "        plt.legend(); plt.grid(True); plt.tight_layout(); save_path_loss = os.path.join(plot_dir, f'training_loss_{plot_by}.png'); plt.savefig(save_path_loss, dpi=300); print(f\"Saved training loss plot to: {save_path_loss}\"); plt.close()\n",
    "    else: print(\"No training loss data to plot.\")\n",
    "\n",
    "    # Validation Metrics\n",
    "    if valid_val_results and x_axis_val:\n",
    "        first_valid_val_result = valid_val_results[0]\n",
    "        if first_valid_val_result and isinstance(first_valid_val_result, dict):\n",
    "            metrics_to_plot = [k for k in first_valid_val_result.keys() if k != 'loss']\n",
    "            num_plots = len(metrics_to_plot)\n",
    "            if num_plots > 0:\n",
    "                ncols = 2; nrows = math.ceil(num_plots / ncols); fig, axes = plt.subplots(nrows, ncols, figsize=(8 * ncols, 6 * nrows), squeeze=False); axes = axes.flatten()\n",
    "                for i, metric_name in enumerate(metrics_to_plot):\n",
    "                    metric_values = [val_metrics_data[step].get(metric_name, float('nan')) for step in x_axis_val] if plot_by == 'step' else [res.get(metric_name, float('nan')) for res in valid_val_results]\n",
    "                    x_axis_val_metric = x_axis_val if len(metric_values) == len(x_axis_val) else range(1, len(metric_values) + 1) if plot_by=='epoch' else x_axis_val[:len(metric_values)]\n",
    "                    if any(not math.isnan(v) for v in metric_values):\n",
    "                        axes[i].plot(x_axis_val_metric, metric_values, 'r-o', label=f'Validation {metric_name}'); axes[i].set_xlabel(x_label); axes[i].set_ylabel(metric_name.replace('_', ' ').capitalize()); axes[i].set_title(f'Validation {metric_name} over {x_label.capitalize()}'); axes[i].legend(); axes[i].grid(True)\n",
    "                    else: axes[i].set_title(f'Validation {metric_name} (No Data)'); axes[i].text(0.5, 0.5, 'No Data', ha='center', va='center')\n",
    "                for j in range(i + 1, len(axes)): fig.delaxes(axes[j])\n",
    "                fig.suptitle(f'Validation Metrics over {x_label.capitalize()}', fontsize=16, y=1.02 if nrows>1 else 1.05); plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "                save_path_val = os.path.join(plot_dir, f'validation_metrics_{plot_by}.png'); plt.savefig(save_path_val, dpi=300); print(f\"Saved validation metrics plot to: {save_path_val}\"); plt.close()\n",
    "            else: print(\"No validation metrics (excluding loss) found to plot.\")\n",
    "        else: print(\"No valid validation results found.\")\n",
    "    else: print(\"No validation metrics found in history to plot.\")\n",
    "\n",
    "\n",
    "# --- Plotting ---\n",
    "plot_directory = f\"{config.model_path}/plots\"\n",
    "plotting_mode = 'step' if config.validation_interval_steps > 0 else 'epoch'\n",
    "\n",
    "if 'history' in locals() and isinstance(history, dict):\n",
    "    plot_training_metrics(history, plot_directory, plot_by=plotting_mode)\n",
    "else:\n",
    "    print(\"No training history found. Run training first.\")\n",
    "\n",
    "# --- END OF SCRIPT ---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
