{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Installs and Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time # For timing epochs\n",
    "from py_vncorenlp import VnCoreNLP\n",
    "\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model output path: ./ViCLIP_uitcopenviic\n",
      "Image base path (for resolving paths in JSON): /home/researcher/huypq69/TuningModels/data/UIT-OpenViIC-dataset\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration Class (CFG)\n",
    "# import torch\n",
    "# import os\n",
    "\n",
    "# ==============================================================================\n",
    "# Configuration Class (CFG)\n",
    "# ==============================================================================\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    # Base directory where your train.json, dev.json, test.json are located\n",
    "    data_path = \"./json_data/\"\n",
    "    image_path = \"./data/UIT-OpenViIC-dataset\"\n",
    "\n",
    "    # Output directory for saved models\n",
    "    model_path = \"./ViCLIP_uitcopenviic\"\n",
    "\n",
    "    # --- Available Models ---\n",
    "    text_models = {\n",
    "        \"PhoBERT-base\": \"vinai/phobert-base\",\n",
    "        \"PhoBERT-large\": \"vinai/phobert-large\",\n",
    "        \"ViT5-base\": \"VietAI/vit5-base\",\n",
    "        \"ViT5-large\": \"VietAI/vit5-large\"\n",
    "    }\n",
    "    image_models = {\n",
    "        \"ViT-S\": \"vit_small_patch16_224\", \"ViT-B\": \"vit_base_patch16_224\",\n",
    "        \"ViT-L\": \"vit_large_patch16_224\", \"ViT-H\": \"vit_huge_patch16_224\",\n",
    "        \"ResNet50\": \"resnet50\"\n",
    "    }\n",
    "\n",
    "    # --- User Selections ---\n",
    "    selected_text_model = \"PhoBERT-base\"\n",
    "    selected_image_model = \"ResNet50\"\n",
    "\n",
    "    # --- Model parameters based on selection (Properties) ---\n",
    "    @property\n",
    "    def model_name(self): return self.image_models[self.selected_image_model]\n",
    "    @property\n",
    "    def text_encoder_model(self): return self.text_models[self.selected_text_model]\n",
    "    @property\n",
    "    def text_tokenizer(self): return self.text_models[self.selected_text_model]\n",
    "\n",
    "    @property\n",
    "    def text_embedding(self): # Encoder output dim before projection\n",
    "        model_key = self.text_models[self.selected_text_model]\n",
    "        if \"large\" in model_key: return 1024\n",
    "        elif \"base\" in model_key: return 768\n",
    "        else: print(f\"Warning: Unknown text embedding size for {model_key}, defaulting to 768.\"); return 768\n",
    "\n",
    "    @property\n",
    "    def image_embedding(self): # Encoder output dim before projection\n",
    "        if self.selected_image_model == \"ResNet50\": return 2048\n",
    "        elif self.selected_image_model == \"ViT-S\": return 384\n",
    "        elif self.selected_image_model == \"ViT-B\": return 768\n",
    "        elif self.selected_image_model == \"ViT-L\": return 1024\n",
    "        elif self.selected_image_model == \"ViT-H\": return 1280\n",
    "        else: print(f\"Warning: Unknown image embedding size for {self.model_name}, defaulting to 768.\"); return 768\n",
    "    \n",
    "    # --- Fixed parameters ---\n",
    "    projection_dim = 256\n",
    "    seed = 42\n",
    "\n",
    "    # --- Training parameters ---\n",
    "    batch_size = 64 \n",
    "    num_workers = 8   \n",
    "    head_lr = 2e-4\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 2e-5\n",
    "    weight_decay = 5e-3\n",
    "    patience = 2\n",
    "    factor = 0.8\n",
    "    epochs = 8\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- Image/Text parameters ---\n",
    "    size = 224 \n",
    "    max_length = 200\n",
    "\n",
    "    # --- Loss/Saving parameters ---\n",
    "    temperature = 0.07\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"avg_acc\"\n",
    "    mode = \"max\" if metric_to_track != \"loss\" else \"min\"\n",
    "\n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_path)}\") # Show resolved path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Seeding for Reproducibility\n",
    "\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # For multi-GPU\n",
    "        # These can slow down training, use cautiously if performance is critical\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Metric Calculation Utilities\n",
    "\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        # Ensure val is a scalar number before adding to sum\n",
    "        if torch.is_tensor(val):\n",
    "             val = val.item() # Convert tensor to Python number\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "        # else:\n",
    "            # Optionally print a warning if the value is not usable\n",
    "            # print(f\"Warning: Cannot update AvgMeter '{self.name}' with value type {type(val)}\")\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    n = similarity_matrix.shape[1-dim]\n",
    "    correct_count = 0\n",
    "    top_k_indices = torch.topk(similarity_matrix, k, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "\n",
    "    if dim == 0: # I2T\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]:\n",
    "                correct_count += 1\n",
    "    elif dim == 1: # T2I\n",
    "        for txt_idx in range(n):\n",
    "             if ground_truth[txt_idx] in top_k_indices[txt_idx, :]:\n",
    "                correct_count += 1\n",
    "    else: raise ValueError(\"dim must be 0 or 1\")\n",
    "    return correct_count / n if n > 0 else 0.0\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    sim_matrix = text_embeddings @ image_embeddings.T\n",
    "    sim_matrix = sim_matrix.float() # Ensure float for calculations\n",
    "    n = sim_matrix.shape[0]\n",
    "    if n == 0:\n",
    "        # Return default zero metrics for empty batch\n",
    "        return {\n",
    "            \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "            \"avg_cosine_sim\": 0.0,\n",
    "            \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "        }\n",
    "\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "    i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "    t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "    t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "    avg_acc = (i2t_acc + t2i_acc) / 2\n",
    "    avg_cosine_sim = torch.diagonal(sim_matrix).mean().item()\n",
    "\n",
    "    i2t_recall = {}\n",
    "    t2i_recall = {}\n",
    "    recall_k_values = [k for k in [1, 5, 10] if k <= n]\n",
    "    for k in recall_k_values:\n",
    "        i2t_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "        t2i_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "\n",
    "    # Ensure all keys R@1, R@5, R@10 exist even if k>n\n",
    "    for k in [1, 5, 10]:\n",
    "        k_str = f\"R@{k}\"\n",
    "        if k_str not in i2t_recall: i2t_recall[k_str] = 0.0\n",
    "        if k_str not in t2i_recall: t2i_recall[k_str] = 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "        \"avg_cosine_sim\": avg_cosine_sim,\n",
    "        \"i2t_recall\": i2t_recall, \"t2i_recall\": t2i_recall\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageCaptionDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Dataset Class Definition\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, tokenizer, transforms, max_length):\n",
    "        super().__init__()\n",
    "        print(f\"Attempting to load data from: {os.path.abspath(json_path)}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                self.data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: JSON file not found at {json_path}\")\n",
    "            print(f\"       Please ensure '{json_path}' exists relative to your notebook or provide the full path.\")\n",
    "            self.data = []\n",
    "            # Optionally raise error: raise FileNotFoundError(f\"JSON file not found at {json_path}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Could not decode JSON from {json_path}\")\n",
    "            self.data = []\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred loading {json_path}: {e}\")\n",
    "            self.data = []\n",
    "\n",
    "        print(f\"Found {len(self.data)} samples in {os.path.basename(json_path)}.\")\n",
    "        self.image_base_path = image_base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transforms = transforms\n",
    "        self.max_length = max_length\n",
    "        # Optional: Check if the image base path exists\n",
    "        if not os.path.isdir(self.image_base_path):\n",
    "             print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "             print(f\"         Ensure 'image_path' in CFG points to the correct directory relative to your notebook.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data):\n",
    "             raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path', None) # Use .get for safety\n",
    "        caption = item.get('caption', '') # Use .get for safety\n",
    "\n",
    "        if relative_image_path is None:\n",
    "            print(f\"Warning: Missing 'image_path' for item at index {idx}. Returning dummy data.\")\n",
    "            image = torch.zeros((3, config.size, config.size))\n",
    "        else:\n",
    "            image_path = os.path.normpath(os.path.join(self.image_base_path, relative_image_path))\n",
    "            # /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000000000.png\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image = self.transforms(image)\n",
    "            except FileNotFoundError:\n",
    "                # Reduced verbosity for notebook, but keep essential info\n",
    "                print(f\"Warning: Img not found: {image_path}. Base: {self.image_base_path}, Rel: {relative_image_path}\")\n",
    "                image = torch.zeros((3, config.size, config.size))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading image {image_path}: {e}\")\n",
    "                image = torch.zeros((3, config.size, config.size))\n",
    "\n",
    "\n",
    "        text_inputs = self.tokenizer(\n",
    "            caption, padding='max_length', truncation=True,\n",
    "            max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        input_ids = text_inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = text_inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        if input_ids.dim() == 0: input_ids = input_ids.unsqueeze(0)\n",
    "        if attention_mask.dim() == 0: attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "print(\"ImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model components and loss function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Model Component Definitions (Encoders, CLIP Model, Loss)\n",
    "\n",
    "# --- Image Encoder ---\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, config, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        print(f\"Initializing Image Encoder: {config.selected_image_model}\")\n",
    "        if config.selected_image_model == \"ResNet50\":\n",
    "            weights = models.ResNet50_Weights.DEFAULT if pretrained else None\n",
    "            self.model = models.resnet50(weights=weights)\n",
    "            self.input_features = config.image_embedding\n",
    "            self.model.fc = nn.Identity()\n",
    "            print(f\"  Loaded ResNet50 from torchvision. Input features: {self.input_features}\")\n",
    "        # --- Add ViT logic here if needed ---\n",
    "        # elif \"ViT\" in config.selected_image_model:\n",
    "        #     try:\n",
    "        #         import timm\n",
    "        #     except ImportError:\n",
    "        #         print(\"Please install timm library ('pip install timm') to use ViT models.\")\n",
    "        #         raise\n",
    "        #     print(f\"  Loading {config.model_name} using timm.\")\n",
    "        #     self.model = timm.create_model(config.model_name, pretrained=pretrained)\n",
    "        #     self.input_features = config.image_embedding # Should match timm model output before head\n",
    "        #     # Verify input_features matches actual output if possible, e.g., self.model.embed_dim\n",
    "        #     # print(f\"    ViT embed_dim: {self.model.embed_dim}\") # Example check\n",
    "        #     self.model.head = nn.Identity() # Remove classification head\n",
    "        #     print(f\"    Removed head. Input features to projection: {self.input_features}\")\n",
    "        # --- End ViT logic ---\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image model type in config: {config.selected_image_model}\")\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, self.config.projection_dim)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {self.config.projection_dim}\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        # ViT models might output class token + patch tokens, check model architecture\n",
    "        # If using ViT, might need features = features[:, 0] to get class token\n",
    "        features = features.view(features.size(0), -1) # Flatten if needed\n",
    "        projected_features = self.projection(features)\n",
    "        projected_features = F.normalize(projected_features, p=2, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "# --- Text Encoder ---\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, config, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.input_features = config.text_embedding # Dim before projection\n",
    "        print(f\"Initializing Text Encoder: {config.text_encoder_model}\")\n",
    "        print(f\"  Expected input features: {self.input_features}\")\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config.text_encoder_model)\n",
    "        else:\n",
    "            model_config = AutoConfig.from_pretrained(config.text_encoder_model)\n",
    "            self.model = AutoModel.from_config(model_config)\n",
    "\n",
    "        # Check if actual model hidden size matches config\n",
    "        actual_hidden_size = self.model.config.hidden_size\n",
    "        if actual_hidden_size != self.input_features:\n",
    "             print(f\"WARNING: Configured text_embedding ({self.input_features}) does not match actual model hidden size ({actual_hidden_size}). Using actual size for projection.\")\n",
    "             self.input_features = actual_hidden_size # Use actual size\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, self.config.projection_dim)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {self.config.projection_dim}\")\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        if input_ids.dim() == 1: input_ids = input_ids.unsqueeze(0)\n",
    "        if attention_mask.dim() == 1: attention_mask = attention_mask.unsqueeze(0)\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Using [CLS] token embedding\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        projected_features = self.projection(cls_embedding)\n",
    "        projected_features = F.normalize(projected_features, p=2, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "# --- CLIP Model ---\n",
    "class CLIPViModel(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, config):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.temperature = config.temperature\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.image_encoder(image)\n",
    "        text_features = self.text_encoder(input_ids, attention_mask)\n",
    "        logit_scale = 1 / self.temperature\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        return logits_per_image, logits_per_text, image_features, text_features\n",
    "\n",
    "# --- Loss Function ---\n",
    "def contrastive_loss(logits_per_image, logits_per_text):\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    if batch_size == 0: return torch.tensor(0.0, device=logits_per_image.device, requires_grad=True) # Handle empty batch\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "    loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "    total_loss = (loss_img + loss_txt) / 2.0\n",
    "    return total_loss\n",
    "\n",
    "print(\"Model components and loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation epoch functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Training and Validation Epoch Functions\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch_num):\n",
    "    model.train()\n",
    "    loss_meter = AvgMeter(f\"Train Loss E{epoch_num}\")\n",
    "    # Using tqdm.notebook for progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_size = images.size(0)\n",
    "        if batch_size == 0: continue # Skip empty batches\n",
    "\n",
    "        logits_per_image, logits_per_text, _, _ = model(images, input_ids, attention_mask)\n",
    "        loss = contrastive_loss(logits_per_image, logits_per_text)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_meter.update(loss.item(), batch_size)\n",
    "        progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    return loss_meter.avg\n",
    "\n",
    "def validate_epoch(model, dataloader, device, epoch_num):\n",
    "    model.eval()\n",
    "    # Initialize meters\n",
    "    loss_meter = AvgMeter(f\"Val Loss E{epoch_num}\")\n",
    "    acc_meter = AvgMeter(f\"Val Acc E{epoch_num}\")\n",
    "    cos_sim_meter = AvgMeter(f\"Val CosSim E{epoch_num}\")\n",
    "    # Initialize all potential recall meters\n",
    "    recall_meters = {\n",
    "        \"i2t_R@1\": AvgMeter(f\"Val I2T R@1 E{epoch_num}\"), \"i2t_R@5\": AvgMeter(f\"Val I2T R@5 E{epoch_num}\"), \"i2t_R@10\": AvgMeter(f\"Val I2T R@10 E{epoch_num}\"),\n",
    "        \"t2i_R@1\": AvgMeter(f\"Val T2I R@1 E{epoch_num}\"), \"t2i_R@5\": AvgMeter(f\"Val T2I R@5 E{epoch_num}\"), \"t2i_R@10\": AvgMeter(f\"Val T2I R@10 E{epoch_num}\"),\n",
    "    }\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = images.size(0)\n",
    "            if batch_size == 0: continue # Skip empty batches\n",
    "\n",
    "            logits_per_image, logits_per_text, image_features, text_features = model(images, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(logits_per_image, logits_per_text)\n",
    "            metrics = compute_metrics(image_features, text_features)\n",
    "\n",
    "            # Update meters\n",
    "            loss_meter.update(loss.item(), batch_size)\n",
    "            acc_meter.update(metrics[\"avg_acc\"], batch_size)\n",
    "            cos_sim_meter.update(metrics[\"avg_cosine_sim\"], batch_size)\n",
    "            for k_val, meter in recall_meters.items():\n",
    "                 recall_type, recall_key = k_val.split('_') # 'i2t'/'t2i', 'R@k'\n",
    "                 if recall_key in metrics[f\"{recall_type}_recall\"]:\n",
    "                    meter.update(metrics[f\"{recall_type}_recall\"][recall_key], batch_size)\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\", acc=f\"{acc_meter.avg:.3f}\")\n",
    "\n",
    "    # Collect results\n",
    "    validation_results = {\"loss\": loss_meter.avg, \"avg_acc\": acc_meter.avg, \"avg_cosine_sim\": cos_sim_meter.avg}\n",
    "    for k_val, meter in recall_meters.items():\n",
    "         validation_results[k_val.replace(\"_\", \" \")] = meter.avg # Format key nicely\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "print(\"Training and Validation epoch functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer: vinai/phobert-base\n",
      "Tokenizer loaded successfully.\n",
      "Image transforms defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Setup - Tokenizer and Transforms\n",
    "\n",
    "# 1. Load Tokenizer\n",
    "print(f\"Loading Tokenizer: {config.text_tokenizer}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer)\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading tokenizer '{config.text_tokenizer}': {e}\")\n",
    "    print(\"Please ensure the model name is correct and you have internet access or the model is cached.\")\n",
    "    # Optionally raise the error to stop execution\n",
    "    # raise e\n",
    "\n",
    "# 2. Define Image Transforms\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(config.size),\n",
    "    transforms.CenterCrop(config.size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # ImageNet stats\n",
    "])\n",
    "print(\"Image transforms defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/json_data/train.json\n",
      "Found 41238 samples in train.json.\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/json_data/dev.json\n",
      "Found 10002 samples in dev.json.\n",
      "\n",
      "Creating dataloaders...\n",
      "Using 8 workers for DataLoaders.\n",
      "Train loader created with 645 batches.\n",
      "Validation loader created with 157 batches.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Setup - Datasets and DataLoaders\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "# Construct JSON paths using config.data_path\n",
    "# /Users/quanghuypham/Desktop/CP_Glimpse/Tuning-CLIP/data/UIT-OpenViIC-dataset/test.json\n",
    "train_json = f\"{os.path.abspath(config.data_path)}/train.json\"\n",
    "dev_json = f\"{os.path.abspath(config.data_path)}/dev.json\"\n",
    "test_json = f\"{os.path.abspath(config.data_path)}/test.json\"\n",
    "\n",
    "# Make sure tokenizer is loaded before creating datasets\n",
    "if 'tokenizer' not in globals():\n",
    "     print(\"ERROR: Tokenizer not loaded. Please run the previous cell.\")\n",
    "else:\n",
    "    train_dataset = ImageCaptionDataset(\n",
    "        json_path=train_json, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, transforms=image_transforms, max_length=config.max_length\n",
    "    )\n",
    "    dev_dataset = ImageCaptionDataset(\n",
    "        json_path=dev_json, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, transforms=image_transforms, max_length=config.max_length\n",
    "    )\n",
    "\n",
    "    # Basic checks after loading\n",
    "    if not train_dataset.data:\n",
    "        print(\"\\nERROR: Failed to load training data. Check 'train_json' path and format.\")\n",
    "        # Optionally raise an error: raise ValueError(\"Training data failed to load\")\n",
    "    if not dev_dataset.data:\n",
    "         print(\"\\nWARNING: Failed to load validation data. Validation steps will be skipped.\")\n",
    "\n",
    "    print(\"\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    train_loader = None\n",
    "    if train_dataset.data:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False # Keep last incomplete batch for training\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "\n",
    "    dev_loader = None\n",
    "    if dev_dataset.data:\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False # Keep last incomplete batch for validation\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "\n",
    "    if not train_loader:\n",
    "         print(\"\\nERROR: Train loader could not be created. Cannot proceed.\")\n",
    "         # raise ValueError(\"Train loader creation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model components...\n",
      "Initializing Image Encoder: ResNet50\n",
      "  Loaded ResNet50 from torchvision. Input features: 2048\n",
      "  Added projection head: 2048 -> 256\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Expected input features: 768\n",
      "  Added projection head: 768 -> 256\n",
      "\n",
      "CLIPViModel initialized successfully on cuda.\n",
      "Trainable parameters: 159.23 M\n",
      "\n",
      "Setting up optimizer...\n",
      "  Param counts: ImgBase=159, ImgHead=2, TxtBase=199, TxtHead=2\n",
      "Optimizer AdamW initialized.\n",
      "LR Scheduler ReduceLROnPlateau initialized (mode='max', factor=0.8, patience=2)\n",
      "Early stopping initialized (patience=5, min_delta=0.001)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Setup - Model, Optimizer, Scheduler\n",
    "\n",
    "print(\"\\nInitializing model components...\")\n",
    "try:\n",
    "    image_encoder = ImageEncoder(config).to(config.device)\n",
    "    text_encoder = TextEncoder(config).to(config.device)\n",
    "    model = CLIPViModel(image_encoder, text_encoder, config).to(config.device)\n",
    "    print(f\"\\nCLIPViModel initialized successfully on {config.device}.\")\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {num_params / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing models: {e}\")\n",
    "    print(\"Check model names in CFG, internet connection, and available memory.\")\n",
    "    # raise e # Optionally stop execution\n",
    "\n",
    "# --- Optimizer Setup ---\n",
    "if 'model' in globals() : # Check if model was created\n",
    "    print(\"\\nSetting up optimizer...\")\n",
    "    image_encoder_params = list(model.image_encoder.model.parameters())\n",
    "    image_head_params = list(model.image_encoder.projection.parameters())\n",
    "    text_encoder_params = list(model.text_encoder.model.parameters())\n",
    "    text_head_params = list(model.text_encoder.projection.parameters())\n",
    "\n",
    "    print(f\"  Param counts: ImgBase={len(image_encoder_params)}, ImgHead={len(image_head_params)}, TxtBase={len(text_encoder_params)}, TxtHead={len(text_head_params)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for p in image_encoder_params if p.requires_grad], \"lr\": config.image_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": [p for p in image_head_params if p.requires_grad], \"lr\": config.head_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": [p for p in text_encoder_params if p.requires_grad], \"lr\": config.text_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": [p for p in text_head_params if p.requires_grad], \"lr\": config.head_lr, \"weight_decay\": config.weight_decay},\n",
    "    ]\n",
    "\n",
    "    # Filter out groups with no parameters (can happen if parts are frozen)\n",
    "    optimizer_grouped_parameters = [g for g in optimizer_grouped_parameters if g['params']]\n",
    "\n",
    "    if not optimizer_grouped_parameters:\n",
    "         print(\"ERROR: No parameters found for the optimizer. Check model structure and requires_grad flags.\")\n",
    "         # raise ValueError(\"Optimizer has no parameters\")\n",
    "    else:\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "        print(f\"Optimizer AdamW initialized.\")\n",
    "\n",
    "        # --- LR Scheduler Setup ---\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode=config.mode, # Use mode from config ('min' for loss, 'max' for acc/recall)\n",
    "            factor=config.factor, patience=config.patience\n",
    "        )\n",
    "        print(f\"LR Scheduler ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.factor}, patience={config.patience})\")\n",
    "        \n",
    "        # --- Early Stopping Setup ---\n",
    "        early_stopping_patience = getattr(config, 'early_stopping_patience', 5) \n",
    "        early_stopping_min_delta = getattr(config, 'early_stopping_min_delta', 0.001)\n",
    "        early_stopping_counter = 0\n",
    "        print(f\"Early stopping initialized (patience={early_stopping_patience}, min_delta={early_stopping_min_delta})\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not initialized. Skipping optimizer/scheduler setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 8 epochs...\n",
      "Tracking metric: 'avg_acc' (mode: max)\n",
      "\n",
      "--- Epoch 1/8 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12f00cce044432d9e176d4d864d53d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E1:   0%|          | 0/645 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/researcher/huypq69/TuningModels/.venv/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/researcher/huypq69/TuningModels/.venv/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/researcher/huypq69/TuningModels/.venv/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/researcher/huypq69/TuningModels/.venv/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Training Loop\n",
    "\n",
    "if 'model' in globals() and 'train_loader' in globals() and 'optimizer' in globals(): # Check prerequisites\n",
    "    print(f\"\\nStarting training for {config.epochs} epochs...\")\n",
    "    print(f\"Tracking metric: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    history = {'train_loss': [], 'validation_results': []}\n",
    "    start_train_time = time.time()\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "\n",
    "        # --- Training ---\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, config.device, epoch+1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_results = {\"loss\": float('inf'), \"avg_acc\": 0.0} # Default if no validation\n",
    "        if dev_loader:\n",
    "            val_results = validate_epoch(model, dev_loader, config.device, epoch+1)\n",
    "            history['validation_results'].append(val_results)\n",
    "            # Print validation metrics\n",
    "            print(\"  Validation Metrics:\")\n",
    "            metric_log_str = \"  \"\n",
    "            for name, value in val_results.items():\n",
    "                metric_log_str += f\"{name}: {value:.4f} | \"\n",
    "            print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "            # Step the LR scheduler based on the tracked metric\n",
    "            current_val_metric_for_scheduler = val_results.get(config.metric_to_track, None)\n",
    "            if current_val_metric_for_scheduler is not None:\n",
    "                 lr_scheduler.step(current_val_metric_for_scheduler)\n",
    "                 current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "                 # Simplified LR print: print first + last group LR assuming Head LRs are same\n",
    "                 print(f\"  Current LRs: ImgEnc={current_lrs[0]:.2e}, Head={current_lrs[1]:.2e}, TxtEnc={current_lrs[2]:.2e}\")\n",
    "            else:\n",
    "                 print(f\"  Warning: Metric '{config.metric_to_track}' not found in validation results. Scheduler not stepped.\")\n",
    "\n",
    "        else:\n",
    "             print(\"  Validation skipped.\")\n",
    "             history['validation_results'].append(None) # Append None if no validation\n",
    "\n",
    "        # --- Save Checkpoint ---\n",
    "        current_val_metric = val_results.get(config.metric_to_track, -float('inf') if config.mode == \"max\" else float('inf'))\n",
    "\n",
    "        is_best = False\n",
    "        if dev_loader: # Only compare if validation was done\n",
    "            if config.mode == \"max\" and current_val_metric > best_val_metric:\n",
    "                is_best = True\n",
    "                best_val_metric = current_val_metric\n",
    "            elif config.mode == \"min\" and current_val_metric < best_val_metric:\n",
    "                is_best = True\n",
    "                best_val_metric = current_val_metric\n",
    "\n",
    "        # Prepare save dictionary\n",
    "        save_dict = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'validation_results': val_results,\n",
    "            'best_val_metric': best_val_metric, # Store the best metric value seen so far\n",
    "            'metric_tracked': config.metric_to_track, # Store which metric was tracked\n",
    "        }\n",
    "\n",
    "        # Save logic\n",
    "        if config.save_best_only and dev_loader:\n",
    "            if is_best:\n",
    "                best_checkpoint_path = os.path.join(config.model_path, \"clip_vi_best.pt\")\n",
    "                torch.save(save_dict, best_checkpoint_path)\n",
    "                print(f\"  Saved Best Model (Epoch {epoch+1}, {config.metric_to_track}={current_val_metric:.4f}) to {best_checkpoint_path}\")\n",
    "        else: # Save every epoch if not save_best_only\n",
    "            epoch_checkpoint_path = os.path.join(config.model_path, f\"clip_vi_epoch_{epoch+1}.pt\")\n",
    "            torch.save(save_dict, epoch_checkpoint_path)\n",
    "            print(f\"  Saved Epoch {epoch+1} Checkpoint to {epoch_checkpoint_path}\")\n",
    "            if is_best and dev_loader: # Also save a copy as best if it's the best so far\n",
    "                 best_checkpoint_path = os.path.join(config.model_path, \"clip_vi_best.pt\")\n",
    "                 torch.save(save_dict, best_checkpoint_path)\n",
    "                 print(f\"  (Also saved as best model)\")\n",
    "\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {epoch_end_time - epoch_start_time:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "    # --- End of Training ---\n",
    "    end_train_time = time.time()\n",
    "    total_train_time = end_train_time - start_train_time\n",
    "    print(f\"\\n=============== Training Finished ===============\")\n",
    "    print(f\"Total Training Time: {total_train_time:.2f} seconds ({total_train_time/60:.2f} minutes)\")\n",
    "    final_model_path = os.path.join(config.model_path, 'clip_vi_final_epoch.pt')\n",
    "    torch.save(save_dict, final_model_path) # Save the final epoch state regardless\n",
    "    print(f\"Final epoch model state saved to {final_model_path}\")\n",
    "    if config.save_best_only and dev_loader and os.path.exists(os.path.join(config.model_path, \"clip_vi_best.pt\")):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) saved to: {os.path.join(config.model_path, 'clip_vi_best.pt')}\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training (model, dataloader, optimizer) not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Starting Test Set Evaluation ===============\n",
      "Loading test data from: ./data/UIT-OpenViIC-dataset/test.json\n",
      "Attempting to load data from: /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/test.json\n",
      "Found 10001 samples in test.json.\n",
      "Test loader created with 157 batches.\n",
      "Initializing Image Encoder: ResNet50\n",
      "  Loaded ResNet50 from torchvision. Input features: 2048\n",
      "  Added projection head: 2048 -> 256\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Expected input features: 768\n",
      "  Added projection head: 768 -> 256\n",
      "Model structure for testing created.\n",
      "Attempting to load best model weights from: ./clip_vi/clip_vi_best.pt\n",
      "Model weights loaded successfully from ./clip_vi/clip_vi_best.pt\n",
      "\n",
      "Running evaluation on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3890adecb3d40b1b0d32cc3b4b5faf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation ETest:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000001010.jpg: image file is truncated (155 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000001010.jpg: image file is truncated (155 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000001010.jpg: image file is truncated (155 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000001010.jpg: image file is truncated (155 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000001010.jpg: image file is truncated (155 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000005095.jpg: image file is truncated (76 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000005095.jpg: image file is truncated (76 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000005095.jpg: image file is truncated (76 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000005095.jpg: image file is truncated (76 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000005095.jpg: image file is truncated (76 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000000768.jpg: image file is truncated (18 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000000768.jpg: image file is truncated (18 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000000768.jpg: image file is truncated (18 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000000768.jpg: image file is truncated (18 bytes not processed)\n",
      "Warning: Error loading image /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000000768.jpg: image file is truncated (18 bytes not processed)\n",
      "\n",
      "--- Test Set Results ---\n",
      "loss: 2.3549\n",
      "  avg_acc: 0.1869\n",
      "  avg_cosine_sim: 0.5004\n",
      "  i2t R@1: 0.1962\n",
      "  i2t R@5: 0.8134\n",
      "  i2t R@10: 0.9316\n",
      "  t2i R@1: 0.1776\n",
      "  t2i R@5: 0.8455\n",
      "  t2i R@10: 0.9390\n",
      "------------------------\n",
      "\n",
      "================= Evaluation Finished =================\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Final Evaluation on Test Set\n",
    "\n",
    "print(\"\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "\n",
    "if os.path.exists(test_json_path) and 'tokenizer' in globals():\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    test_dataset = ImageCaptionDataset(\n",
    "        json_path=test_json_path, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, transforms=image_transforms, max_length=config.max_length\n",
    "    )\n",
    "\n",
    "    if test_dataset.data:\n",
    "        num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False\n",
    "        )\n",
    "        print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "\n",
    "        # --- Load Model for Testing ---\n",
    "        # Create a fresh model instance\n",
    "        try:\n",
    "            test_image_encoder = ImageEncoder(config).to(config.device)\n",
    "            test_text_encoder = TextEncoder(config).to(config.device)\n",
    "            model_to_test = CLIPViModel(test_image_encoder, test_text_encoder, config).to(config.device)\n",
    "            print(\"Model structure for testing created.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR creating model structure for testing: {e}\")\n",
    "            model_to_test = None # Prevent loading if structure fails\n",
    "\n",
    "        if model_to_test:\n",
    "            # Determine which model weights to load (best or final)\n",
    "            best_model_path = os.path.join(config.model_path, \"clip_vi_best.pt\")\n",
    "            final_model_path = os.path.join(config.model_path, \"clip_vi_final_epoch.pt\") # Use final epoch saved name\n",
    "\n",
    "            load_path = None\n",
    "            if os.path.exists(best_model_path):\n",
    "                load_path = best_model_path\n",
    "                print(f\"Attempting to load best model weights from: {load_path}\")\n",
    "            elif os.path.exists(final_model_path):\n",
    "                load_path = final_model_path\n",
    "                print(f\"Best model not found. Attempting to load final epoch weights from: {load_path}\")\n",
    "            else:\n",
    "                print(\"WARNING: No saved model checkpoints ('best' or 'final') found in output directory.\")\n",
    "                print(\"         Evaluation will not be performed.\")\n",
    "\n",
    "            if load_path:\n",
    "                try:\n",
    "                    checkpoint = torch.load(load_path, map_location=config.device)\n",
    "                    state_dict = checkpoint['model_state_dict']\n",
    "                    # Handle potential 'module.' prefix if saved with DataParallel or DDP\n",
    "                    if next(iter(state_dict)).startswith('module.'):\n",
    "                        print(\"Detected 'module.' prefix, removing for loading.\")\n",
    "                        from collections import OrderedDict\n",
    "                        new_state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "                        model_to_test.load_state_dict(new_state_dict)\n",
    "                    else:\n",
    "                        model_to_test.load_state_dict(state_dict)\n",
    "                    print(f\"Model weights loaded successfully from {load_path}\")\n",
    "\n",
    "                    # --- Run Evaluation ---\n",
    "                    print(\"\\nRunning evaluation on test set...\")\n",
    "                    test_results = validate_epoch(model_to_test, test_loader, config.device, epoch_num=\"Test\") # Use validate func\n",
    "\n",
    "                    print(\"\\n--- Test Set Results ---\")\n",
    "                    metric_log_str = \"\"\n",
    "                    for name, value in test_results.items():\n",
    "                        metric_log_str += f\"  {name}: {value:.4f}\\n\"\n",
    "                    print(metric_log_str.strip())\n",
    "                    print(\"------------------------\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nERROR loading model weights or running evaluation from {load_path}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc() # Print detailed traceback\n",
    "            # --- End Model Loading/Eval ---\n",
    "        # --- End Model Structure Check ---\n",
    "    else:\n",
    "         print(\"Could not load test data. Skipping test evaluation.\")\n",
    "else:\n",
    "    if not os.path.exists(test_json_path):\n",
    "         print(f\"\\nTest JSON path not found ({test_json_path}). Skipping test evaluation.\")\n",
    "    if 'tokenizer' not in globals():\n",
    "         print(\"\\nTokenizer not available. Skipping test evaluation.\")\n",
    "\n",
    "print(\"\\n================= Evaluation Finished =================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Training Results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Check if we have training history to visualize\n",
    "if 'training_stats' in globals() and training_stats:\n",
    "    print(\"Generating training visualization plots...\")\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Training and Validation Metrics', fontsize=16)\n",
    "    \n",
    "    # Extract metrics from training stats\n",
    "    epochs = [stat['epoch'] for stat in training_stats]\n",
    "    train_loss = [stat['train_loss'] for stat in training_stats]\n",
    "    val_loss = [stat['val_loss'] for stat in training_stats if 'val_loss' in stat]\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    axs[0, 0].plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "    if val_loss:\n",
    "        axs[0, 0].plot(epochs[:len(val_loss)], val_loss, 'r-', label='Validation Loss')\n",
    "    axs[0, 0].set_title('Loss')\n",
    "    axs[0, 0].set_xlabel('Epoch')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].grid(True)\n",
    "    \n",
    "    # Plot other metrics if available\n",
    "    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    plot_positions = [(0, 1), (1, 0), (1, 1)]\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        if i < len(plot_positions):\n",
    "            train_metric = [stat.get(f'train_{metric}', np.nan) for stat in training_stats]\n",
    "            val_metric = [stat.get(f'val_{metric}', np.nan) for stat in training_stats]\n",
    "            \n",
    "            # Only plot if we have valid data\n",
    "            if not all(np.isnan(train_metric)) or not all(np.isnan(val_metric)):\n",
    "                row, col = plot_positions[i]\n",
    "                if not all(np.isnan(train_metric)):\n",
    "                    axs[row, col].plot(epochs, train_metric, 'b-', label=f'Training {metric.capitalize()}')\n",
    "                if not all(np.isnan(val_metric)):\n",
    "                    axs[row, col].plot(epochs, val_metric, 'r-', label=f'Validation {metric.capitalize()}')\n",
    "                axs[row, col].set_title(f'{metric.capitalize()}')\n",
    "                axs[row, col].set_xlabel('Epoch')\n",
    "                axs[row, col].set_ylabel(metric.capitalize())\n",
    "                axs[row, col].legend()\n",
    "                axs[row, col].grid(True)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    # Save the figure if output directory exists\n",
    "    if 'config' in globals() and hasattr(config, 'output_dir') and os.path.exists(config.output_dir):\n",
    "        plot_path = os.path.join(config.output_dir, 'training_metrics.png')\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"Training visualization saved to: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training statistics available for visualization.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
