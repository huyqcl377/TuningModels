{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "Transformers Version: 4.50.0\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: Imports ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR # Keep Cosine for option\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True # Handle potential image loading issues\n",
    "\n",
    "# --- Import SigLIP Vision model and its Processor ---\n",
    "from transformers import SiglipVisionModel, SiglipConfig, SiglipImageProcessor\n",
    "# --- Keep PhoBERT parts ---\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "# --- Blip models no longer needed for loading ---\n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import transformers\n",
    "import gc\n",
    "import traceback # Import traceback\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    # Ensure you're setting the correct device index if needed\n",
    "    device_idx = 0 # Or 0\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(device_idx)}\")\n",
    "    torch.cuda.set_device(device_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Per-Device Batch Size: 128\n",
      "Accumulation Steps: 1\n",
      "Effective Batch Size (per optimizer step): 128\n",
      "Model output path: ./trained_models/ViSigLIP_vivqa\n",
      "Selected Vision Source: google/siglip-base-patch16-224\n",
      "Selected Text Model: vinai/phobert-base\n",
      "Image base path (for resolving paths in JSON): /home/researcher/huypq69/TuningModels/data/OpenViVQA-dataset\n",
      "AMP Enabled: True\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Configuration Class (CFG) - Modified for SigLIP Vision + PhoBERT Text ===\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    data_path = \"./refined_json/\"\n",
    "    image_base_path = \"./data/OpenViVQA-dataset/\"\n",
    "    model_path = \"./trained_models/ViSigLIP_vivqa\"\n",
    "\n",
    "    # --- Model Selection ---\n",
    "    # --- SigLIP Vision Model ---\n",
    "    selected_vision_source = \"google/siglip-base-patch16-224\"\n",
    "    # --- Keep PhoBERT Text Model ---\n",
    "    selected_text_model = \"vinai/phobert-base\"\n",
    "    text_tokenizer_name = selected_text_model\n",
    "\n",
    "    # --- Model parameters ---\n",
    "    vision_model_name = selected_vision_source # For clarity\n",
    "    text_model_name = selected_text_model   # For clarity\n",
    "    # --- Image Processor: Use SigLIP's processor ---\n",
    "    image_processor_name = selected_vision_source\n",
    "\n",
    "    @property\n",
    "    def text_embedding(self): return 768 # PhoBERT-base output\n",
    "    @property\n",
    "    def vision_embedding(self): return 768 # Siglip-base-patch16-224 output\n",
    "\n",
    "    projection_dim = 768 # Common projection dim for CLIP-style models (adjust if needed, e.g., 768)\n",
    "\n",
    "    # --- Fine-tuning parameters ---\n",
    "    seed = 42\n",
    "    # Adjust batch size based on VRAM for SigLIP base + PhoBERT base\n",
    "    batch_size = 128\n",
    "    num_workers = 20\n",
    "    accumulation_steps = 1 # Effective batch size = 128\n",
    "\n",
    "    # --- Learning Rates for Fine-tuning ---\n",
    "    projection_lr = 1e-4\n",
    "    vision_encoder_lr = 1e-5 # Lower LR for SigLIP backbone\n",
    "    text_encoder_lr = 2e-5   # Slightly higher LR for PhoBERT backbone\n",
    "    weight_decay = 1e-4     # Lower weight decay for fine-tuning\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    # --- Use standard contrastive loss temperature (like CLIP) ---\n",
    "    temperature = 0.07\n",
    "    learnable_temperature = True\n",
    "    # --- Bias term is NOT used in standard contrastive loss ---\n",
    "    learnable_bias = False\n",
    "    bias_init = 0.0\n",
    "\n",
    "    # --- Scheduler ---\n",
    "    scheduler_type = \"reduce_on_plateau\" # RoP often used for fine-tuning\n",
    "    rop_patience = 2\n",
    "    rop_factor = 0.8\n",
    "\n",
    "    epochs = 200 \n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_amp = True # Keep AMP enabled\n",
    "\n",
    "    # --- Image/Text parameters ---\n",
    "    max_length = 77 \n",
    "\n",
    "    # --- Loss/Saving parameters ---\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"avg_acc\" # Track validation accuracy\n",
    "    mode = \"max\"\n",
    "    # Adjust intervals if needed\n",
    "    save_interval_steps = 1000 # Save periodically during fine-tuning (optional)\n",
    "    validation_interval_steps = 1000 # Validate more often during fine-tuning\n",
    "    log_interval_steps = 50\n",
    "\n",
    "    early_stopping_patience = 5 # Patience in terms of validation checks\n",
    "    early_stopping_min_delta = 0.001 # Min change to be considered improvement\n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Per-Device Batch Size: {config.batch_size}\")\n",
    "print(f\"Accumulation Steps: {config.accumulation_steps}\")\n",
    "print(f\"Effective Batch Size (per optimizer step): {config.batch_size * config.accumulation_steps}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Selected Vision Source: {config.selected_vision_source}\")\n",
    "print(f\"Selected Text Model: {config.selected_text_model}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_base_path)}\")\n",
    "print(f\"AMP Enabled: {config.use_amp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: Seeding ===\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Metric & AvgMeter Utilities ===\n",
    "# (Keep AvgMeter, compute_recall_at_k, compute_metrics as before)\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0.0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        if torch.is_tensor(val):\n",
    "            val = val.item()\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = float(self.sum) / self.count if self.count != 0 else 0.0\n",
    "        else:\n",
    "            print(f\"Warning: Cannot update AvgMeter '{self.name}' with value type {type(val)}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.name}: {self.avg:.4f}\"\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    n = similarity_matrix.shape[1-dim]\n",
    "    if n == 0: return 0.0\n",
    "    correct_count = 0\n",
    "    actual_k = min(k, similarity_matrix.shape[dim])\n",
    "    if actual_k == 0: return 0.0\n",
    "    top_k_indices = torch.topk(similarity_matrix, actual_k, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "    if dim == 0: # I2T\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]:\n",
    "                correct_count += 1\n",
    "    elif dim == 1: # T2I\n",
    "        for txt_idx in range(n):\n",
    "             if ground_truth[txt_idx] in top_k_indices[txt_idx, :]:\n",
    "                correct_count += 1\n",
    "    else: raise ValueError(\"dim must be 0 or 1\")\n",
    "    return float(correct_count) / n if n > 0 else 0.0\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    sim_matrix = text_embeddings.float() @ image_embeddings.float().T\n",
    "    n = sim_matrix.shape[0]\n",
    "    if n == 0:\n",
    "        return {\n",
    "            \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "            \"avg_cosine_sim\": 0.0,\n",
    "            \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "        }\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "    i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "    t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "    t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "    avg_acc = (i2t_acc + t2i_acc) / 2.0\n",
    "    avg_cosine_sim = torch.diag(sim_matrix).mean().item()\n",
    "    i2t_recall = {}\n",
    "    t2i_recall = {}\n",
    "    recall_k_values = [k for k in [1, 5, 10] if k <= n]\n",
    "    for k in recall_k_values:\n",
    "        i2t_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "        t2i_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "    for k in [1, 5, 10]:\n",
    "        k_str = f\"R@{k}\"\n",
    "        if k_str not in i2t_recall: i2t_recall[k_str] = 0.0\n",
    "        if k_str not in t2i_recall: t2i_recall[k_str] = 0.0\n",
    "    metrics = {\n",
    "        \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "        \"avg_cosine_sim\": avg_cosine_sim,\n",
    "        \"i2t_recall\": i2t_recall, \"t2i_recall\": t2i_recall\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomImageCaptionDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Dataset Class Definition (Corrected JSON Loading & Processor Update) ===\n",
    "\n",
    "import traceback\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True # Keep allowing truncated images\n",
    "\n",
    "class CustomImageCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads image-caption pairs from JSON metadata.\n",
    "    Handles both single JSON list format and JSON-per-line format.\n",
    "    Uses specified image_processor (SigLIP or other).\n",
    "    \"\"\"\n",
    "    def __init__(self, json_path_or_list, image_base_path, tokenizer, image_processor, max_length):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        if isinstance(json_path_or_list, str) and os.path.isdir(json_path_or_list):\n",
    "             json_files = [os.path.join(json_path_or_list, f) for f in os.listdir(json_path_or_list) if f.endswith('.json')]\n",
    "             print(f\"Found {len(json_files)} JSON files in {json_path_or_list}\")\n",
    "        elif isinstance(json_path_or_list, str) and os.path.isfile(json_path_or_list):\n",
    "            json_files = [json_path_or_list]\n",
    "        elif isinstance(json_path_or_list, list):\n",
    "            json_files = json_path_or_list\n",
    "        else:\n",
    "            raise ValueError(\"json_path_or_list must be a directory, a single JSON file, or a list of JSON files.\")\n",
    "\n",
    "        print(\"Loading JSON metadata...\")\n",
    "        total_loaded_count = 0\n",
    "        for json_path in tqdm(json_files, desc=\"Loading JSONs\"):\n",
    "            try:\n",
    "                with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        file_data = json.load(f)\n",
    "                        if isinstance(file_data, list):\n",
    "                             self.data.extend(file_data)\n",
    "                             total_loaded_count += len(file_data)\n",
    "                        else:\n",
    "                             self.data.append(file_data)\n",
    "                             total_loaded_count += 1\n",
    "                             print(f\"  Warning: Loaded single JSON object from {json_path}, expected a list.\")\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"  Info: Failed to load {json_path} as single JSON. Attempting JSON-per-line format...\")\n",
    "                        f.seek(0)\n",
    "                        count_line_by_line = 0\n",
    "                        for line in f:\n",
    "                            line = line.strip()\n",
    "                            if line:\n",
    "                                try:\n",
    "                                    line_data = json.loads(line)\n",
    "                                    self.data.append(line_data)\n",
    "                                    count_line_by_line += 1\n",
    "                                except json.JSONDecodeError as line_err:\n",
    "                                     print(f\"  ERROR parsing line in {json_path}: {line_err}. Line content (partial): {line[:100]}...\")\n",
    "                        total_loaded_count += count_line_by_line\n",
    "                        if count_line_by_line > 0:\n",
    "                            print(f\"  Successfully loaded {count_line_by_line} items using JSON-per-line format from {json_path}.\")\n",
    "                        else:\n",
    "                             print(f\"  Failed to load any data using JSON-per-line format from {json_path} either.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR opening or processing file {json_path}: {e}\")\n",
    "\n",
    "        print(f\"Loaded {total_loaded_count} samples total from {len(json_files)} file(s).\")\n",
    "        self.data = [item for item in self.data if item]\n",
    "        print(f\"Dataset size after potential cleaning: {len(self.data)}\")\n",
    "\n",
    "        if not self.data:\n",
    "             print(\"WARNING: No data loaded!\")\n",
    "\n",
    "        self.image_base_path = image_base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor # Store the passed processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # --- Get image size from the loaded processor ---\n",
    "        try:\n",
    "            # SiglipImageProcessor uses config.image_size\n",
    "            self.img_size = image_processor.config.image_size\n",
    "        except AttributeError:\n",
    "            # Fallback for BlipImageProcessor or older versions\n",
    "            try:\n",
    "                 if isinstance(image_processor.size, dict):\n",
    "                     proc_size = image_processor.size\n",
    "                     self.img_size = proc_size.get('height', proc_size.get('shortest_edge', 224))\n",
    "                 else:\n",
    "                     self.img_size = image_processor.size\n",
    "                     if isinstance(self.img_size, (tuple, list)): self.img_size = self.img_size[0]\n",
    "            except AttributeError:\n",
    "                 print(\"Warning: Could not determine image size from processor, defaulting to 224.\")\n",
    "                 self.img_size = 224\n",
    "        print(f\"Using image target size: {self.img_size}x{self.img_size}\")\n",
    "        # ----------------------------------------------\n",
    "\n",
    "        if not os.path.isdir(self.image_base_path):\n",
    "            print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data): raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "\n",
    "        relative_image_path = item.get('image_path', item.get('url', item.get('filename')))\n",
    "        caption_data = item.get('caption', item.get('text', item.get('title', '')))\n",
    "        if isinstance(caption_data, list):\n",
    "            caption = caption_data[0] if caption_data else \"\"\n",
    "        elif isinstance(caption_data, str):\n",
    "            caption = caption_data\n",
    "        else:\n",
    "            caption = \"\"\n",
    "\n",
    "        if not relative_image_path or not caption:\n",
    "            return self._get_dummy_item()\n",
    "\n",
    "        # Load Image\n",
    "        try:\n",
    "            image_path = os.path.join(self.image_base_path, relative_image_path)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            # --- Use the stored image processor ---\n",
    "            image_inputs = self.image_processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = image_inputs['pixel_values'].squeeze(0)\n",
    "            # ------------------------------------\n",
    "        except Exception: # Catch broad exceptions during loading/processing\n",
    "            return self._get_dummy_item()\n",
    "\n",
    "        # Process Text\n",
    "        try:\n",
    "            text_inputs = self.tokenizer(\n",
    "                caption, padding='max_length', truncation=True,\n",
    "                max_length=self.max_length, return_tensors='pt'\n",
    "            )\n",
    "            input_ids = text_inputs['input_ids'].squeeze(0)\n",
    "            attention_mask = text_inputs['attention_mask'].squeeze(0)\n",
    "        except Exception:\n",
    "            return self._get_dummy_item()\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "    def _get_dummy_item(self):\n",
    "        # Use self.img_size determined in __init__\n",
    "        return {\n",
    "            \"pixel_values\": torch.zeros((3, self.img_size, self.img_size), dtype=torch.float),\n",
    "            \"input_ids\": torch.zeros(self.max_length, dtype=torch.long),\n",
    "            \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"CustomImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViPhobertSiglip Model components defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Model Definition (SigLIP Vision + PhoBERT Text) ===\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Encodes images using SigLIP's Vision Model.\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        print(f\"Initializing SigLIP Vision Encoder from: {config_train.vision_model_name}\")\n",
    "\n",
    "        if pretrained:\n",
    "            try:\n",
    "                self.vision_model = SiglipVisionModel.from_pretrained(config_train.vision_model_name)\n",
    "                print(\"  SigLIP Vision model loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR loading pretrained SiglipVisionModel: {e}\")\n",
    "                raise # Stop if vision model fails to load\n",
    "        else:\n",
    "            print(\"  Initializing SiglipVisionModel from scratch.\")\n",
    "            siglip_vision_config = SiglipConfig.from_pretrained(config_train.vision_model_name).vision_config\n",
    "            self.vision_model = SiglipVisionModel(siglip_vision_config)\n",
    "\n",
    "        try:\n",
    "            self.input_features = self.vision_model.config.hidden_size\n",
    "        except AttributeError as e:\n",
    "             print(f\"  ERROR accessing vision_model.config.hidden_size: {e}. Attempting config_train value.\")\n",
    "             self.input_features = config_train.vision_embedding # Fallback\n",
    "\n",
    "        if hasattr(config_train, 'vision_embedding') and self.input_features != config_train.vision_embedding:\n",
    "             print(f\"  WARNING: Configured vision_embedding ({config_train.vision_embedding}) doesn't match loaded model hidden size ({self.input_features}). Using actual size.\")\n",
    "        else:\n",
    "             print(f\"  Confirmed/Using vision model hidden size: {self.input_features}\")\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        vision_outputs = self.vision_model(pixel_values=pixel_values, return_dict=True)\n",
    "        image_embed = vision_outputs.pooler_output\n",
    "        projected_features = self.projection(image_embed)\n",
    "        return projected_features\n",
    "\n",
    "# --- TextEncoder remains the same (loading PhoBERT) ---\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Encodes text using PhoBERT-Base.\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        print(f\"Initializing Text Encoder: {config_train.text_model_name}\")\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config_train.text_model_name)\n",
    "        else:\n",
    "            model_config = AutoConfig.from_pretrained(config_train.text_model_name)\n",
    "            self.model = AutoModel.from_config(model_config)\n",
    "        try:\n",
    "            self.input_features = self.model.config.hidden_size\n",
    "        except AttributeError as e:\n",
    "            print(f\"  ERROR accessing model.config.hidden_size: {e}. Attempting config_train value.\")\n",
    "            self.input_features = config_train.text_embedding # Fallback\n",
    "        if hasattr(config_train, 'text_embedding') and self.input_features != config_train.text_embedding:\n",
    "             print(f\"  WARNING: Configured text_embedding ({config_train.text_embedding}) doesn't match loaded PhoBERT hidden size ({self.input_features}). Using actual size.\")\n",
    "        else:\n",
    "            print(f\"  Confirmed text model hidden size: {self.input_features}\")\n",
    "        self.projection = nn.Linear(self.input_features, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        text_features = outputs.last_hidden_state[:, 0, :]\n",
    "        projected_features = self.projection(text_features)\n",
    "        return projected_features\n",
    "\n",
    "# --- Combined Model (CLIP-Style) ---\n",
    "class ViPhobertSiglipModel(nn.Module): # Renamed for clarity\n",
    "    \"\"\"Combines SigLIP Vision encoder and PhoBERT Text encoder for contrastive retrieval.\"\"\"\n",
    "    def __init__(self, image_encoder, text_encoder, config_train):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.config_train = config_train\n",
    "\n",
    "        if config_train.learnable_temperature:\n",
    "            init_val_t = torch.tensor(np.log(1 / config_train.temperature), dtype=torch.float)\n",
    "            self.logit_scale = nn.Parameter(init_val_t)\n",
    "            print(f\"Using learnable temperature (logit_scale), initialized to {self.logit_scale.exp().item():.4f}\")\n",
    "        else:\n",
    "            temp_tensor = torch.tensor(np.log(1 / config_train.temperature), dtype=torch.float)\n",
    "            self.register_buffer('logit_scale', temp_tensor)\n",
    "            print(f\"Using fixed temperature: {config_train.temperature}\")\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        pixel_values = pixel_values.to(self.config_train.device)\n",
    "        input_ids = input_ids.to(self.config_train.device)\n",
    "        attention_mask = attention_mask.to(self.config_train.device)\n",
    "\n",
    "        image_embed = self.image_encoder(pixel_values)\n",
    "        text_embed = self.text_encoder(input_ids, attention_mask)\n",
    "\n",
    "        image_features = F.normalize(image_embed, p=2, dim=-1)\n",
    "        text_features = F.normalize(text_embed, p=2, dim=-1)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp().clamp(max=100)\n",
    "\n",
    "        logits_per_image = logit_scale.float() * image_features.float() @ text_features.float().t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text, image_features, text_features\n",
    "\n",
    "print(\"ViPhobertSiglip Model components defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Contrastive loss function defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 7: Loss Function (Standard Contrastive Loss) ===\n",
    "def contrastive_loss(logits_per_image, logits_per_text):\n",
    "    \"\"\" Standard InfoNCE-based contrastive loss \"\"\"\n",
    "    logits_per_image = logits_per_image.float()\n",
    "    logits_per_text = logits_per_text.float()\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    if batch_size == 0:\n",
    "        return torch.tensor(0.0, device=logits_per_image.device, requires_grad=True)\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "    loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "    total_loss = (loss_img + loss_txt) / 2.0\n",
    "    return total_loss\n",
    "\n",
    "print(\"Standard Contrastive loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer: vinai/phobert-base\n",
      "PhoBERT Tokenizer loaded successfully.\n",
      "Loading Image Processor from: google/siglip-base-patch16-224\n",
      "SigLIP Image Processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8: Setup - Tokenizer and Image Processor (Using SigLIP Processor) ===\n",
    "# --- Use SiglipImageProcessor ---\n",
    "from transformers import AutoTokenizer, SiglipImageProcessor\n",
    "\n",
    "tokenizer = None\n",
    "image_processor = None\n",
    "\n",
    "print(f\"Loading Tokenizer: {config.text_tokenizer_name}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer_name)\n",
    "    print(\"PhoBERT Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading tokenizer '{config.text_tokenizer_name}': {e}\")\n",
    "\n",
    "print(f\"Loading Image Processor from: {config.image_processor_name}\") # Use SigLIP name from CFG\n",
    "try:\n",
    "    # --- Load SiglipImageProcessor ---\n",
    "    image_processor = SiglipImageProcessor.from_pretrained(config.image_processor_name)\n",
    "    print(\"SigLIP Image Processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading image processor '{config.image_processor_name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nCreating datasets...\n",
      "Attempting to load training data from: ./refined_json/train.json\n",
      "Loading JSON metadata...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0509b7554b46de8124804ca3078f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading JSONs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15111 samples total from 1 file(s).\n",
      "Dataset size after potential cleaning: 15111\n",
      "Using image target size: 224x224\n",
      "Attempting to load validation data from: ./refined_json/dev.json\n",
      "Loading JSON metadata...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968aabc7a7a344c589d793f3dd158f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading JSONs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1765 samples total from 1 file(s).\n",
      "Dataset size after potential cleaning: 1765\n",
      "Using image target size: 224x224\n",
      "\\nCreating dataloaders...\n",
      "Using 20 workers for DataLoaders.\n",
      "Train loader created with 118 batches.\n",
      "Validation loader created with 7 batches.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 9: Setup - Datasets and DataLoaders (FIXED Validation Path) ===\n",
    "# Uses the dataset class defined above.\n",
    "\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "\n",
    "# Define paths\n",
    "validation_json_path = os.path.join(config.data_path, \"dev.json\") # <<< CHANGED FILENAME\n",
    "train_json_path = os.path.join(config.data_path, \"train.json\")\n",
    "\n",
    "if tokenizer and image_processor:\n",
    "    print(\"\\\\nCreating datasets...\")\n",
    "    # --- Training Dataset ---\n",
    "    try:\n",
    "        print(f\"Attempting to load training data from: {train_json_path}\")\n",
    "        train_dataset = CustomImageCaptionDataset(\n",
    "            json_path_or_list=train_json_path,\n",
    "            image_base_path=config.image_base_path,\n",
    "            tokenizer=tokenizer,\n",
    "            image_processor=image_processor, # Pass the loaded processor\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "        if not train_dataset.data: print(\"\\\\nERROR: Failed to load training data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating training dataset: {e}\")\n",
    "        train_dataset = None\n",
    "\n",
    "    # --- Validation Dataset ---\n",
    "    if os.path.exists(validation_json_path):\n",
    "         try:\n",
    "             print(f\"Attempting to load validation data from: {validation_json_path}\")\n",
    "             dev_dataset = CustomImageCaptionDataset(\n",
    "                 json_path_or_list=validation_json_path,\n",
    "                 image_base_path=config.image_base_path, # Assumes same base path\n",
    "                 tokenizer=tokenizer,\n",
    "                 image_processor=image_processor, # Use same processor\n",
    "                 max_length=config.max_length\n",
    "             )\n",
    "             if not dev_dataset.data: print(\"\\\\nWARNING: Failed to load validation data.\")\n",
    "         except Exception as e:\n",
    "             print(f\"ERROR creating validation dataset: {e}\")\n",
    "             dev_dataset = None\n",
    "    else:\n",
    "        print(f\"Validation JSON file not found at {validation_json_path}, skipping validation set creation.\")\n",
    "        dev_dataset = None\n",
    "\n",
    "    print(\"\\\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    if train_dataset and train_dataset.data:\n",
    "        persist_workers = (num_workers > 0)\n",
    "        try: # Check if persistent_workers is supported\n",
    "             _ = DataLoader(train_dataset, num_workers=num_workers, persistent_workers=persist_workers)\n",
    "        except TypeError:\n",
    "             persist_workers = False\n",
    "             print(\"Note: `persistent_workers=True` not supported by this PyTorch version/DataLoader setup.\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=True, # Keep drop_last=True for more stable training steps\n",
    "            persistent_workers=persist_workers\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "        # Calculate total training steps (only if using Cosine scheduler)\n",
    "        if config.scheduler_type == \"cosine\":\n",
    "            config.total_training_steps = len(train_loader) * config.epochs // config.accumulation_steps\n",
    "            print(f\"Total estimated training steps for Cosine Scheduler: {config.total_training_steps}\")\n",
    "    else:\n",
    "        print(\"Skipping train loader creation (no data).\")\n",
    "        config.total_training_steps = 0 # Set default if no loader\n",
    "\n",
    "    if dev_dataset and dev_dataset.data:\n",
    "        persist_workers_dev = (num_workers > 0)\n",
    "        try: # Check support for dev loader too\n",
    "             _ = DataLoader(dev_dataset, num_workers=num_workers, persistent_workers=persist_workers_dev)\n",
    "        except TypeError:\n",
    "             persist_workers_dev = False\n",
    "\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset, batch_size=config.batch_size * 2, shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False,\n",
    "            persistent_workers=persist_workers_dev\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "    else: print(\"Skipping validation loader creation.\")\n",
    "\n",
    "    if not train_loader: print(\"\\\\nERROR: Train loader could not be created.\")\n",
    "else:\n",
    "     print(\"ERROR: Tokenizer or Image Processor not loaded. Skipping dataset/loader creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nInitializing ViPhobertSiglip model components...\n",
      "Initializing SigLIP Vision Encoder from: google/siglip-base-patch16-224\n",
      "  SigLIP Vision model loaded successfully.\n",
      "  Confirmed/Using vision model hidden size: 768\n",
      "  Added projection head: 768 -> 768\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Confirmed text model hidden size: 768\n",
      "  Added projection head: 768 -> 768\n",
      "Using learnable temperature (logit_scale), initialized to 14.2857\n",
      "\\nViPhobertSiglip Model initialized successfully on cuda.\n",
      "Total parameters: 229.06 M\n",
      "Trainable parameters: 229.06 M\n",
      "\\nSetting up optimizer...\n",
      "Optimizer AdamW initialized with grouped LRs (Vision: 1e-05, Text: 2e-05, Proj/Temp: 0.0001), WD: 0.0001\n",
      "LR Scheduler: ReduceLROnPlateau initialized (mode='max', factor=0.8, patience=2)\n",
      "AMP GradScaler initialized.\n",
      "Early stopping enabled with patience: 5\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10: Setup - Model, Optimizer, Scheduler (Fine-tuning LRs & Corrected AMP) ===\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "scaler = None # For AMP\n",
    "\n",
    "print(\"\\\\nInitializing ViPhobertSiglip model components...\")\n",
    "try:\n",
    "    # Instantiate the encoders and main model\n",
    "    image_encoder = ImageEncoder(config).to(config.device)\n",
    "    text_encoder = TextEncoder(config).to(config.device)\n",
    "    # --- Instantiate the correct model ---\n",
    "    model = ViPhobertSiglipModel(image_encoder, text_encoder, config).to(config.device)\n",
    "\n",
    "    print(f\"\\\\nViPhobertSiglip Model initialized successfully on {config.device}.\")\n",
    "    num_params_total = sum(p.numel() for p in model.parameters())\n",
    "    num_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {num_params_total / 1e6:.2f} M\")\n",
    "    print(f\"Trainable parameters: {num_params_trainable / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing model components: {e}\")\n",
    "    traceback.print_exc()\n",
    "    model = None\n",
    "\n",
    "if model and train_loader: # Check train_loader exists\n",
    "    print(\"\\\\nSetting up optimizer...\")\n",
    "    # --- Optimizer with Fine-tuning LRs ---\n",
    "    vision_encoder_params = list(model.image_encoder.vision_model.parameters())\n",
    "    image_head_params = list(model.image_encoder.projection.parameters())\n",
    "    text_encoder_params = list(model.text_encoder.model.parameters())\n",
    "    text_head_params = list(model.text_encoder.projection.parameters())\n",
    "    logit_scale_param = [model.logit_scale] if isinstance(model.logit_scale, nn.Parameter) else []\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for p in vision_encoder_params if p.requires_grad], 'lr': config.vision_encoder_lr, 'weight_decay': config.weight_decay},\n",
    "        {'params': [p for p in image_head_params if p.requires_grad], 'lr': config.projection_lr, 'weight_decay': config.weight_decay},\n",
    "        {'params': [p for p in text_encoder_params if p.requires_grad], 'lr': config.text_encoder_lr, 'weight_decay': config.weight_decay},\n",
    "        {'params': [p for p in text_head_params if p.requires_grad], 'lr': config.projection_lr, 'weight_decay': config.weight_decay},\n",
    "        {'params': [p for p in logit_scale_param if p.requires_grad], 'lr': config.projection_lr, 'weight_decay': 0.0 }\n",
    "    ]\n",
    "\n",
    "    optimizer_grouped_parameters = [g for g in optimizer_grouped_parameters if g['params']]\n",
    "\n",
    "    if not optimizer_grouped_parameters:\n",
    "        print(\"ERROR: No trainable parameters found for the optimizer.\")\n",
    "    else:\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters, lr=config.learning_rate) # Base LR used if param not in group\n",
    "        print(f\"Optimizer AdamW initialized with grouped LRs (Vision: {config.vision_encoder_lr}, Text: {config.text_encoder_lr}, Proj/Temp: {config.projection_lr}), WD: {config.weight_decay}\")\n",
    "\n",
    "        # --- LR Scheduler ---\n",
    "        if config.scheduler_type == \"cosine\":\n",
    "            if hasattr(config, 'total_training_steps') and config.total_training_steps > 0:\n",
    "                 lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "                     optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=config.total_training_steps\n",
    "                 )\n",
    "                 print(f\"LR Scheduler: Cosine with Warmup ({config.warmup_steps} steps) initialized.\")\n",
    "            else:\n",
    "                 print(\"ERROR: total_training_steps not calculated or zero. Cannot init Cosine scheduler.\")\n",
    "                 lr_scheduler = None\n",
    "        elif config.scheduler_type == \"reduce_on_plateau\":\n",
    "            lr_scheduler = ReduceLROnPlateau(\n",
    "                optimizer, mode=config.mode, factor=config.rop_factor, patience=config.rop_patience\n",
    "            )\n",
    "            print(f\"LR Scheduler: ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.rop_factor}, patience={config.rop_patience})\")\n",
    "        else:\n",
    "            print(\"No LR Scheduler specified.\")\n",
    "            lr_scheduler = None\n",
    "\n",
    "        # --- Automatic Mixed Precision (AMP) Scaler ---\n",
    "        if config.use_amp:\n",
    "            scaler = torch.amp.GradScaler('cuda') # <<< CORRECTED\n",
    "            print(\"AMP GradScaler initialized.\")\n",
    "        else:\n",
    "            scaler = None\n",
    "\n",
    "        # Early stopping setup\n",
    "        early_stopping_counter = 0\n",
    "        best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "        print(f\"Early stopping enabled with patience: {config.early_stopping_patience}\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not initialized or train_loader not available. Skipping optimizer/scheduler setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step (contrastive) and validation epoch functions defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 11: Training and Validation Functions (Using Contrastive Loss) ===\n",
    "import traceback\n",
    "\n",
    "def train_step(model, batch, optimizer, scaler, device, use_amp):\n",
    "    \"\"\" Performs a single training step with CONTRASTIVE loss and optional AMP \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    pixel_values = batch['pixel_values']\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        logits_per_image, logits_per_text, _, _ = model(pixel_values, input_ids, attention_mask)\n",
    "        loss = contrastive_loss(logits_per_image, logits_per_text) # <<< USE CONTRASTIVE LOSS\n",
    "\n",
    "    if use_amp:\n",
    "        scaler.scale(loss).backward()\n",
    "    else:\n",
    "        loss.backward()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def validate_epoch(model, dataloader, device):\n",
    "    \"\"\" Performs validation, returning metrics \"\"\"\n",
    "    model.eval()\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Validation\", leave=False, unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values']\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=config.use_amp):\n",
    "                logits_per_image, logits_per_text, image_embeds_norm, text_embeds_norm = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "            all_image_embeddings.append(image_embeds_norm.cpu())\n",
    "            all_text_embeddings.append(text_embeds_norm.cpu())\n",
    "\n",
    "    if not all_image_embeddings or not all_text_embeddings:\n",
    "         print(\"Warning: No embeddings collected during validation.\")\n",
    "         return { \"loss\": float('inf'), \"avg_acc\": 0.0, \"avg_cosine_sim\": 0.0,\n",
    "                  \"i2t recall R@1\": 0.0, \"i2t recall R@5\": 0.0, \"i2t recall R@10\": 0.0,\n",
    "                  \"t2i recall R@1\": 0.0, \"t2i recall R@5\": 0.0, \"t2i recall R@10\": 0.0 }\n",
    "\n",
    "    try:\n",
    "        all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "        all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error concatenating embeddings: {e}\")\n",
    "        return { \"loss\": float('inf'), \"avg_acc\": 0.0, \"avg_cosine_sim\": 0.0,\n",
    "                 \"i2t recall R@1\": 0.0, \"i2t recall R@5\": 0.0, \"i2t recall R@10\": 0.0,\n",
    "                 \"t2i recall R@1\": 0.0, \"t2i recall R@5\": 0.0, \"t2i recall R@10\": 0.0 }\n",
    "\n",
    "    # --- Log Temp/Bias (if learnable) ---\n",
    "    current_temp_val = model.logit_scale.exp().item() if isinstance(model.logit_scale, nn.Parameter) else (1 / config.temperature)\n",
    "    print(f\"DEBUG: Validation - Current Temp (exp(logit_scale)): {current_temp_val:.4f}\")\n",
    "    # No bias term in this model version\n",
    "\n",
    "    print(f\"\\\\nComputing metrics over {all_image_embeddings.shape[0]} validation samples...\")\n",
    "    validation_metrics = compute_metrics(all_image_embeddings.to(device), all_text_embeddings.to(device))\n",
    "\n",
    "    # Format results\n",
    "    final_results = {}\n",
    "    for k, v in validation_metrics.items():\n",
    "        if isinstance(v, dict):\n",
    "            for recall_k, recall_v in v.items(): final_results[f\"{k.replace('_', ' ')} {recall_k}\"] = recall_v\n",
    "        else: final_results[k.replace('_', ' ')] = v\n",
    "\n",
    "    del all_image_embeddings, all_text_embeddings\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "    return final_results\n",
    "\n",
    "print(\"Training step (contrastive) and validation epoch functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nStarting ViPhobertSiglip fine-tuning for 200 epochs...\n",
      "Target metric for saving best model: 'avg_acc' (mode: max)\n",
      "\\n--- Epoch 1/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1927be78424c718a979d01768b8413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E1:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_405537/3834100572.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1 Time: 0:01:07.596302 ---\n",
      "--- Average Train Loss for Epoch 1: 2.2512 ---\n",
      "\\n--- Epoch 2/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb9b4394a69497a84d7992c596a7264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E2:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 2 Time: 0:01:07.943021 ---\n",
      "--- Average Train Loss for Epoch 2: 0.9525 ---\n",
      "\\n--- Epoch 3/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb74b14bc02e4bc58a4109844fb93eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E3:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 3 Time: 0:00:58.720722 ---\n",
      "--- Average Train Loss for Epoch 3: 0.6302 ---\n",
      "\\n--- Epoch 4/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0adb569f8244cb8875f99c2e4ef67a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E4:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 4 Time: 0:00:54.952016 ---\n",
      "--- Average Train Loss for Epoch 4: 0.4611 ---\n",
      "\\n--- Epoch 5/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d24b6a37844aecbb984fa0f11c4743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E5:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 5 Time: 0:01:29.212774 ---\n",
      "--- Average Train Loss for Epoch 5: 0.3545 ---\n",
      "\\n--- Epoch 6/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e5352b0aa440e99870b2ca030cc534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E6:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 6 Time: 0:01:04.412107 ---\n",
      "--- Average Train Loss for Epoch 6: 0.2746 ---\n",
      "\\n--- Epoch 7/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad74782ba8d04a1f92729ba885dec479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E7:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 7 Time: 0:00:47.626928 ---\n",
      "--- Average Train Loss for Epoch 7: 0.2262 ---\n",
      "\\n--- Epoch 8/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ad6f4e009e496c845d560735b17245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E8:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 8 Time: 0:01:03.356893 ---\n",
      "--- Average Train Loss for Epoch 8: 0.1837 ---\n",
      "\\n--- Epoch 9/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1b6038c7164b8ea598db288190c77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E9:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 1000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be033bdcb6334396b7e9cb93cf50a5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_405537/3834100572.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=config.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 15.5192\n",
      "\\nComputing metrics over 1765 validation samples...\n",
      "Validation finished in 18.58s\n",
      "Validation Step 1000: avg acc: 0.3306 | avg cosine sim: 0.6169 | i2t acc: 0.3467 | i2t recall R@1: 0.3467 | i2t recall R@10: 0.8459 | i2t recall R@5: 0.7575 | t2i acc: 0.3144 | t2i recall R@1: 0.3144 | t2i recall R@10: 0.8391 | t2i recall R@5: 0.7433\n",
      "  RoP Scheduler step called with avg_acc=0.3306\n",
      "  Metric 'avg_acc' improved from -inf to 0.3306. Saving best model.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_vivqa/phobert_siglip_step_1000.pt\n",
      "--- Epoch 9 Time: 0:01:34.258878 ---\n",
      "--- Average Train Loss for Epoch 9: 0.1531 ---\n",
      "\\n--- Epoch 10/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d98d8c317d140b5acdb585f036fe2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E10:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 10 Time: 0:00:51.307949 ---\n",
      "--- Average Train Loss for Epoch 10: 0.1336 ---\n",
      "\\n--- Epoch 11/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f7125b31ec470db3695c99083e2680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E11:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 11 Time: 0:01:06.673449 ---\n",
      "--- Average Train Loss for Epoch 11: 0.1177 ---\n",
      "\\n--- Epoch 12/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4791426362f4a32b05c6e6719ca185a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E12:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 12 Time: 0:01:04.771074 ---\n",
      "--- Average Train Loss for Epoch 12: 0.1019 ---\n",
      "\\n--- Epoch 13/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad497b7183534393b376adad76620537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E13:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 13 Time: 0:01:07.051095 ---\n",
      "--- Average Train Loss for Epoch 13: 0.0944 ---\n",
      "\\n--- Epoch 14/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a057a77be7fc41b98d5c9a95de4320ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E14:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 14 Time: 0:01:06.727667 ---\n",
      "--- Average Train Loss for Epoch 14: 0.0848 ---\n",
      "\\n--- Epoch 15/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d972cc6ee7f4480dabdbc2ebbf78160d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E15:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 15 Time: 0:00:57.051508 ---\n",
      "--- Average Train Loss for Epoch 15: 0.0805 ---\n",
      "\\n--- Epoch 16/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6190a012f2bb472fa8f05763e652c923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E16:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 16 Time: 0:01:00.741451 ---\n",
      "--- Average Train Loss for Epoch 16: 0.0750 ---\n",
      "\\n--- Epoch 17/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27224ec112645998e3f9c0075adbee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E17:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 2000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d312d3acde47b48d014fdfc8c84aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 16.4830\n",
      "\\nComputing metrics over 1765 validation samples...\n",
      "Validation finished in 19.64s\n",
      "Validation Step 2000: avg acc: 0.3385 | avg cosine sim: 0.6166 | i2t acc: 0.3564 | i2t recall R@1: 0.3564 | i2t recall R@10: 0.8555 | i2t recall R@5: 0.7739 | t2i acc: 0.3207 | t2i recall R@1: 0.3207 | t2i recall R@10: 0.8521 | t2i recall R@5: 0.7558\n",
      "  RoP Scheduler step called with avg_acc=0.3385\n",
      "  Metric 'avg_acc' improved from 0.3306 to 0.3385. Saving best model.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_vivqa/phobert_siglip_step_2000.pt\n",
      "--- Epoch 17 Time: 0:01:38.034996 ---\n",
      "--- Average Train Loss for Epoch 17: 0.0660 ---\n",
      "\\n--- Epoch 18/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d9d53163b840e9ad98176cc4893a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E18:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 18 Time: 0:01:04.921036 ---\n",
      "--- Average Train Loss for Epoch 18: 0.0624 ---\n",
      "\\n--- Epoch 19/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b3b7f6f7464b59b0e42992a2224c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E19:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 19 Time: 0:01:04.838239 ---\n",
      "--- Average Train Loss for Epoch 19: 0.0588 ---\n",
      "\\n--- Epoch 20/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efba594d8224403aa8bd963d2807265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E20:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 20 Time: 0:00:50.599396 ---\n",
      "--- Average Train Loss for Epoch 20: 0.0580 ---\n",
      "\\n--- Epoch 21/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0899be3d4cfc454a8a1915b423278882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E21:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 21 Time: 0:01:06.027035 ---\n",
      "--- Average Train Loss for Epoch 21: 0.0535 ---\n",
      "\\n--- Epoch 22/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0a5bee64364642b74552bfe3f10412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E22:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 22 Time: 0:01:03.591650 ---\n",
      "--- Average Train Loss for Epoch 22: 0.0541 ---\n",
      "\\n--- Epoch 23/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e35750a7d5f40cb8e3d12484168570c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E23:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 23 Time: 0:01:05.577090 ---\n",
      "--- Average Train Loss for Epoch 23: 0.0473 ---\n",
      "\\n--- Epoch 24/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3825cdafb1ea4c63bc66b783e54872f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E24:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 24 Time: 0:01:07.201355 ---\n",
      "--- Average Train Loss for Epoch 24: 0.0463 ---\n",
      "\\n--- Epoch 25/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5461784c2f0847e78502db665b342227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E25:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 25 Time: 0:01:02.877349 ---\n",
      "--- Average Train Loss for Epoch 25: 0.0415 ---\n",
      "\\n--- Epoch 26/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b369f0e4b74ae7a51fadff6a96a460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E26:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 3000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a3d64554e24e7da67878ef563503ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 17.3767\n",
      "\\nComputing metrics over 1765 validation samples...\n",
      "Validation finished in 14.18s\n",
      "Validation Step 3000: avg acc: 0.3547 | avg cosine sim: 0.6106 | i2t acc: 0.3745 | i2t recall R@1: 0.3745 | i2t recall R@10: 0.8567 | i2t recall R@5: 0.7768 | t2i acc: 0.3348 | t2i recall R@1: 0.3348 | t2i recall R@10: 0.8527 | t2i recall R@5: 0.7671\n",
      "  RoP Scheduler step called with avg_acc=0.3547\n",
      "  Metric 'avg_acc' improved from 0.3385 to 0.3547. Saving best model.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_vivqa/phobert_siglip_step_3000.pt\n",
      "--- Epoch 26 Time: 0:01:23.101860 ---\n",
      "--- Average Train Loss for Epoch 26: 0.0425 ---\n",
      "\\n--- Epoch 27/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921896cf6a7f466d8b7bb191c0de9926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E27:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 27 Time: 0:01:04.672355 ---\n",
      "--- Average Train Loss for Epoch 27: 0.0377 ---\n",
      "\\n--- Epoch 28/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ade0237f454b018349e93d691f79ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E28:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 28 Time: 0:01:04.216963 ---\n",
      "--- Average Train Loss for Epoch 28: 0.0382 ---\n",
      "\\n--- Epoch 29/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49dde2bf2d014810bac7c406d2890faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E29:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 29 Time: 0:01:05.285393 ---\n",
      "--- Average Train Loss for Epoch 29: 0.0362 ---\n",
      "\\n--- Epoch 30/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778d99d8d91d40a785e0ba9a6321a3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E30:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 30 Time: 0:00:57.957997 ---\n",
      "--- Average Train Loss for Epoch 30: 0.0373 ---\n",
      "\\n--- Epoch 31/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae261ff29ce0412aba160b6d14801432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E31:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 31 Time: 0:00:55.802921 ---\n",
      "--- Average Train Loss for Epoch 31: 0.0351 ---\n",
      "\\n--- Epoch 32/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bee6ffe11444e7b803e7fca3c029dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E32:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 32 Time: 0:01:08.256536 ---\n",
      "--- Average Train Loss for Epoch 32: 0.0367 ---\n",
      "\\n--- Epoch 33/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e558bfa291244a92ba64d0cec2840d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E33:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 33 Time: 0:01:08.205912 ---\n",
      "--- Average Train Loss for Epoch 33: 0.0344 ---\n",
      "\\n--- Epoch 34/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293e0b06d3ca4d31a5cc8493bbe8b4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E34:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 4000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636bee279b6a440ca4dd0b006b93ae1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 18.3195\n",
      "\\nComputing metrics over 1765 validation samples...\n",
      "Validation finished in 17.82s\n",
      "Validation Step 4000: avg acc: 0.3501 | avg cosine sim: 0.6048 | i2t acc: 0.3598 | i2t recall R@1: 0.3598 | i2t recall R@10: 0.8521 | i2t recall R@5: 0.7756 | t2i acc: 0.3405 | t2i recall R@1: 0.3405 | t2i recall R@10: 0.8476 | t2i recall R@5: 0.7632\n",
      "  RoP Scheduler step called with avg_acc=0.3501\n",
      "  Metric 'avg_acc' did not improve. Best: 0.3547. Counter: 1/5\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_vivqa/phobert_siglip_step_4000.pt\n",
      "--- Epoch 34 Time: 0:01:29.274792 ---\n",
      "--- Average Train Loss for Epoch 34: 0.0361 ---\n",
      "\\n--- Epoch 35/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a46e9c7c3e54ba987fce6ed7a2b8529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E35:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 35 Time: 0:00:59.972449 ---\n",
      "--- Average Train Loss for Epoch 35: 0.0314 ---\n",
      "\\n--- Epoch 36/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941f8bb705eb4eb9a6f1e65b297ead74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E36:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 36 Time: 0:00:55.200486 ---\n",
      "--- Average Train Loss for Epoch 36: 0.0308 ---\n",
      "\\n--- Epoch 37/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382e955ea74d4d659964dbde58b3faa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E37:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 37 Time: 0:01:08.481722 ---\n",
      "--- Average Train Loss for Epoch 37: 0.0305 ---\n",
      "\\n--- Epoch 38/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272fa0fdd5c540e49318dd0ff54fc33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E38:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 38 Time: 0:01:06.900180 ---\n",
      "--- Average Train Loss for Epoch 38: 0.0293 ---\n",
      "\\n--- Epoch 39/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f6ac7e32474eb78e75ca8b9f80395c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E39:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 39 Time: 0:01:07.018844 ---\n",
      "--- Average Train Loss for Epoch 39: 0.0283 ---\n",
      "\\n--- Epoch 40/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15b7610b9e44a3e9ebb4b575d1d61a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E40:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 40 Time: 0:01:32.682296 ---\n",
      "--- Average Train Loss for Epoch 40: 0.0319 ---\n",
      "\\n--- Epoch 41/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de58cb6d843e43dba20a30a8bfdbf5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E41:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 41 Time: 0:00:53.010496 ---\n",
      "--- Average Train Loss for Epoch 41: 0.0298 ---\n",
      "\\n--- Epoch 42/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9956f08617f34786bcbf23153b879a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E42:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 42 Time: 0:00:57.351197 ---\n",
      "--- Average Train Loss for Epoch 42: 0.0286 ---\n",
      "\\n--- Epoch 43/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546ec5ea61974ac883b2c0f2c08df31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E43:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 5000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d5721b2ac042bbb750c11c200ed06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 19.3626\n",
      "\\nComputing metrics over 1765 validation samples...\n",
      "Validation finished in 20.81s\n",
      "Validation Step 5000: avg acc: 0.3476 | avg cosine sim: 0.5901 | i2t acc: 0.3598 | i2t recall R@1: 0.3598 | i2t recall R@10: 0.8414 | i2t recall R@5: 0.7558 | t2i acc: 0.3354 | t2i recall R@1: 0.3354 | t2i recall R@10: 0.8329 | t2i recall R@5: 0.7581\n",
      "  RoP Scheduler step called with avg_acc=0.3476\n",
      "  Metric 'avg_acc' did not improve. Best: 0.3547. Counter: 2/5\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_vivqa/phobert_siglip_step_5000.pt\n",
      "--- Epoch 43 Time: 0:01:31.644739 ---\n",
      "--- Average Train Loss for Epoch 43: 0.0282 ---\n",
      "\\n--- Epoch 44/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac03026be11641c0a93ef7a0e1939abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E44:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 44 Time: 0:01:07.718989 ---\n",
      "--- Average Train Loss for Epoch 44: 0.0268 ---\n",
      "\\n--- Epoch 45/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271c2e921b2b41b39e31ed91373718ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E45:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 45 Time: 0:01:04.703442 ---\n",
      "--- Average Train Loss for Epoch 45: 0.0252 ---\n",
      "\\n--- Epoch 46/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2845abe837244c8b5767dde96a75e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E46:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 46 Time: 0:01:04.241142 ---\n",
      "--- Average Train Loss for Epoch 46: 0.0261 ---\n",
      "\\n--- Epoch 47/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5dee1e482b4e339fcfef8ce9f8c161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E47:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 47 Time: 0:00:51.366713 ---\n",
      "--- Average Train Loss for Epoch 47: 0.0233 ---\n",
      "\\n--- Epoch 48/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74916fdaf40846f0b28d5e924b595b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E48:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 48 Time: 0:01:07.132859 ---\n",
      "--- Average Train Loss for Epoch 48: 0.0242 ---\n",
      "\\n--- Epoch 49/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782d1c0912ff4aa88da10993d99fd70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E49:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 49 Time: 0:01:07.467238 ---\n",
      "--- Average Train Loss for Epoch 49: 0.0257 ---\n",
      "\\n--- Epoch 50/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90de5c928ef948bc86baf0c74439f9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E50:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 50 Time: 0:01:04.550452 ---\n",
      "--- Average Train Loss for Epoch 50: 0.0256 ---\n",
      "\\n--- Epoch 51/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75d8daf219a466a9fceec144caa7b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E51:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 6000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3110e930c24e6eb2c2b6471f176dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 20.5307\n",
      "\\nComputing metrics over 1765 validation samples...\n",
      "Validation finished in 15.40s\n",
      "Validation Step 6000: avg acc: 0.3450 | avg cosine sim: 0.5822 | i2t acc: 0.3581 | i2t recall R@1: 0.3581 | i2t recall R@10: 0.8215 | i2t recall R@5: 0.7484 | t2i acc: 0.3320 | t2i recall R@1: 0.3320 | t2i recall R@10: 0.8323 | t2i recall R@5: 0.7688\n",
      "  RoP Scheduler step called with avg_acc=0.3450\n",
      "  Metric 'avg_acc' did not improve. Best: 0.3547. Counter: 3/5\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_vivqa/phobert_siglip_step_6000.pt\n",
      "--- Epoch 51 Time: 0:01:28.029712 ---\n",
      "--- Average Train Loss for Epoch 51: 0.0243 ---\n",
      "\\n--- Epoch 52/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d7548627974aa088c3f2a849822ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E52:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 52 Time: 0:00:51.009235 ---\n",
      "--- Average Train Loss for Epoch 52: 0.0221 ---\n",
      "\\n--- Epoch 53/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b324b2d672e34208a4a22026ed749e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E53:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 53 Time: 0:01:05.943298 ---\n",
      "--- Average Train Loss for Epoch 53: 0.0194 ---\n",
      "\\n--- Epoch 54/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d889f9cde4945248d4f073f16aef319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E54:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 54 Time: 0:01:07.648335 ---\n",
      "--- Average Train Loss for Epoch 54: 0.0252 ---\n",
      "\\n--- Epoch 55/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b395ab6d498640daa66dff8701fafe67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E55:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 55 Time: 0:01:04.953791 ---\n",
      "--- Average Train Loss for Epoch 55: 0.0207 ---\n",
      "\\n--- Epoch 56/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eac15e9d36543b7b2d7413e8d848061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E56:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 56 Time: 0:01:04.111459 ---\n",
      "--- Average Train Loss for Epoch 56: 0.0186 ---\n",
      "\\n--- Epoch 57/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07822c60d67347bc96ec054fab892c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E57:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 57 Time: 0:01:03.334628 ---\n",
      "--- Average Train Loss for Epoch 57: 0.0177 ---\n",
      "\\n--- Epoch 58/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9bf826023b247feb1a872f114119b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E58:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 58 Time: 0:00:54.042237 ---\n",
      "--- Average Train Loss for Epoch 58: 0.0158 ---\n",
      "\\n--- Epoch 59/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9912fbef4b2e45a38f2ce2214f1cc8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E59:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 59 Time: 0:01:05.696449 ---\n",
      "--- Average Train Loss for Epoch 59: 0.0165 ---\n",
      "\\n--- Epoch 60/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a99c1798e243a29070ad1303f8dac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E60:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 7000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93921eee8b4c484cb59bba5f3bca237e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 21.4350\n",
      "\\nComputing metrics over 1765 validation samples...\n",
      "Validation finished in 21.59s\n",
      "Validation Step 7000: avg acc: 0.3456 | avg cosine sim: 0.5818 | i2t acc: 0.3581 | i2t recall R@1: 0.3581 | i2t recall R@10: 0.8453 | i2t recall R@5: 0.7615 | t2i acc: 0.3331 | t2i recall R@1: 0.3331 | t2i recall R@10: 0.8482 | t2i recall R@5: 0.7637\n",
      "  RoP Scheduler step called with avg_acc=0.3456\n",
      "  Metric 'avg_acc' did not improve. Best: 0.3547. Counter: 4/5\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_vivqa/phobert_siglip_step_7000.pt\n",
      "--- Epoch 60 Time: 0:01:28.609268 ---\n",
      "--- Average Train Loss for Epoch 60: 0.0192 ---\n",
      "\\n--- Epoch 61/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6c2e713f0744fc805f14e372ed2e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E61:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 61 Time: 0:01:03.026478 ---\n",
      "--- Average Train Loss for Epoch 61: 0.0170 ---\n",
      "\\n--- Epoch 62/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c549ec01a20491c83ba09a0d0981ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E62:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 62 Time: 0:01:02.838881 ---\n",
      "--- Average Train Loss for Epoch 62: 0.0182 ---\n",
      "\\n--- Epoch 63/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a469a25dfbfa4b0d8570b1c23bfb1506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E63:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 63 Time: 0:00:44.080429 ---\n",
      "--- Average Train Loss for Epoch 63: 0.0205 ---\n",
      "\\n--- Epoch 64/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f334f4a5c44ac4bc2afd8d442d49ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E64:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 64 Time: 0:01:07.757416 ---\n",
      "--- Average Train Loss for Epoch 64: 0.0198 ---\n",
      "\\n--- Epoch 65/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5614de15cb438bba9702bca9f5ea4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E65:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 65 Time: 0:01:06.312659 ---\n",
      "--- Average Train Loss for Epoch 65: 0.0179 ---\n",
      "\\n--- Epoch 66/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2017acd90f4dd2b756103295e3af75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E66:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 66 Time: 0:01:07.038459 ---\n",
      "--- Average Train Loss for Epoch 66: 0.0177 ---\n",
      "\\n--- Epoch 67/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d316cb77f25f43009917cff7e440195e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E67:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 67 Time: 0:01:04.832716 ---\n",
      "--- Average Train Loss for Epoch 67: 0.0192 ---\n",
      "\\n--- Epoch 68/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0d892e82ac40bfbdb7acd6aacb1465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E68:   0%|          | 0/118 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 8000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e0b5e3b4e24710bf70887c41dc5ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 22.5354\n",
      "\\nComputing metrics over 1765 validation samples...\n",
      "Validation finished in 7.25s\n",
      "Validation Step 8000: avg acc: 0.3453 | avg cosine sim: 0.5779 | i2t acc: 0.3581 | i2t recall R@1: 0.3581 | i2t recall R@10: 0.8448 | i2t recall R@5: 0.7649 | t2i acc: 0.3326 | t2i recall R@1: 0.3326 | t2i recall R@10: 0.8504 | t2i recall R@5: 0.7637\n",
      "  RoP Scheduler step called with avg_acc=0.3453\n",
      "  Metric 'avg_acc' did not improve. Best: 0.3547. Counter: 5/5\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_vivqa/phobert_siglip_step_8000.pt\n",
      "\\nEarly stopping triggered after 5 validation checks without improvement.\n",
      "--- Epoch 68 Time: 0:00:58.369544 ---\n",
      "--- Average Train Loss for Epoch 68: 0.0181 ---\n",
      "=============== Fine-tuning Finished ================\n",
      "Total Training Time: 1:14:49.932177\n",
      "Final model state saved to ./trained_models/ViSigLIP_vivqa/phobert_siglip_final.pt\n",
      "Best model based on 'avg_acc' (0.3547) is saved at: ./trained_models/ViSigLIP_vivqa/phobert_siglip_best.pt\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "# === Cell 12: Fine-tuning Loop ===\n",
    "import datetime\n",
    "\n",
    "if model and train_loader and optimizer:  # Basic check\n",
    "    print(f\"\\\\nStarting ViPhobertSiglip fine-tuning for {config.epochs} epochs...\") # Updated print\n",
    "    print(f\"Target metric for saving best model: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    global_step = 0\n",
    "    total_loss_since_log = 0.0\n",
    "    steps_since_log = 0\n",
    "    start_train_time = time.time()\n",
    "    early_stopping_counter = 0 # Initialize here\n",
    "\n",
    "    history = {'steps': [], 'train_loss': [], 'val_metrics': {}} # Use steps for logging x-axis\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Training E{epoch+1}\", leave=True, unit=\"batch\")\n",
    "        epoch_loss_meter = AvgMeter(f\"Train Loss E{epoch+1}\") # Track epoch average loss\n",
    "\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            # Skip dummy batches if any errors occurred during data loading\n",
    "            if batch['pixel_values'].shape[0] < config.batch_size and torch.all(batch['pixel_values'] == 0):\n",
    "                continue\n",
    "\n",
    "            # --- Training Step ---\n",
    "            loss = train_step(model, batch, optimizer, scaler, config.device, config.use_amp)\n",
    "            epoch_loss_meter.update(loss, batch['pixel_values'].shape[0]) # Update epoch meter\n",
    "\n",
    "            # Accumulate loss for logging interval\n",
    "            loss_normalized_for_log = loss / config.accumulation_steps\n",
    "            total_loss_since_log += loss_normalized_for_log\n",
    "            steps_since_log += 1\n",
    "\n",
    "            # --- Gradient Accumulation & Optimizer Step ---\n",
    "            is_update_step = (global_step + 1) % config.accumulation_steps == 0\n",
    "            if is_update_step:\n",
    "                # Unscale gradients before clipping (if needed) and optimizer step\n",
    "                if config.use_amp:\n",
    "                    scaler.unscale_(optimizer) # Unscales the gradients of optimizer's assigned params in-place\n",
    "                    # Optional: Gradient Clippinglearnable_bias\n",
    "                    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    # Optional: Gradient Clipping\n",
    "                    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # --- LR Scheduler Step (per optimizer step for Cosine, skipped for RoP here) ---\n",
    "                if lr_scheduler and config.scheduler_type == \"cosine\":\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "            global_step += 1 # Increment global step after processing a batch\n",
    "\n",
    "            # --- Logging ---\n",
    "            if global_step % config.log_interval_steps == 0 and steps_since_log > 0:\n",
    "                avg_loss = total_loss_since_log / steps_since_log\n",
    "                current_lr = optimizer.param_groups[0]['lr'] # Get first group's LR for logging\n",
    "                progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\", lr=f\"{current_lr:.2e}\", step=f\"{global_step}\")\n",
    "                history['steps'].append(global_step)\n",
    "                history['train_loss'].append(avg_loss)\n",
    "                total_loss_since_log = 0.0\n",
    "                steps_since_log = 0\n",
    "\n",
    "            # --- Validation & Checkpointing (Based on Steps) ---\n",
    "            if dev_loader and global_step % config.validation_interval_steps == 0 and global_step > 0:\n",
    "                print(f\"\\\\nRunning validation at step {global_step}...\")\n",
    "                val_start_time = time.time()\n",
    "                val_results = validate_epoch(model, dev_loader, config.device)\n",
    "                val_end_time = time.time()\n",
    "                print(f\"Validation finished in {val_end_time - val_start_time:.2f}s\")\n",
    "\n",
    "                metric_log_str = f\"  Validation Step {global_step}: \"\n",
    "                history['val_metrics'][global_step] = val_results\n",
    "                sorted_keys = sorted(val_results.keys())\n",
    "                for name in sorted_keys:\n",
    "                    metric_log_str += f\"{name}: {val_results[name]:.4f} | \"\n",
    "                print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "                # --- Scheduler Step (ReduceLROnPlateau) ---\n",
    "                current_val_metric_for_scheduler = val_results.get(config.metric_to_track.replace('_', ' '), None)\n",
    "                if lr_scheduler and config.scheduler_type == \"reduce_on_plateau\":\n",
    "                     if current_val_metric_for_scheduler is not None:\n",
    "                         lr_scheduler.step(current_val_metric_for_scheduler)\n",
    "                         print(f\"  RoP Scheduler step called with {config.metric_to_track}={current_val_metric_for_scheduler:.4f}\")\n",
    "                     else:\n",
    "                         print(f\"  Warning: Metric '{config.metric_to_track}' not found. RoP Scheduler not stepped.\")\n",
    "\n",
    "                # --- Save Checkpoint Logic ---\n",
    "                current_val_metric = val_results.get(config.metric_to_track.replace('_', ' '), None)\n",
    "                is_best = False\n",
    "                save_path = None\n",
    "                save_path_periodic = None\n",
    "\n",
    "                if current_val_metric is not None:\n",
    "                    improvement_threshold = best_val_metric + config.early_stopping_min_delta if config.mode == \"max\" else best_val_metric - config.early_stopping_min_delta\n",
    "                    if config.mode == \"max\": is_best = current_val_metric > improvement_threshold\n",
    "                    else: is_best = current_val_metric < improvement_threshold\n",
    "\n",
    "                    if is_best:\n",
    "                        print(f\"  Metric '{config.metric_to_track}' improved from {best_val_metric:.4f} to {current_val_metric:.4f}. Saving best model.\")\n",
    "                        best_val_metric = current_val_metric\n",
    "                        early_stopping_counter = 0 # Reset counter\n",
    "                        # --- Use new checkpoint name ---\n",
    "                        save_path = os.path.join(config.model_path, \"phobert_siglip_best.pt\")\n",
    "                    else:\n",
    "                        early_stopping_counter += 1\n",
    "                        print(f\"  Metric '{config.metric_to_track}' did not improve. Best: {best_val_metric:.4f}. Counter: {early_stopping_counter}/{config.early_stopping_patience}\")\n",
    "\n",
    "                    # Save periodic checkpoint\n",
    "                    if global_step % config.save_interval_steps == 0:\n",
    "                        # --- Use new checkpoint name ---\n",
    "                        periodic_save_path = os.path.join(config.model_path, f\"phobert_siglip_step_{global_step}.pt\")\n",
    "                        if save_path != periodic_save_path:\n",
    "                            print(f\"  Saving periodic checkpoint to {periodic_save_path}\")\n",
    "                            save_path_periodic = periodic_save_path\n",
    "\n",
    "                    # Prepare Save Dictionary & Save\n",
    "                    if save_path or save_path_periodic:\n",
    "                        save_dict = {\n",
    "                            'step': global_step, 'epoch': epoch + 1,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_metric': best_val_metric,\n",
    "                            'metric_tracked': config.metric_to_track,\n",
    "                            'current_val_metrics': val_results,\n",
    "                            # Save relevant configs\n",
    "                            'vision_model_name': config.vision_model_name, # Use specific names\n",
    "                            'text_model_name': config.text_model_name,\n",
    "                            'projection_dim': config.projection_dim,\n",
    "                            'learnable_temperature': config.learnable_temperature,\n",
    "                            'temperature': config.temperature, # Save base temp\n",
    "                            'max_length': config.max_length,\n",
    "                        }\n",
    "                        if lr_scheduler: save_dict['scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "                        if scaler: save_dict['scaler_state_dict'] = scaler.state_dict()\n",
    "                        if save_path: torch.save(save_dict, save_path)\n",
    "                        if save_path_periodic: torch.save(save_dict, save_path_periodic)\n",
    "                else:\n",
    "                    print(f\"  Warning: Metric '{config.metric_to_track}' not found. Cannot save best or check early stopping.\")\n",
    "                    early_stopping_counter += 1 # Still count as no improvement\n",
    "\n",
    "                # --- Early Stopping Check ---\n",
    "                if early_stopping_counter >= config.early_stopping_patience:\n",
    "                    print(f\"\\\\nEarly stopping triggered after {early_stopping_counter} validation checks without improvement.\")\n",
    "                    break # Break INNER loop\n",
    "\n",
    "                model.train() # Reset model to train mode\n",
    "\n",
    "        # --- End of Epoch ---\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {datetime.timedelta(seconds=epoch_end_time - epoch_start_time)} ---\")\n",
    "        print(f\"--- Average Train Loss for Epoch {epoch+1}: {epoch_loss_meter.avg:.4f} ---\") # Log epoch loss\n",
    "\n",
    "        # Break OUTER loop if early stopping was triggered\n",
    "        if early_stopping_counter >= config.early_stopping_patience:\n",
    "           break\n",
    "\n",
    "    # --- End of Training ---\n",
    "    end_train_time = time.time()\n",
    "    total_duration = datetime.timedelta(seconds=end_train_time - start_train_time)\n",
    "    print(f\"=============== Fine-tuning Finished ================\") # Updated print\n",
    "    print(f\"Total Training Time: {total_duration}\")\n",
    "\n",
    "    # Save final model state\n",
    "    # --- Use new checkpoint name ---\n",
    "    final_model_path = os.path.join(config.model_path, 'phobert_siglip_final.pt')\n",
    "    final_save_dict = {\n",
    "        'step': global_step, 'epoch': epoch + 1, # Save last completed epoch\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_metric': best_val_metric,\n",
    "        'metric_tracked': config.metric_to_track,\n",
    "        'vision_model_name': config.vision_model_name,\n",
    "        'text_model_name': config.text_model_name,\n",
    "        'projection_dim': config.projection_dim,\n",
    "        'learnable_temperature': config.learnable_temperature,\n",
    "        'temperature': config.temperature,\n",
    "        'max_length': config.max_length,\n",
    "    }\n",
    "    if lr_scheduler: final_save_dict['scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "    if scaler: final_save_dict['scaler_state_dict'] = scaler.state_dict()\n",
    "    torch.save(final_save_dict, final_model_path)\n",
    "    print(f\"Final model state saved to {final_model_path}\")\n",
    "\n",
    "    # --- Use new checkpoint name ---\n",
    "    best_model_file = os.path.join(config.model_path, \"phobert_siglip_best.pt\")\n",
    "    if dev_loader and os.path.exists(best_model_file):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) is saved at: {best_model_file}\")\n",
    "    elif dev_loader: print(\"Best model checkpoint file not found (or validation was skipped/no improvement).\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training (model, dataloader, optimizer) not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=============== Starting Test Set Evaluation ===============\n",
      "Loading test data from: ./refined_json/test.json\n",
      "Loading JSON metadata...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92a83c765f745319dd5a8a2a536b5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading JSONs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1734 samples total from 1 file(s).\n",
      "Dataset size after potential cleaning: 1734\n",
      "Using image target size: 224x224\n",
      "Test loader created with 7 batches.\n",
      "\\nLoading best model: ./trained_models/ViSigLIP_vivqa/phobert_siglip_best.pt\n",
      "Re-creating model structure for testing...\n",
      "  Using Vision Source: google/siglip-base-patch16-224\n",
      "  Using Text Model: vinai/phobert-base\n",
      "Initializing SigLIP Vision Encoder from: google/siglip-base-patch16-224\n",
      "  SigLIP Vision model loaded successfully.\n",
      "  Confirmed/Using vision model hidden size: 768\n",
      "  Added projection head: 768 -> 768\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Confirmed text model hidden size: 768\n",
      "  Added projection head: 768 -> 768\n",
      "Using learnable temperature (logit_scale), initialized to 14.2857\n",
      "  State dict loading result: <All keys matched successfully>\n",
      "Model weights loaded successfully.\n",
      "\\nRunning evaluation on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63f64a8308744998678a11b8d44777b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_405537/3834100572.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=config.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 17.3767\n",
      "\\nComputing metrics over 1734 validation samples...\n",
      "\\n--- Test Set Results ---\n",
      "avg acc: 0.3313\\n  avg cosine sim: 0.5963\\n  i2t acc: 0.3431\\n  i2t recall R@1: 0.3431\\n  i2t recall R@10: 0.8345\\n  i2t recall R@5: 0.7416\\n  t2i acc: 0.3195\\n  t2i recall R@1: 0.3195\\n  t2i recall R@10: 0.8258\\n  t2i recall R@5: 0.7393\\n\n",
      "------------------------\n",
      "\\n================= Evaluation Finished ==================\n"
     ]
    }
   ],
   "source": [
    "# === Cell 13: Final Evaluation on Test Set (Updated for Phobert+Siglip) ===\n",
    "import traceback\n",
    "from types import SimpleNamespace\n",
    "from collections import OrderedDict\n",
    "\n",
    "print(\"\\\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_loader = None\n",
    "model_to_test = None\n",
    "\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "test_image_path = config.image_base_path\n",
    "\n",
    "# 1. Check prerequisites & Create Test Loader\n",
    "if os.path.exists(test_json_path) and 'tokenizer' in globals() and tokenizer and 'image_processor' in globals() and image_processor:\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    try:\n",
    "        test_dataset = CustomImageCaptionDataset(\n",
    "            json_path_or_list=test_json_path, image_base_path=test_image_path,\n",
    "            tokenizer=tokenizer, image_processor=image_processor,\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "        if test_dataset.data:\n",
    "            num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            persist_workers_test = (num_workers > 0)\n",
    "            try: _ = DataLoader(test_dataset, num_workers=num_workers, persistent_workers=persist_workers_test)\n",
    "            except TypeError: persist_workers_test = False\n",
    "\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=config.batch_size * 2, shuffle=False,\n",
    "                num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "                drop_last=False, persistent_workers=persist_workers_test\n",
    "            )\n",
    "            print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "        else: print(\"Test dataset loaded but is empty.\")\n",
    "    except Exception as e: print(f\"Error creating test dataset/loader: {e}\")\n",
    "else: print(\"Skipping test evaluation: Test JSON, Tokenizer or Image Processor not found/loaded.\")\n",
    "\n",
    "\n",
    "# 2. Load Model for Testing\n",
    "if test_loader:\n",
    "    try:\n",
    "        # --- Use updated checkpoint names ---\n",
    "        best_model_path = os.path.join(config.model_path, \"phobert_siglip_best.pt\")\n",
    "        final_model_path = os.path.join(config.model_path, \"phobert_siglip_final.pt\")\n",
    "        load_path = None\n",
    "\n",
    "        if os.path.exists(best_model_path):\n",
    "             load_path = best_model_path\n",
    "             print(f\"\\\\nLoading best model: {load_path}\")\n",
    "        elif os.path.exists(final_model_path):\n",
    "             load_path = final_model_path\n",
    "             print(f\"\\\\nLoading final model (best not found): {load_path}\")\n",
    "        else:\n",
    "            print(f\"\\\\nWARNING: No checkpoints found in {config.model_path} to evaluate.\")\n",
    "\n",
    "        if load_path:\n",
    "            checkpoint = torch.load(load_path, map_location=config.device)\n",
    "            print(\"Re-creating model structure for testing...\")\n",
    "\n",
    "            # --- Create temp config based on saved checkpoint ---\n",
    "            temp_config_dict = {\n",
    "                'device': config.device,\n",
    "                # Use model names saved in checkpoint\n",
    "                'vision_model_name': checkpoint.get('vision_model_name', config.selected_vision_source),\n",
    "                'text_model_name': checkpoint.get('text_model_name', config.selected_text_model),\n",
    "                # Use embedding sizes from current config (should match base models)\n",
    "                'vision_embedding': config.vision_embedding,\n",
    "                'text_embedding': config.text_embedding,\n",
    "                # Get these from checkpoint or current config\n",
    "                'projection_dim': checkpoint.get('projection_dim', config.projection_dim),\n",
    "                'learnable_temperature': checkpoint.get('learnable_temperature', config.learnable_temperature),\n",
    "                'temperature': checkpoint.get('temperature', config.temperature),\n",
    "                # Bias is not used/saved in this setup\n",
    "                'learnable_bias': False,\n",
    "                'bias_init': 0.0,\n",
    "            }\n",
    "            temp_config = SimpleNamespace(**temp_config_dict)\n",
    "\n",
    "            print(f\"  Using Vision Source: {temp_config.vision_model_name}\")\n",
    "            print(f\"  Using Text Model: {temp_config.text_model_name}\")\n",
    "\n",
    "            # --- Instantiate the CORRECT model class ---\n",
    "            test_image_encoder = ImageEncoder(temp_config).to(config.device)\n",
    "            test_text_encoder = TextEncoder(temp_config).to(config.device)\n",
    "            model_to_test = ViPhobertSiglipModel(test_image_encoder, test_text_encoder, temp_config).to(config.device)\n",
    "            # -----------------------------------------\n",
    "\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "                print(\"Detected 'module.' prefix, removing.\")\n",
    "                state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "\n",
    "            load_result = model_to_test.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"  State dict loading result: {load_result}\")\n",
    "            if load_result.missing_keys: print(f\"  Warning: Missing keys: {load_result.missing_keys}\")\n",
    "            if load_result.unexpected_keys: print(f\"  Warning: Unexpected keys: {load_result.unexpected_keys}\")\n",
    "            print(f\"Model weights loaded successfully.\")\n",
    "\n",
    "            print(\"\\\\nRunning evaluation on test set...\")\n",
    "            test_results = validate_epoch(model_to_test, test_loader, config.device)\n",
    "\n",
    "            print(\"\\\\n--- Test Set Results ---\")\n",
    "            metric_log_str = \"\"\n",
    "            sorted_keys = sorted(test_results.keys())\n",
    "            for name in sorted_keys: metric_log_str += f\"  {name}: {test_results[name]:.4f}\\\\n\"\n",
    "            print(metric_log_str.strip())\n",
    "            print(\"------------------------\")\n",
    "        else:\n",
    "            print(\"Evaluation skipped (no weights found).\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\nERROR during test setup/evaluation: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\\\n================= Evaluation Finished ==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot directory ensured at: /home/researcher/huypq69/TuningModels/train_plot/ViSigLIP_vivqa\n",
      "Saved training loss plot to: ./train_plot/ViSigLIP_vivqa/training_loss_step.png\n",
      "Saved validation metrics plot to: ./train_plot/ViSigLIP_vivqa/validation_metrics_step.png\n"
     ]
    }
   ],
   "source": [
    "# === Cell 14: Training Visualization (Adapted for Steps/Epochs) ===\n",
    "# This function plots based on epochs if validation runs per epoch,\n",
    "# or steps if validation runs based on steps.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math # Import math\n",
    "\n",
    "def plot_training_metrics(history, plot_dir, plot_by='epoch'):\n",
    "    \"\"\"Plots training and validation metrics.\"\"\"\n",
    "\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    print(f\"Plot directory ensured at: {os.path.abspath(plot_dir)}\")\n",
    "\n",
    "    if not history:\n",
    "        print(\"No history data provided.\")\n",
    "        return\n",
    "\n",
    "    # Determine x-axis based on available data and preference\n",
    "    if plot_by == 'step' and history.get('steps') and history.get('train_loss'):\n",
    "        x_axis_train = history['steps']\n",
    "        x_label = 'Global Steps'\n",
    "        x_axis_val = sorted(history.get('val_metrics', {}).keys()) if history.get('val_metrics') else []\n",
    "    elif history.get('train_loss') and history.get('validation_results'):\n",
    "         # Use epochs if validation results are stored per epoch\n",
    "         num_epochs = len(history['train_loss'])\n",
    "         x_axis_train = range(1, num_epochs + 1)\n",
    "         x_axis_val = range(1, len(history['validation_results']) + 1)\n",
    "         x_label = 'Epoch'\n",
    "         plot_by = 'epoch' # Force epoch plotting if step data is missing for val\n",
    "    else:\n",
    "        print(\"Insufficient history data (need train_loss and either steps or validation_results per epoch).\")\n",
    "        return\n",
    "\n",
    "    val_metrics_data = history.get('val_metrics', {}) if plot_by == 'step' else history.get('validation_results', [])\n",
    "\n",
    "    # --- Training Loss ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_axis_train, history['train_loss'], 'b-', label=f'Training Loss (Avg per Log Interval if steps)')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss over {x_label.capitalize()}')\n",
    "\n",
    "    # Plot validation loss if available (assuming it's stored in val_results/val_metrics)\n",
    "    if val_metrics_data:\n",
    "        try:\n",
    "            if plot_by == 'step':\n",
    "                val_loss = [val_metrics_data[step].get('loss', float('nan')) for step in x_axis_val]\n",
    "            else: # Plot by epoch\n",
    "                val_loss = [res.get('loss', float('nan')) for res in val_metrics_data if res]\n",
    "            if any(not math.isnan(vl) for vl in val_loss): # Only plot if loss was calculated and stored\n",
    "                 plt.plot(x_axis_val, val_loss, 'r-', label='Validation Loss')\n",
    "        except (KeyError, TypeError):\n",
    "             print(\"Validation loss not found or incorrectly formatted in history.\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    save_path_loss = os.path.join(plot_dir, f'training_loss_{plot_by}.png')\n",
    "    plt.savefig(save_path_loss, dpi=300)\n",
    "    print(f\"Saved training loss plot to: {save_path_loss}\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- Validation Metrics ---\n",
    "    if val_metrics_data and x_axis_val:\n",
    "        # Get metric names from the first valid entry\n",
    "        first_valid_val_result = next((res for res in (val_metrics_data.values() if plot_by == 'step' else val_metrics_data) if res and isinstance(res, dict)), None)\n",
    "        if first_valid_val_result:\n",
    "            metrics_to_plot = [k for k in first_valid_val_result.keys() if k != 'loss'] # Exclude loss\n",
    "\n",
    "            num_plots = len(metrics_to_plot)\n",
    "            if num_plots > 0:\n",
    "                ncols = 2\n",
    "                nrows = math.ceil(num_plots / ncols)\n",
    "                fig, axes = plt.subplots(nrows, ncols, figsize=(8 * ncols, 6 * nrows), squeeze=False)\n",
    "                axes = axes.flatten()\n",
    "\n",
    "                for i, metric_name in enumerate(metrics_to_plot):\n",
    "                    if plot_by == 'step':\n",
    "                        metric_values = [val_metrics_data[step].get(metric_name, float('nan')) for step in x_axis_val]\n",
    "                    else: # Plot by epoch\n",
    "                         metric_values = [res.get(metric_name, float('nan')) for res in val_metrics_data if res]\n",
    "\n",
    "                    if any(not math.isnan(v) for v in metric_values): # Check if metric has valid data\n",
    "                        axes[i].plot(x_axis_val, metric_values, 'r-o', label=f'Validation {metric_name}')\n",
    "                        axes[i].set_xlabel(x_label)\n",
    "                        axes[i].set_ylabel(metric_name.replace('_', ' ').capitalize())\n",
    "                        axes[i].set_title(f'Validation {metric_name} over {x_label.capitalize()}')\n",
    "                        axes[i].legend()\n",
    "                        axes[i].grid(True)\n",
    "                    else:\n",
    "                         axes[i].set_title(f'Validation {metric_name} (No Data)')\n",
    "                         axes[i].text(0.5, 0.5, 'No Data', ha='center', va='center')\n",
    "\n",
    "\n",
    "                for j in range(i + 1, len(axes)): fig.delaxes(axes[j]) # Hide unused subplots\n",
    "\n",
    "                fig.suptitle(f'Validation Metrics over {x_label.capitalize()}', fontsize=16, y=1.02)\n",
    "                plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "                save_path_val = os.path.join(plot_dir, f'validation_metrics_{plot_by}.png')\n",
    "                plt.savefig(save_path_val, dpi=300)\n",
    "                print(f\"Saved validation metrics plot to: {save_path_val}\")\n",
    "                plt.close()\n",
    "            else: print(\"No validation metrics (excluding loss) found to plot.\")\n",
    "        else: print(\"No valid validation results found.\")\n",
    "    else: print(\"No validation metrics found in history to plot.\")\n",
    "\n",
    "\n",
    "# --- Plotting ---\n",
    "# Decide whether to plot by 'step' or 'epoch' based on how validation was run\n",
    "plot_directory = \"./train_plot/ViSigLIP_vivqa\"\n",
    "plotting_mode = 'step' if config.validation_interval_steps > 0 else 'epoch'\n",
    "\n",
    "if 'history' in locals() and isinstance(history, dict):\n",
    "    plot_training_metrics(history, plot_directory, plot_by=plotting_mode)\n",
    "else:\n",
    "    print(\"No training history found. Run training first.\")\n",
    "\n",
    "# --- END OF SCRIPT ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
