{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Installs and Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time # For timing epochs\n",
    "from py_vncorenlp import VnCoreNLP\n",
    "\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model output path: ./ViCLIP_landmark\n",
      "Image base path (for resolving paths in JSON): /home/researcher/huypq69/TuningModels/data/LANDMARK-IN-VIETNAM\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration Class (CFG)\n",
    "# import torch\n",
    "# import os\n",
    "\n",
    "# ==============================================================================\n",
    "# Configuration Class (CFG)\n",
    "# ==============================================================================\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    # /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000000000.png\n",
    "    # Base directory where your train.json, dev.json, test.json are located\n",
    "    data_path = \"./data/LANDMARK-IN-VIETNAM/\"\n",
    "    image_path = \"./data/LANDMARK-IN-VIETNAM/\"\n",
    "\n",
    "    # Output directory for saved models\n",
    "    model_path = \"./ViCLIP_landmark\"\n",
    "\n",
    "    # --- Available Models ---\n",
    "    text_models = {\n",
    "        \"PhoBERT-base\": \"vinai/phobert-base\",\n",
    "        \"PhoBERT-large\": \"vinai/phobert-large\",\n",
    "        \"ViT5-base\": \"VietAI/vit5-base\",\n",
    "        \"ViT5-large\": \"VietAI/vit5-large\"\n",
    "    }\n",
    "    image_models = {\n",
    "        \"ViT-S\": \"vit_small_patch16_224\", \"ViT-B\": \"vit_base_patch16_224\",\n",
    "        \"ViT-L\": \"vit_large_patch16_224\", \"ViT-H\": \"vit_huge_patch16_224\",\n",
    "        \"ResNet50\": \"resnet50\"\n",
    "    }\n",
    "\n",
    "    # --- User Selections ---\n",
    "    selected_text_model = \"PhoBERT-base\"\n",
    "    selected_image_model = \"ResNet50\"\n",
    "\n",
    "    # --- Model parameters based on selection (Properties) ---\n",
    "    @property\n",
    "    def model_name(self): return self.image_models[self.selected_image_model]\n",
    "    @property\n",
    "    def text_encoder_model(self): return self.text_models[self.selected_text_model]\n",
    "    @property\n",
    "    def text_tokenizer(self): return self.text_models[self.selected_text_model]\n",
    "\n",
    "    @property\n",
    "    def text_embedding(self): # Encoder output dim before projection\n",
    "        model_key = self.text_models[self.selected_text_model]\n",
    "        if \"large\" in model_key: return 1024\n",
    "        elif \"base\" in model_key: return 768\n",
    "        else: print(f\"Warning: Unknown text embedding size for {model_key}, defaulting to 768.\"); return 768\n",
    "\n",
    "    @property\n",
    "    def image_embedding(self): # Encoder output dim before projection\n",
    "        if self.selected_image_model == \"ResNet50\": return 2048\n",
    "        elif self.selected_image_model == \"ViT-S\": return 384\n",
    "        elif self.selected_image_model == \"ViT-B\": return 768\n",
    "        elif self.selected_image_model == \"ViT-L\": return 1024\n",
    "        elif self.selected_image_model == \"ViT-H\": return 1280\n",
    "        else: print(f\"Warning: Unknown image embedding size for {self.model_name}, defaulting to 768.\"); return 768\n",
    "    \n",
    "    # --- Fixed parameters ---\n",
    "    projection_dim = 256 # Shared latent space dimension\n",
    "\n",
    "    # --- Training parameters ---\n",
    "    seed = 42\n",
    "    batch_size = 64\n",
    "    num_workers = 20  \n",
    "    head_lr = 5e-4\n",
    "    image_encoder_lr = 2e-4\n",
    "    text_encoder_lr = 2e-5\n",
    "    weight_decay = 5e-3\n",
    "    patience = 7\n",
    "    factor = 0.8\n",
    "    epochs = 32\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- Image/Text parameters ---\n",
    "    size = 224 # Input image size\n",
    "    max_length = 200 # Max text sequence length\n",
    "\n",
    "    # --- Loss/Saving parameters ---\n",
    "    temperature = 0.07\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"avg_acc\"\n",
    "    mode = \"max\" if metric_to_track != \"loss\" else \"min\"\n",
    "\n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Seeding for Reproducibility\n",
    "\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # For multi-GPU\n",
    "        # These can slow down training, use cautiously if performance is critical\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Metric Calculation Utilities\n",
    "\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        # Ensure val is a scalar number before adding to sum\n",
    "        if torch.is_tensor(val):\n",
    "             val = val.item() # Convert tensor to Python number\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "        # else:\n",
    "            # Optionally print a warning if the value is not usable\n",
    "            # print(f\"Warning: Cannot update AvgMeter '{self.name}' with value type {type(val)}\")\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    n = similarity_matrix.shape[1-dim]\n",
    "    correct_count = 0\n",
    "    top_k_indices = torch.topk(similarity_matrix, k, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "\n",
    "    if dim == 0: # I2T\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]:\n",
    "                correct_count += 1\n",
    "    elif dim == 1: # T2I\n",
    "        for txt_idx in range(n):\n",
    "             if ground_truth[txt_idx] in top_k_indices[txt_idx, :]:\n",
    "                correct_count += 1\n",
    "    else: raise ValueError(\"dim must be 0 or 1\")\n",
    "    return correct_count / n if n > 0 else 0.0\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    sim_matrix = text_embeddings @ image_embeddings.T\n",
    "    sim_matrix = sim_matrix.float() # Ensure float for calculations\n",
    "    n = sim_matrix.shape[0]\n",
    "    if n == 0:\n",
    "        # Return default zero metrics for empty batch\n",
    "        return {\n",
    "            \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "            \"avg_cosine_sim\": 0.0,\n",
    "            \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "        }\n",
    "\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "    i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "    t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "    t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "    avg_acc = (i2t_acc + t2i_acc) / 2\n",
    "    avg_cosine_sim = torch.diagonal(sim_matrix).mean().item()\n",
    "\n",
    "    i2t_recall = {}\n",
    "    t2i_recall = {}\n",
    "    recall_k_values = [k for k in [1, 5, 10] if k <= n]\n",
    "    for k in recall_k_values:\n",
    "        i2t_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "        t2i_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "\n",
    "    # Ensure all keys R@1, R@5, R@10 exist even if k>n\n",
    "    for k in [1, 5, 10]:\n",
    "        k_str = f\"R@{k}\"\n",
    "        if k_str not in i2t_recall: i2t_recall[k_str] = 0.0\n",
    "        if k_str not in t2i_recall: t2i_recall[k_str] = 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "        \"avg_cosine_sim\": avg_cosine_sim,\n",
    "        \"i2t_recall\": i2t_recall, \"t2i_recall\": t2i_recall\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageCaptionDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Dataset Class Definition\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, tokenizer, transforms, max_length):\n",
    "        super().__init__()\n",
    "        print(f\"Attempting to load data from: {os.path.abspath(json_path)}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                self.data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: JSON file not found at {json_path}\")\n",
    "            print(f\"       Please ensure '{json_path}' exists relative to your notebook or provide the full path.\")\n",
    "            self.data = []\n",
    "            # Optionally raise error: raise FileNotFoundError(f\"JSON file not found at {json_path}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Could not decode JSON from {json_path}\")\n",
    "            self.data = []\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred loading {json_path}: {e}\")\n",
    "            self.data = []\n",
    "\n",
    "        print(f\"Found {len(self.data)} samples in {os.path.basename(json_path)}.\")\n",
    "        self.image_base_path = image_base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transforms = transforms\n",
    "        self.max_length = max_length\n",
    "        # Optional: Check if the image base path exists\n",
    "        if not os.path.isdir(self.image_base_path):\n",
    "             print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "             print(f\"         Ensure 'image_path' in CFG points to the correct directory relative to your notebook.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data):\n",
    "             raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path', None) # Use .get for safety\n",
    "        caption = item.get('caption', '') # Use .get for safety\n",
    "\n",
    "        if relative_image_path is None:\n",
    "            print(f\"Warning: Missing 'image_path' for item at index {idx}. Returning dummy data.\")\n",
    "            image = torch.zeros((3, config.size, config.size))\n",
    "        else:\n",
    "            image_path = os.path.normpath(os.path.join(self.image_base_path, relative_image_path))\n",
    "            # /root/Tuning-CLIP/data/UIT-OpenViIC-dataset/images/00000000000.png\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image = self.transforms(image)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Img not found: {image_path}. Base: {self.image_base_path}, Rel: {relative_image_path}\")\n",
    "                image = torch.zeros((3, config.size, config.size))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading image {image_path}: {e}\")\n",
    "                image = torch.zeros((3, config.size, config.size))\n",
    "\n",
    "        # Process text with consistent dimensions\n",
    "        text_inputs = self.tokenizer(\n",
    "            caption, padding='max_length', truncation=True,\n",
    "            max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Always get first dimension (which should be batch dim of 1) and ensure 1D tensor\n",
    "        input_ids = text_inputs['input_ids'][0]\n",
    "        attention_mask = text_inputs['attention_mask'][0]\n",
    "        \n",
    "        # Verify tensors are 1D\n",
    "        if input_ids.dim() > 1:\n",
    "            # If somehow we get 2D tensor, flatten to 1D\n",
    "            input_ids = input_ids.view(-1)\n",
    "            \n",
    "        if attention_mask.dim() > 1:\n",
    "            attention_mask = attention_mask.view(-1)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "print(\"ImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model components and loss function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Model Component Definitions (Encoders, CLIP Model, Loss)\n",
    "\n",
    "# --- Image Encoder ---\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, config, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        print(f\"Initializing Image Encoder: {config.selected_image_model}\")\n",
    "        if config.selected_image_model == \"ResNet50\":\n",
    "            weights = models.ResNet50_Weights.DEFAULT if pretrained else None\n",
    "            self.model = models.resnet50(weights=weights)\n",
    "            self.input_features = config.image_embedding\n",
    "            self.model.fc = nn.Identity()\n",
    "            print(f\"  Loaded ResNet50 from torchvision. Input features: {self.input_features}\")\n",
    "        # --- Add ViT logic here if needed ---\n",
    "        # elif \"ViT\" in config.selected_image_model:\n",
    "        #     try:\n",
    "        #         import timm\n",
    "        #     except ImportError:\n",
    "        #         print(\"Please install timm library ('pip install timm') to use ViT models.\")\n",
    "        #         raise\n",
    "        #     print(f\"  Loading {config.model_name} using timm.\")\n",
    "        #     self.model = timm.create_model(config.model_name, pretrained=pretrained)\n",
    "        #     self.input_features = config.image_embedding # Should match timm model output before head\n",
    "        #     # Verify input_features matches actual output if possible, e.g., self.model.embed_dim\n",
    "        #     # print(f\"    ViT embed_dim: {self.model.embed_dim}\") # Example check\n",
    "        #     self.model.head = nn.Identity() # Remove classification head\n",
    "        #     print(f\"    Removed head. Input features to projection: {self.input_features}\")\n",
    "        # --- End ViT logic ---\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image model type in config: {config.selected_image_model}\")\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, self.config.projection_dim)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {self.config.projection_dim}\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        # ViT models might output class token + patch tokens, check model architecture\n",
    "        # If using ViT, might need features = features[:, 0] to get class token\n",
    "        features = features.view(features.size(0), -1) # Flatten if needed\n",
    "        projected_features = self.projection(features)\n",
    "        projected_features = F.normalize(projected_features, p=2, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "# --- Text Encoder ---\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, config, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.input_features = config.text_embedding # Dim before projection\n",
    "        print(f\"Initializing Text Encoder: {config.text_encoder_model}\")\n",
    "        print(f\"  Expected input features: {self.input_features}\")\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config.text_encoder_model)\n",
    "        else:\n",
    "            model_config = AutoConfig.from_pretrained(config.text_encoder_model)\n",
    "            self.model = AutoModel.from_config(model_config)\n",
    "\n",
    "        # Check if actual model hidden size matches config\n",
    "        actual_hidden_size = self.model.config.hidden_size\n",
    "        if actual_hidden_size != self.input_features:\n",
    "             print(f\"WARNING: Configured text_embedding ({self.input_features}) does not match actual model hidden size ({actual_hidden_size}). Using actual size for projection.\")\n",
    "             self.input_features = actual_hidden_size # Use actual size\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, self.config.projection_dim)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {self.config.projection_dim}\")\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        if input_ids.dim() == 1: input_ids = input_ids.unsqueeze(0)\n",
    "        if attention_mask.dim() == 1: attention_mask = attention_mask.unsqueeze(0)\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Using [CLS] token embedding\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        projected_features = self.projection(cls_embedding)\n",
    "        projected_features = F.normalize(projected_features, p=2, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "# --- CLIP Model ---\n",
    "class CLIPViModel(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, config):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.temperature = config.temperature\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.image_encoder(image)\n",
    "        text_features = self.text_encoder(input_ids, attention_mask)\n",
    "        logit_scale = 1 / self.temperature\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        return logits_per_image, logits_per_text, image_features, text_features\n",
    "\n",
    "# --- Loss Function ---\n",
    "def contrastive_loss(logits_per_image, logits_per_text):\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    if batch_size == 0: return torch.tensor(0.0, device=logits_per_image.device, requires_grad=True) # Handle empty batch\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "    loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "    total_loss = (loss_img + loss_txt) / 2.0\n",
    "    return total_loss\n",
    "\n",
    "print(\"Model components and loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation epoch functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Training and Validation Epoch Functions\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch_num):\n",
    "    model.train()\n",
    "    loss_meter = AvgMeter(f\"Train Loss E{epoch_num}\")\n",
    "    # Using tqdm.notebook for progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_size = images.size(0)\n",
    "        if batch_size == 0: continue # Skip empty batches\n",
    "\n",
    "        logits_per_image, logits_per_text, _, _ = model(images, input_ids, attention_mask)\n",
    "        loss = contrastive_loss(logits_per_image, logits_per_text)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_meter.update(loss.item(), batch_size)\n",
    "        progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\")\n",
    "\n",
    "    return loss_meter.avg\n",
    "\n",
    "def validate_epoch(model, dataloader, device, epoch_num):\n",
    "    model.eval()\n",
    "    # Initialize meters\n",
    "    loss_meter = AvgMeter(f\"Val Loss E{epoch_num}\")\n",
    "    acc_meter = AvgMeter(f\"Val Acc E{epoch_num}\")\n",
    "    cos_sim_meter = AvgMeter(f\"Val CosSim E{epoch_num}\")\n",
    "    # Initialize all potential recall meters\n",
    "    recall_meters = {\n",
    "        \"i2t_R@1\": AvgMeter(f\"Val I2T R@1 E{epoch_num}\"), \"i2t_R@5\": AvgMeter(f\"Val I2T R@5 E{epoch_num}\"), \"i2t_R@10\": AvgMeter(f\"Val I2T R@10 E{epoch_num}\"),\n",
    "        \"t2i_R@1\": AvgMeter(f\"Val T2I R@1 E{epoch_num}\"), \"t2i_R@5\": AvgMeter(f\"Val T2I R@5 E{epoch_num}\"), \"t2i_R@10\": AvgMeter(f\"Val T2I R@10 E{epoch_num}\"),\n",
    "    }\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Validation E{epoch_num}\", leave=False, unit=\"batch\")\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = images.size(0)\n",
    "            if batch_size == 0: continue # Skip empty batches\n",
    "\n",
    "            logits_per_image, logits_per_text, image_features, text_features = model(images, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(logits_per_image, logits_per_text)\n",
    "            metrics = compute_metrics(image_features, text_features)\n",
    "\n",
    "            # Update meters\n",
    "            loss_meter.update(loss.item(), batch_size)\n",
    "            acc_meter.update(metrics[\"avg_acc\"], batch_size)\n",
    "            cos_sim_meter.update(metrics[\"avg_cosine_sim\"], batch_size)\n",
    "            for k_val, meter in recall_meters.items():\n",
    "                 recall_type, recall_key = k_val.split('_') # 'i2t'/'t2i', 'R@k'\n",
    "                 if recall_key in metrics[f\"{recall_type}_recall\"]:\n",
    "                    meter.update(metrics[f\"{recall_type}_recall\"][recall_key], batch_size)\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss_meter.avg:.4f}\", acc=f\"{acc_meter.avg:.3f}\")\n",
    "\n",
    "    # Collect results\n",
    "    validation_results = {\"loss\": loss_meter.avg, \"avg_acc\": acc_meter.avg, \"avg_cosine_sim\": cos_sim_meter.avg}\n",
    "    for k_val, meter in recall_meters.items():\n",
    "         validation_results[k_val.replace(\"_\", \" \")] = meter.avg # Format key nicely\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "print(\"Training and Validation epoch functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer: vinai/phobert-base\n",
      "Tokenizer loaded successfully.\n",
      "Image transforms defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Setup - Tokenizer and Transforms\n",
    "\n",
    "# 1. Load Tokenizer\n",
    "print(f\"Loading Tokenizer: {config.text_tokenizer}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer)\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading tokenizer '{config.text_tokenizer}': {e}\")\n",
    "    print(\"Please ensure the model name is correct and you have internet access or the model is cached.\")\n",
    "    # Optionally raise the error to stop execution\n",
    "    # raise e\n",
    "\n",
    "# 2. Define Image Transforms\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(config.size),\n",
    "    transforms.CenterCrop(config.size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # ImageNet stats\n",
    "])\n",
    "print(\"Image transforms defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/data/LANDMARK-IN-VIETNAM/train.json\n",
      "Found 19844 samples in train.json.\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/data/LANDMARK-IN-VIETNAM/dev.json\n",
      "Found 5667 samples in dev.json.\n",
      "\n",
      "Creating dataloaders...\n",
      "Using 20 workers for DataLoaders.\n",
      "Train loader created with 311 batches.\n",
      "Validation loader created with 89 batches.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Setup - Datasets and DataLoaders\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "# Construct JSON paths using config.data_path\n",
    "# /Users/quanghuypham/Desktop/CP_Glimpse/Tuning-CLIP/data/UIT-OpenViIC-dataset/test.json\n",
    "train_json = f\"{os.path.abspath(config.data_path)}/train.json\"\n",
    "dev_json = f\"{os.path.abspath(config.data_path)}/dev.json\"\n",
    "test_json = f\"{os.path.abspath(config.data_path)}/test.json\"\n",
    "\n",
    "# Make sure tokenizer is loaded before creating datasets\n",
    "if 'tokenizer' not in globals():\n",
    "     print(\"ERROR: Tokenizer not loaded. Please run the previous cell.\")\n",
    "else:\n",
    "    train_dataset = ImageCaptionDataset(\n",
    "        json_path=train_json, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, transforms=image_transforms, max_length=config.max_length\n",
    "    )\n",
    "    dev_dataset = ImageCaptionDataset(\n",
    "        json_path=dev_json, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, transforms=image_transforms, max_length=config.max_length\n",
    "    )\n",
    "\n",
    "    # Basic checks after loading\n",
    "    if not train_dataset.data:\n",
    "        print(\"\\nERROR: Failed to load training data. Check 'train_json' path and format.\")\n",
    "        # Optionally raise an error: raise ValueError(\"Training data failed to load\")\n",
    "    if not dev_dataset.data:\n",
    "         print(\"\\nWARNING: Failed to load validation data. Validation steps will be skipped.\")\n",
    "\n",
    "    print(\"\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    train_loader = None\n",
    "    if train_dataset.data:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False # Keep last incomplete batch for training\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "\n",
    "    dev_loader = None\n",
    "    if dev_dataset.data:\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False # Keep last incomplete batch for validation\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "\n",
    "    if not train_loader:\n",
    "         print(\"\\nERROR: Train loader could not be created. Cannot proceed.\")\n",
    "         # raise ValueError(\"Train loader creation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model components...\n",
      "Initializing Image Encoder: ResNet50\n",
      "  Loaded ResNet50 from torchvision. Input features: 2048\n",
      "  Added projection head: 2048 -> 256\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Expected input features: 768\n",
      "  Added projection head: 768 -> 256\n",
      "\n",
      "CLIPViModel initialized successfully on cuda.\n",
      "Trainable parameters: 159.23 M\n",
      "\n",
      "Setting up optimizer...\n",
      "  Param counts: ImgBase=159, ImgHead=2, TxtBase=199, TxtHead=2\n",
      "Optimizer AdamW initialized.\n",
      "LR Scheduler ReduceLROnPlateau initialized (mode='max', factor=0.8, patience=7)\n",
      "Early stopping initialized (patience=5, min_delta=0.001)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Setup - Model, Optimizer, Scheduler\n",
    "\n",
    "print(\"\\nInitializing model components...\")\n",
    "try:\n",
    "    image_encoder = ImageEncoder(config).to(config.device)\n",
    "    text_encoder = TextEncoder(config).to(config.device)\n",
    "    model = CLIPViModel(image_encoder, text_encoder, config).to(config.device)\n",
    "    print(f\"\\nCLIPViModel initialized successfully on {config.device}.\")\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {num_params / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing models: {e}\")\n",
    "    print(\"Check model names in CFG, internet connection, and available memory.\")\n",
    "    # raise e # Optionally stop execution\n",
    "\n",
    "# --- Optimizer Setup ---\n",
    "if 'model' in globals() : # Check if model was created\n",
    "    print(\"\\nSetting up optimizer...\")\n",
    "    image_encoder_params = list(model.image_encoder.model.parameters())\n",
    "    image_head_params = list(model.image_encoder.projection.parameters())\n",
    "    text_encoder_params = list(model.text_encoder.model.parameters())\n",
    "    text_head_params = list(model.text_encoder.projection.parameters())\n",
    "\n",
    "    print(f\"  Param counts: ImgBase={len(image_encoder_params)}, ImgHead={len(image_head_params)}, TxtBase={len(text_encoder_params)}, TxtHead={len(text_head_params)}\")\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for p in image_encoder_params if p.requires_grad], \"lr\": config.image_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": [p for p in image_head_params if p.requires_grad], \"lr\": config.head_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": [p for p in text_encoder_params if p.requires_grad], \"lr\": config.text_encoder_lr, \"weight_decay\": config.weight_decay},\n",
    "        {\"params\": [p for p in text_head_params if p.requires_grad], \"lr\": config.head_lr, \"weight_decay\": config.weight_decay},\n",
    "    ]\n",
    "\n",
    "    # Filter out groups with no parameters (can happen if parts are frozen)\n",
    "    optimizer_grouped_parameters = [g for g in optimizer_grouped_parameters if g['params']]\n",
    "\n",
    "    if not optimizer_grouped_parameters:\n",
    "         print(\"ERROR: No parameters found for the optimizer. Check model structure and requires_grad flags.\")\n",
    "         # raise ValueError(\"Optimizer has no parameters\")\n",
    "    else:\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "        print(f\"Optimizer AdamW initialized.\")\n",
    "\n",
    "        # --- LR Scheduler Setup ---\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode=config.mode, # Use mode from config ('min' for loss, 'max' for acc/recall)\n",
    "            factor=config.factor, patience=config.patience\n",
    "        )\n",
    "        print(f\"LR Scheduler ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.factor}, patience={config.patience})\")\n",
    "        \n",
    "        # --- Early Stopping Setup ---\n",
    "        early_stopping_patience = getattr(config, 'early_stopping_patience', 5) \n",
    "        early_stopping_min_delta = getattr(config, 'early_stopping_min_delta', 0.001)\n",
    "        early_stopping_counter = 0\n",
    "        print(f\"Early stopping initialized (patience={early_stopping_patience}, min_delta={early_stopping_min_delta})\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not initialized. Skipping optimizer/scheduler setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 32 epochs...\n",
      "Tracking metric: 'avg_acc' (mode: max)\n",
      "\n",
      "--- Epoch 1/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4d9db72d534d8bb56685c25140b1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E1:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.2413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6502b43ad8b148eca53abe68d9d24ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E1:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 2.5007 | avg_acc: 0.1685 | avg_cosine_sim: 0.5260 | i2t R@1: 0.1735 | i2t R@5: 0.6965 | i2t R@10: 0.8742 | t2i R@1: 0.1636 | t2i R@5: 0.7275 | t2i R@10: 0.8733\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "  Saved Best Model (Epoch 1, avg_acc=0.1685) to ./ViCLIP_landmark/clip_vi_best.pt\n",
      "--- Epoch 1 Time: 136.03 seconds ---\n",
      "\n",
      "--- Epoch 2/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b77fc6813e4e4aa2c0b8dccf353dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E2:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 1.4354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cc0a67b1da4740aff5ec37939d9389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E2:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 2.4817 | avg_acc: 0.1795 | avg_cosine_sim: 0.5067 | i2t R@1: 0.1842 | i2t R@5: 0.7258 | i2t R@10: 0.8950 | t2i R@1: 0.1747 | t2i R@5: 0.7623 | t2i R@10: 0.8975\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "  Saved Best Model (Epoch 2, avg_acc=0.1795) to ./ViCLIP_landmark/clip_vi_best.pt\n",
      "--- Epoch 2 Time: 131.38 seconds ---\n",
      "\n",
      "--- Epoch 3/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc76699950124d70a11c3e8285e174c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E3:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 1.1067\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9729ce26826443a4886886ff21f0a410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation E3:   0%|          | 0/89 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Metrics:\n",
      "loss: 2.4986 | avg_acc: 0.1762 | avg_cosine_sim: 0.4919 | i2t R@1: 0.1819 | i2t R@5: 0.7311 | i2t R@10: 0.8968 | t2i R@1: 0.1705 | t2i R@5: 0.7604 | t2i R@10: 0.9014\n",
      "  Current LRs: ImgEnc=2.00e-04, Head=5.00e-04, TxtEnc=2.00e-05\n",
      "--- Epoch 3 Time: 130.39 seconds ---\n",
      "\n",
      "--- Epoch 4/32 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecdc8266cd440c19f681b815b12805a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E4:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# --- Training ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_loss)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device, epoch_num)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Using tqdm.notebook for progress bar\u001b[39;00m\n\u001b[32m      7\u001b[39m progress_bar = tqdm(dataloader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining E\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m, unit=\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/Tuning-CLIP/.venv/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/Tuning-CLIP/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/Tuning-CLIP/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/Tuning-CLIP/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/Tuning-CLIP/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1410\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1410\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1411\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1412\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/huypq69/Tuning-CLIP/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1248\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1254\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1256\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 11: Training Loop\n",
    "\n",
    "if 'model' in globals() and 'train_loader' in globals() and 'optimizer' in globals(): # Check prerequisites\n",
    "    print(f\"\\nStarting training for {config.epochs} epochs...\")\n",
    "    print(f\"Tracking metric: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    history = {'train_loss': [], 'validation_results': []}\n",
    "    start_train_time = time.time()\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "        # --- Training ---\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, config.device, epoch+1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_results = {\"loss\": float('inf'), \"avg_acc\": 0.0} # Default if no validation\n",
    "        if dev_loader:\n",
    "            val_results = validate_epoch(model, dev_loader, config.device, epoch+1)\n",
    "            history['validation_results'].append(val_results)\n",
    "            # Print validation metrics\n",
    "            print(\"  Validation Metrics:\")\n",
    "            metric_log_str = \"  \"\n",
    "            for name, value in val_results.items():\n",
    "                metric_log_str += f\"{name}: {value:.4f} | \"\n",
    "            print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "            # Step the LR scheduler based on the tracked metric\n",
    "            current_val_metric_for_scheduler = val_results.get(config.metric_to_track, None)\n",
    "            if current_val_metric_for_scheduler is not None:\n",
    "                 lr_scheduler.step(current_val_metric_for_scheduler)\n",
    "                 current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "                 # Simplified LR print: print first + last group LR assuming Head LRs are same\n",
    "                 print(f\"  Current LRs: ImgEnc={current_lrs[0]:.2e}, Head={current_lrs[1]:.2e}, TxtEnc={current_lrs[2]:.2e}\")\n",
    "            else:\n",
    "                 print(f\"  Warning: Metric '{config.metric_to_track}' not found in validation results. Scheduler not stepped.\")\n",
    "\n",
    "        else:\n",
    "             print(\"  Validation skipped.\")\n",
    "             history['validation_results'].append(None) # Append None if no validation\n",
    "\n",
    "        # --- Save Checkpoint ---\n",
    "        current_val_metric = val_results.get(config.metric_to_track, -float('inf') if config.mode == \"max\" else float('inf'))\n",
    "\n",
    "        is_best = False\n",
    "        if dev_loader: # Only compare if validation was done\n",
    "            if config.mode == \"max\" and current_val_metric > best_val_metric:\n",
    "                is_best = True\n",
    "                best_val_metric = current_val_metric\n",
    "            elif config.mode == \"min\" and current_val_metric < best_val_metric:\n",
    "                is_best = True\n",
    "                best_val_metric = current_val_metric\n",
    "\n",
    "        # Prepare save dictionary\n",
    "        save_dict = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'validation_results': val_results,\n",
    "            'best_val_metric': best_val_metric, # Store the best metric value seen so far\n",
    "            'metric_tracked': config.metric_to_track, # Store which metric was tracked\n",
    "        }\n",
    "\n",
    "        # Save logic\n",
    "        if config.save_best_only and dev_loader:\n",
    "            if is_best:\n",
    "                best_checkpoint_path = os.path.join(config.model_path, \"clip_vi_best.pt\")\n",
    "                torch.save(save_dict, best_checkpoint_path)\n",
    "                print(f\"  Saved Best Model (Epoch {epoch+1}, {config.metric_to_track}={current_val_metric:.4f}) to {best_checkpoint_path}\")\n",
    "        else: # Save every epoch if not save_best_only\n",
    "            epoch_checkpoint_path = os.path.join(config.model_path, f\"clip_vi_epoch_{epoch+1}.pt\")\n",
    "            torch.save(save_dict, epoch_checkpoint_path)\n",
    "            print(f\"  Saved Epoch {epoch+1} Checkpoint to {epoch_checkpoint_path}\")\n",
    "            if is_best and dev_loader: # Also save a copy as best if it's the best so far\n",
    "                 best_checkpoint_path = os.path.join(config.model_path, \"clip_vi_best.pt\")\n",
    "                 torch.save(save_dict, best_checkpoint_path)\n",
    "                 print(f\"  (Also saved as best model)\")\n",
    "\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {epoch_end_time - epoch_start_time:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "    # --- End of Training ---\n",
    "    end_train_time = time.time()\n",
    "    total_train_time = end_train_time - start_train_time\n",
    "    print(f\"\\n=============== Training Finished ===============\")\n",
    "    print(f\"Total Training Time: {total_train_time:.2f} seconds ({total_train_time/60:.2f} minutes)\")\n",
    "    final_model_path = os.path.join(config.model_path, 'clip_vi_final_epoch.pt')\n",
    "    torch.save(save_dict, final_model_path) # Save the final epoch state regardless\n",
    "    print(f\"Final epoch model state saved to {final_model_path}\")\n",
    "    if config.save_best_only and dev_loader and os.path.exists(os.path.join(config.model_path, \"clip_vi_best.pt\")):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) saved to: {os.path.join(config.model_path, 'clip_vi_best.pt')}\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training (model, dataloader, optimizer) not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Starting Test Set Evaluation ===============\n",
      "Loading test data from: ./data/LANDMARK-IN-VIETNAM/test.json\n",
      "Attempting to load data from: /home/researcher/huypq69/TuningModels/data/LANDMARK-IN-VIETNAM/test.json\n",
      "Found 2845 samples in test.json.\n",
      "Test loader created with 45 batches.\n",
      "Initializing Image Encoder: ResNet50\n",
      "  Loaded ResNet50 from torchvision. Input features: 2048\n",
      "  Added projection head: 2048 -> 256\n",
      "ERROR creating model structure for testing: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "================= Evaluation Finished =================\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Final Evaluation on Test Set\n",
    "\n",
    "print(\"\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "\n",
    "if os.path.exists(test_json_path) and 'tokenizer' in globals():\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    test_dataset = ImageCaptionDataset(\n",
    "        json_path=test_json_path, image_base_path=config.image_path,\n",
    "        tokenizer=tokenizer, transforms=image_transforms, max_length=config.max_length\n",
    "    )\n",
    "\n",
    "    if test_dataset.data:\n",
    "        num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False\n",
    "        )\n",
    "        print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "\n",
    "        # --- Load Model for Testing ---\n",
    "        # Create a fresh model instance\n",
    "        try:\n",
    "            test_image_encoder = ImageEncoder(config).to(config.device)\n",
    "            test_text_encoder = TextEncoder(config).to(config.device)\n",
    "            model_to_test = CLIPViModel(test_image_encoder, test_text_encoder, config).to(config.device)\n",
    "            print(\"Model structure for testing created.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR creating model structure for testing: {e}\")\n",
    "            model_to_test = None # Prevent loading if structure fails\n",
    "\n",
    "        if model_to_test:\n",
    "            # Determine which model weights to load (best or final)\n",
    "            best_model_path = os.path.join(config.model_path, \"clip_vi_best.pt\")\n",
    "            final_model_path = os.path.join(config.model_path, \"clip_vi_final_epoch.pt\") # Use final epoch saved name\n",
    "\n",
    "            load_path = None\n",
    "            if os.path.exists(best_model_path):\n",
    "                load_path = best_model_path\n",
    "                print(f\"Attempting to load best model weights from: {load_path}\")\n",
    "            elif os.path.exists(final_model_path):\n",
    "                load_path = final_model_path\n",
    "                print(f\"Best model not found. Attempting to load final epoch weights from: {load_path}\")\n",
    "            else:\n",
    "                print(\"WARNING: No saved model checkpoints ('best' or 'final') found in output directory.\")\n",
    "                print(\"         Evaluation will not be performed.\")\n",
    "\n",
    "            if load_path:\n",
    "                try:\n",
    "                    checkpoint = torch.load(load_path, map_location=config.device)\n",
    "                    state_dict = checkpoint['model_state_dict']\n",
    "                    # Handle potential 'module.' prefix if saved with DataParallel or DDP\n",
    "                    if next(iter(state_dict)).startswith('module.'):\n",
    "                        print(\"Detected 'module.' prefix, removing for loading.\")\n",
    "                        from collections import OrderedDict\n",
    "                        new_state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "                        model_to_test.load_state_dict(new_state_dict)\n",
    "                    else:\n",
    "                        model_to_test.load_state_dict(state_dict)\n",
    "                    print(f\"Model weights loaded successfully from {load_path}\")\n",
    "\n",
    "                    # --- Run Evaluation ---\n",
    "                    print(\"\\nRunning evaluation on test set...\")\n",
    "                    test_results = validate_epoch(model_to_test, test_loader, config.device, epoch_num=\"Test\") # Use validate func\n",
    "\n",
    "                    print(\"\\n--- Test Set Results ---\")\n",
    "                    metric_log_str = \"\"\n",
    "                    for name, value in test_results.items():\n",
    "                        metric_log_str += f\"  {name}: {value:.4f}\\n\"\n",
    "                    print(metric_log_str.strip())\n",
    "                    print(\"------------------------\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nERROR loading model weights or running evaluation from {load_path}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc() # Print detailed traceback\n",
    "            # --- End Model Loading/Eval ---\n",
    "        # --- End Model Structure Check ---\n",
    "    else:\n",
    "         print(\"Could not load test data. Skipping test evaluation.\")\n",
    "else:\n",
    "    if not os.path.exists(test_json_path):\n",
    "         print(f\"\\nTest JSON path not found ({test_json_path}). Skipping test evaluation.\")\n",
    "    if 'tokenizer' not in globals():\n",
    "         print(\"\\nTokenizer not available. Skipping test evaluation.\")\n",
    "\n",
    "print(\"\\n================= Evaluation Finished =================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
