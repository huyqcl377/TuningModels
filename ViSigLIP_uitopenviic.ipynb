{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "Transformers Version: 4.50.0\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: Imports ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from transformers import (\n",
    "    BlipForImageTextRetrieval,\n",
    "    BlipVisionModel,\n",
    "    BlipConfig,\n",
    "    BlipImageProcessor,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import transformers\n",
    "import gc\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(1)}\")\n",
    "    torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Effective Batch Size (per optimizer step): 2048\n",
      "Model output path: ./trained_models/ViSigLIP_uitopenviic\n",
      "Selected Vision Source: Salesforce/blip-image-captioning-base\n",
      "Selected Text Model: vinai/phobert-base\n",
      "Image base path (for resolving paths in JSON): /home/researcher/huypq69/TuningModels/data/OpenViVQA-dataset\n",
      "AMP Enabled: True\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Configuration Class (CFG) for ViSigLIP Pretraining ===\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    data_path = \"./json_data/\"\n",
    "    image_base_path = \"./data/OpenViVQA-dataset/\"\n",
    "    model_path = \"./trained_models/ViSigLIP_uitopenviic\"\n",
    "\n",
    "    # --- Model Selection ---\n",
    "    selected_vision_source = \"Salesforce/blip-image-captioning-base\"\n",
    "    selected_text_model = \"vinai/phobert-base\"\n",
    "    text_tokenizer_name = selected_text_model\n",
    "\n",
    "    # --- Model parameters ---\n",
    "    blip_vision_model_name = selected_vision_source\n",
    "    blip_image_processor_name = selected_vision_source\n",
    "    @property\n",
    "    def text_embedding(self): return 768\n",
    "    @property\n",
    "    def vision_embedding(self): return 768\n",
    "    projection_dim = 768\n",
    "\n",
    "    # --- SigLIP specific ---\n",
    "    learnable_temperature = True\n",
    "    temperature_init = 10.0\n",
    "    learnable_bias = True\n",
    "    bias_init = -10.0\n",
    "\n",
    "    # --- Training parameters ---\n",
    "    seed = 42\n",
    "    batch_size = 32   # Keep reduced batch size\n",
    "    accumulation_steps = 64 # Keep high accumulation for large effective batch\n",
    "    num_workers = 20\n",
    "\n",
    "    learning_rate = 1e-4\n",
    "    warmup_steps = 1000 # Adjust warmup relative to total steps? Maybe keep at 1k? Or reduce? Let's keep 1000 for now.\n",
    "    weight_decay = 0.1\n",
    "\n",
    "    scheduler_type = \"cosine\"\n",
    "    rop_patience = 5\n",
    "    rop_factor = 0.5\n",
    "\n",
    "    epochs = 50\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_amp = True\n",
    "\n",
    "    max_length = 200\n",
    "\n",
    "    # --- Loss/Saving parameters ---\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"avg_acc\"\n",
    "    mode = \"max\"\n",
    "    # Adjust intervals based on new total steps (calculated in Cell 9)\n",
    "    save_interval_steps = 500 # Example: Save every 500 steps\n",
    "    validation_interval_steps = 1000\n",
    "    log_interval_steps = 50 # Log reasonably often\n",
    "\n",
    "    early_stopping_patience = 5 # Patience in terms of validation checks\n",
    "    early_stopping_min_delta = 0.001\n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Effective Batch Size (per optimizer step): {config.batch_size * config.accumulation_steps}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Selected Vision Source: {config.selected_vision_source}\")\n",
    "print(f\"Selected Text Model: {config.selected_text_model}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_base_path)}\")\n",
    "print(f\"AMP Enabled: {config.use_amp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: Seeding ===\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Metric & AvgMeter Utilities ===\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0.0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        if torch.is_tensor(val):\n",
    "            val = val.item()\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = float(self.sum) / self.count if self.count != 0 else 0.0\n",
    "        else:\n",
    "            print(f\"Warning: Cannot update AvgMeter '{self.name}' with value type {type(val)}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.name}: {self.avg:.4f}\"\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    n = similarity_matrix.shape[1-dim]\n",
    "    if n == 0: \n",
    "        return 0.0\n",
    "    \n",
    "    correct_count = 0\n",
    "    actual_k = min(k, similarity_matrix.shape[dim])\n",
    "    if actual_k == 0: \n",
    "        return 0.0\n",
    "\n",
    "    top_k_indices = torch.topk(similarity_matrix, actual_k, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "\n",
    "    if dim == 0:  # I2T\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]:\n",
    "                correct_count += 1\n",
    "    elif dim == 1:  # T2I\n",
    "        for txt_idx in range(n):\n",
    "            if ground_truth[txt_idx] in top_k_indices[txt_idx, :]:\n",
    "                correct_count += 1\n",
    "    else: \n",
    "        raise ValueError(\"dim must be 0 or 1\")\n",
    "        \n",
    "    return float(correct_count) / n if n > 0 else 0.0\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    # Ensure embeddings are float32 for stable matmul\n",
    "    sim_matrix = text_embeddings.float() @ image_embeddings.float().T\n",
    "    n = sim_matrix.shape[0]\n",
    "    \n",
    "    if n == 0:\n",
    "        # Return default zero metrics for empty batch\n",
    "        return {\n",
    "            \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "            \"avg_cosine_sim\": 0.0,\n",
    "            \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "        }\n",
    "\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "    i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "    t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    \n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "    t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "    avg_acc = (i2t_acc + t2i_acc) / 2.0\n",
    "    avg_cosine_sim = torch.diag(sim_matrix).mean().item()\n",
    "\n",
    "    i2t_recall = {}\n",
    "    t2i_recall = {}\n",
    "    recall_k_values = [k for k in [1, 5, 10] if k <= n]\n",
    "    \n",
    "    for k in recall_k_values:\n",
    "        i2t_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "        t2i_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "\n",
    "    # Ensure all keys R@1, R@5, R@10 exist even if k>n\n",
    "    for k in [1, 5, 10]:\n",
    "        k_str = f\"R@{k}\"\n",
    "        if k_str not in i2t_recall: i2t_recall[k_str] = 0.0\n",
    "        if k_str not in t2i_recall: t2i_recall[k_str] = 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"i2t_acc\": i2t_acc, \n",
    "        \"t2i_acc\": t2i_acc, \n",
    "        \"avg_acc\": avg_acc,\n",
    "        \"avg_cosine_sim\": avg_cosine_sim,\n",
    "        \"i2t_recall\": i2t_recall, \n",
    "        \"t2i_recall\": t2i_recall\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomImageCaptionDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Dataset Class Definition ===\n",
    "\n",
    "class CustomImageCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads image-caption pairs from JSON metadata.\n",
    "    Handles both single JSON list format and JSON-per-line format.\n",
    "    \"\"\"\n",
    "    def __init__(self, json_path_or_list, image_base_path, tokenizer, image_processor, max_length):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        if isinstance(json_path_or_list, str) and os.path.isdir(json_path_or_list):\n",
    "            json_files = [os.path.join(json_path_or_list, f) for f in os.listdir(json_path_or_list) if f.endswith('.json')]\n",
    "            print(f\"Found {len(json_files)} JSON files in {json_path_or_list}\")\n",
    "        elif isinstance(json_path_or_list, str) and os.path.isfile(json_path_or_list):\n",
    "            json_files = [json_path_or_list]\n",
    "        elif isinstance(json_path_or_list, list):\n",
    "            json_files = json_path_or_list\n",
    "        else:\n",
    "            raise ValueError(\"json_path_or_list must be a directory, a single JSON file, or a list of JSON files.\")\n",
    "\n",
    "        print(\"Loading JSON metadata (this might take time for large datasets)...\")\n",
    "        total_loaded_count = 0\n",
    "        for json_path in tqdm(json_files, desc=\"Loading JSONs\"):\n",
    "            try:\n",
    "                with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        # Attempt to load the entire file as a single JSON object\n",
    "                        file_data = json.load(f)\n",
    "                        if isinstance(file_data, list):\n",
    "                            self.data.extend(file_data)\n",
    "                            total_loaded_count += len(file_data)\n",
    "                        else:\n",
    "                            # Handle case where it's a single dict\n",
    "                            self.data.append(file_data)\n",
    "                            total_loaded_count += 1\n",
    "                            print(f\"  Warning: Loaded single JSON object from {json_path}, expected a list.\")\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Try line-by-line (JSON-per-line format)\n",
    "                        print(f\"  Info: Failed to load {json_path} as single JSON. Attempting JSON-per-line format...\")\n",
    "                        f.seek(0)  # Rewind file pointer\n",
    "                        count_line_by_line = 0\n",
    "                        for line in f:\n",
    "                            line = line.strip()\n",
    "                            if line:  # Skip empty lines\n",
    "                                try:\n",
    "                                    line_data = json.loads(line)\n",
    "                                    self.data.append(line_data)\n",
    "                                    count_line_by_line += 1\n",
    "                                except json.JSONDecodeError as line_err:\n",
    "                                    print(f\"  ERROR parsing line in {json_path}: {line_err}. Line content (partial): {line[:100]}...\")\n",
    "                        total_loaded_count += count_line_by_line\n",
    "                        if count_line_by_line > 0:\n",
    "                            print(f\"  Successfully loaded {count_line_by_line} items using JSON-per-line format from {json_path}.\")\n",
    "                        else:\n",
    "                            print(f\"  Failed to load any data using JSON-per-line format from {json_path} either.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR opening or processing file {json_path}: {e}\")\n",
    "\n",
    "        print(f\"Loaded {total_loaded_count} samples total from {len(json_files)} file(s).\")\n",
    "        self.data = [item for item in self.data if item]  # Clean out any potential None entries\n",
    "        print(f\"Dataset size after potential cleaning: {len(self.data)}\")\n",
    "\n",
    "        if not self.data:\n",
    "            print(\"WARNING: No data loaded! Training cannot proceed.\")\n",
    "\n",
    "        self.image_base_path = image_base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "        try:\n",
    "            if isinstance(image_processor.size, dict):\n",
    "                proc_size = image_processor.size\n",
    "                self.img_size = proc_size.get('height', proc_size.get('shortest_edge', 224))\n",
    "            else:\n",
    "                self.img_size = image_processor.size\n",
    "                if isinstance(self.img_size, (tuple, list)): self.img_size = self.img_size[0]\n",
    "        except AttributeError:\n",
    "            print(\"Warning: Could not determine image size from processor, defaulting to 224.\")\n",
    "            self.img_size = 224\n",
    "        print(f\"Using image target size: {self.img_size}x{self.img_size}\")\n",
    "        if not os.path.isdir(self.image_base_path):\n",
    "            print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data): raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "\n",
    "        relative_image_path = item.get('image_path', item.get('url', item.get('filename')))\n",
    "        # Handle caption being list OR string\n",
    "        caption_data = item.get('caption', item.get('text', item.get('title', '')))\n",
    "        if isinstance(caption_data, list):\n",
    "            caption = caption_data[0] if caption_data else \"\"\n",
    "        elif isinstance(caption_data, str):\n",
    "            caption = caption_data\n",
    "        else:\n",
    "            caption = \"\"\n",
    "\n",
    "        if not relative_image_path or not caption:\n",
    "            return self._get_dummy_item()\n",
    "\n",
    "        # Load Image\n",
    "        try:\n",
    "            image_path = os.path.join(self.image_base_path, relative_image_path)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_inputs = self.image_processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = image_inputs['pixel_values'].squeeze(0)\n",
    "        except Exception:\n",
    "            return self._get_dummy_item()\n",
    "\n",
    "        # Process Text\n",
    "        try:\n",
    "            text_inputs = self.tokenizer(\n",
    "                caption, padding='max_length', truncation=True,\n",
    "                max_length=self.max_length, return_tensors='pt'\n",
    "            )\n",
    "            input_ids = text_inputs['input_ids'].squeeze(0)\n",
    "            attention_mask = text_inputs['attention_mask'].squeeze(0)\n",
    "        except Exception:\n",
    "            return self._get_dummy_item()\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "    def _get_dummy_item(self):\n",
    "        return {\n",
    "            \"pixel_values\": torch.zeros((3, self.img_size, self.img_size), dtype=torch.float),\n",
    "            \"input_ids\": torch.zeros(self.max_length, dtype=torch.long),\n",
    "            \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"CustomImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViSigLIP Model components defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Model Definition (ViSigLIP) ===\n",
    "# Uses corrected vision loading V2\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Encodes images using BLIP's Vision Model (Base).\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        print(f\"Initializing BLIP Vision Encoder from: {config_train.blip_vision_model_name} by loading BlipForImageTextRetrieval first.\")\n",
    "\n",
    "        if pretrained:\n",
    "            try:\n",
    "                print(\"  Loading base BlipForImageTextRetrieval...\")\n",
    "                full_blip_model = BlipForImageTextRetrieval.from_pretrained(\n",
    "                    config_train.blip_vision_model_name,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                )\n",
    "                print(\"  Extracting vision_model from BlipForImageTextRetrieval.\")\n",
    "                self.vision_model = full_blip_model.vision_model\n",
    "                del full_blip_model\n",
    "                print(\"  Vision model extracted successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR loading BlipForImageTextRetrieval or extracting vision model: {e}\")\n",
    "                print(\"  Falling back to initializing BlipVisionModel directly (might show warnings).\")\n",
    "                self.vision_model = BlipVisionModel.from_pretrained(config_train.blip_vision_model_name)\n",
    "        else:\n",
    "            print(\"  Initializing BlipVisionModel from scratch (as pretrained=False).\")\n",
    "            blip_vision_config = BlipConfig.from_pretrained(config_train.blip_vision_model_name).vision_config\n",
    "            self.vision_model = BlipVisionModel(blip_vision_config)\n",
    "\n",
    "        try:\n",
    "            self.input_features = self.vision_model.config.hidden_size\n",
    "        except AttributeError as e:\n",
    "            print(f\"  ERROR accessing vision_model.config.hidden_size: {e}. Attempting config_train value.\")\n",
    "            self.input_features = config_train.vision_embedding  # Fallback\n",
    "\n",
    "        if hasattr(config_train, 'vision_embedding') and self.input_features != config_train.vision_embedding:\n",
    "            print(f\"  WARNING: Configured vision_embedding ({config_train.vision_embedding}) doesn't match loaded model hidden size ({self.input_features}). Using loaded size.\")\n",
    "        else:\n",
    "            print(f\"  Confirmed/Using vision model hidden size: {self.input_features}\")\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        vision_outputs = self.vision_model(pixel_values=pixel_values, return_dict=True)\n",
    "        image_features = vision_outputs.pooler_output\n",
    "        projected_features = self.projection(image_features)\n",
    "        return projected_features\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Encodes text using PhoBERT-Base.\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        print(f\"Initializing Text Encoder: {config_train.selected_text_model}\")\n",
    "\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config_train.selected_text_model)\n",
    "        else:\n",
    "            model_config = AutoConfig.from_pretrained(config_train.selected_text_model)\n",
    "            self.model = AutoModel.from_config(model_config)\n",
    "\n",
    "        try:\n",
    "            self.input_features = self.model.config.hidden_size\n",
    "        except AttributeError as e:\n",
    "            print(f\"  ERROR accessing model.config.hidden_size: {e}. Attempting config_train value.\")\n",
    "            self.input_features = config_train.text_embedding  # Fallback\n",
    "\n",
    "        if hasattr(config_train, 'text_embedding') and self.input_features != config_train.text_embedding:\n",
    "            print(f\"  WARNING: Configured text_embedding ({config_train.text_embedding}) doesn't match loaded PhoBERT hidden size ({self.input_features}). Using actual size.\")\n",
    "        else:\n",
    "            print(f\"  Confirmed text model hidden size: {self.input_features}\")\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        text_features = outputs.last_hidden_state[:, 0, :]\n",
    "        projected_features = self.projection(text_features)\n",
    "        return projected_features\n",
    "\n",
    "class ViSigLIPModel(nn.Module):\n",
    "    \"\"\"ViSigLIP Model: BLIP Vision + PhoBERT Text + Sigmoid Loss components\"\"\"\n",
    "    def __init__(self, image_encoder, text_encoder, config_train):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.config_train = config_train\n",
    "\n",
    "        # Learnable Temperature\n",
    "        if config_train.learnable_temperature:\n",
    "            init_val_t = torch.tensor(config_train.temperature_init, dtype=torch.float)\n",
    "            self.logit_scale = nn.Parameter(init_val_t)\n",
    "            print(f\"Using learnable temperature, initialized to {self.logit_scale.item():.4f}\")\n",
    "        else:\n",
    "            temp_tensor = torch.tensor(config_train.temperature_init, dtype=torch.float)\n",
    "            self.register_buffer('logit_scale', temp_tensor)\n",
    "            print(f\"Using fixed temperature: {self.logit_scale.item():.4f}\")\n",
    "\n",
    "        # Learnable Bias\n",
    "        if config_train.learnable_bias:\n",
    "            init_val_b = torch.tensor(config_train.bias_init, dtype=torch.float)\n",
    "            self.logit_bias = nn.Parameter(init_val_b)\n",
    "            print(f\"Using learnable bias, initialized to {self.logit_bias.item():.4f}\")\n",
    "        else:\n",
    "            bias_tensor = torch.tensor(config_train.bias_init, dtype=torch.float)\n",
    "            self.register_buffer('logit_bias', bias_tensor)\n",
    "            print(f\"Using fixed bias: {self.logit_bias.item():.4f}\")\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # Move inputs to device\n",
    "        pixel_values = pixel_values.to(self.config_train.device)\n",
    "        input_ids = input_ids.to(self.config_train.device)\n",
    "        attention_mask = attention_mask.to(self.config_train.device)\n",
    "\n",
    "        # Get embeddings from encoders\n",
    "        image_embed = self.image_encoder(pixel_values)\n",
    "        text_embed = self.text_encoder(input_ids, attention_mask)\n",
    "\n",
    "        # Normalize embeddings L2 norm\n",
    "        image_features = F.normalize(image_embed, p=2, dim=-1)\n",
    "        text_features = F.normalize(text_embed, p=2, dim=-1)\n",
    "\n",
    "        # Return normalized features, temperature, and bias for loss calculation\n",
    "        current_temp = self.logit_scale.to(image_features.device)\n",
    "        current_bias = self.logit_bias.to(image_features.device)\n",
    "\n",
    "        return image_features, text_features, current_temp, current_bias\n",
    "\n",
    "print(\"ViSigLIP Model components defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SigLIP loss function defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 7: SigLIP Loss Function ===\n",
    "def siglip_loss(image_features, text_features, logit_scale, logit_bias):\n",
    "    \"\"\"\n",
    "    Computes the SigLIP loss.\n",
    "    Based on https://arxiv.org/abs/2303.15343\n",
    "    \"\"\"\n",
    "    # Ensure features are float32 for stability\n",
    "    image_features = image_features.float()\n",
    "    text_features = text_features.float()\n",
    "    logit_scale = logit_scale.float()\n",
    "    logit_bias = logit_bias.float()\n",
    "\n",
    "    n = text_features.shape[0]\n",
    "    if n == 0:\n",
    "        return torch.tensor(0.0, device=image_features.device, requires_grad=True)\n",
    "\n",
    "    # Calculate cosine similarity with temperature scaling and bias\n",
    "    logits = image_features @ text_features.t() * logit_scale + logit_bias\n",
    "\n",
    "    # Create labels: 1 for positive pairs (diagonal), 0 for negative pairs\n",
    "    labels = torch.eye(n, device=logits.device, dtype=logits.dtype)\n",
    "\n",
    "    # Binary cross-entropy with logits loss\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='mean')\n",
    "\n",
    "    return loss\n",
    "\n",
    "print(\"SigLIP loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer: vinai/phobert-base\n",
      "PhoBERT Tokenizer loaded successfully.\n",
      "Loading Image Processor from: Salesforce/blip-image-captioning-base\n",
      "BLIP Image Processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8: Setup - Tokenizer and Image Processor ===\n",
    "from transformers import AutoTokenizer, BlipImageProcessor\n",
    "\n",
    "tokenizer = None\n",
    "image_processor = None\n",
    "\n",
    "print(f\"Loading Tokenizer: {config.text_tokenizer_name}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer_name)\n",
    "    print(\"PhoBERT Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading tokenizer '{config.text_tokenizer_name}': {e}\")\n",
    "\n",
    "print(f\"Loading Image Processor from: {config.blip_image_processor_name}\")\n",
    "try:\n",
    "    image_processor = BlipImageProcessor.from_pretrained(config.blip_image_processor_name)\n",
    "    print(\"BLIP Image Processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading image processor '{config.blip_image_processor_name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Attempting to load training data from: ./json_data/train.json\n",
      "Loading JSON metadata (this might take time for large datasets)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a98f0e72e446379a4e35518250c1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading JSONs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41236 samples total from 1 file(s).\n",
      "Dataset size after potential cleaning: 41236\n",
      "Using image target size: 384x384\n",
      "Attempting to load validation data from: ./json_data/dev.json\n",
      "Loading JSON metadata (this might take time for large datasets)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8615444de86a4c70891d70e2c8918adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading JSONs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10002 samples total from 1 file(s).\n",
      "Dataset size after potential cleaning: 10002\n",
      "Using image target size: 384x384\n",
      "\n",
      "Creating dataloaders...\n",
      "Using 20 workers for DataLoaders.\n",
      "Train loader created with 1288 batches.\n",
      "Total estimated training steps: 1006\n",
      "Validation loader created with 157 batches.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 9: Setup - Datasets and DataLoaders ===\n",
    "# Uses the dataset class defined above.\n",
    "\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "\n",
    "# Define paths\n",
    "validation_json_path = os.path.join(config.data_path, \"dev.json\")\n",
    "train_json_path = os.path.join(config.data_path, \"train.json\")\n",
    "\n",
    "if tokenizer and image_processor:\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    # --- Training Dataset ---\n",
    "    try:\n",
    "        print(f\"Attempting to load training data from: {train_json_path}\")\n",
    "        train_dataset = CustomImageCaptionDataset(\n",
    "            json_path_or_list=train_json_path,\n",
    "            image_base_path=config.image_base_path,\n",
    "            tokenizer=tokenizer, \n",
    "            image_processor=image_processor,\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "        if not train_dataset.data:\n",
    "            print(\"\\nERROR: Failed to load training data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating training dataset: {e}\")\n",
    "        train_dataset = None\n",
    "\n",
    "    # --- Validation Dataset ---\n",
    "    if os.path.exists(validation_json_path):\n",
    "        try:\n",
    "            print(f\"Attempting to load validation data from: {validation_json_path}\")\n",
    "            dev_dataset = CustomImageCaptionDataset(\n",
    "                json_path_or_list=validation_json_path,\n",
    "                image_base_path=config.image_base_path,\n",
    "                tokenizer=tokenizer,\n",
    "                image_processor=image_processor,\n",
    "                max_length=config.max_length\n",
    "            )\n",
    "            if not dev_dataset.data:\n",
    "                print(\"\\nWARNING: Failed to load validation data.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR creating validation dataset: {e}\")\n",
    "            dev_dataset = None\n",
    "    else:\n",
    "        print(\"Validation JSON path not found, skipping validation set creation.\")\n",
    "        dev_dataset = None\n",
    "\n",
    "    print(\"\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    if train_dataset and train_dataset.data:\n",
    "        # Check if persistent_workers is supported\n",
    "        persist_workers = (num_workers > 0)\n",
    "        try:\n",
    "            _ = DataLoader(train_dataset, num_workers=num_workers, persistent_workers=persist_workers)\n",
    "        except TypeError:\n",
    "            persist_workers = False\n",
    "            print(\"Note: `persistent_workers=True` not supported by this PyTorch version/DataLoader setup.\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=config.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=True,\n",
    "            persistent_workers=persist_workers\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "        # Calculate total training steps for Cosine Scheduler\n",
    "        config.total_training_steps = len(train_loader) * config.epochs // config.accumulation_steps\n",
    "        print(f\"Total estimated training steps: {config.total_training_steps}\")\n",
    "    else: \n",
    "        print(\"Skipping train loader creation (no data).\")\n",
    "\n",
    "    if dev_dataset and dev_dataset.data:\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset, \n",
    "            batch_size=config.batch_size * 2,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False,\n",
    "            persistent_workers=persist_workers if num_workers > 0 else False\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "    else: \n",
    "        print(\"Skipping validation loader creation.\")\n",
    "\n",
    "    if not train_loader: \n",
    "        print(\"\\nERROR: Train loader could not be created.\")\n",
    "else:\n",
    "    print(\"ERROR: Tokenizer or Image Processor not loaded. Skipping dataset/loader creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing ViSigLIP model components...\n",
      "Initializing BLIP Vision Encoder from: Salesforce/blip-image-captioning-base by loading BlipForImageTextRetrieval first.\n",
      "  Loading base BlipForImageTextRetrieval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipForImageTextRetrieval were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['itm_head.bias', 'itm_head.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.word_embeddings.weight', 'text_proj.bias', 'text_proj.weight', 'vision_proj.bias', 'vision_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracting vision_model from BlipForImageTextRetrieval.\n",
      "  Vision model extracted successfully.\n",
      "  Confirmed/Using vision model hidden size: 768\n",
      "  Added projection head: 768 -> 768\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Confirmed text model hidden size: 768\n",
      "  Added projection head: 768 -> 768\n",
      "Using learnable temperature, initialized to 10.0000\n",
      "Using learnable bias, initialized to -10.0000\n",
      "\n",
      "ViSigLIP Model initialized successfully on cuda.\n",
      "Total parameters: 222.27 M\n",
      "Trainable parameters: 222.27 M\n",
      "\n",
      "Setting up optimizer...\n",
      "Optimizer AdamW initialized with base LR: 0.0001, weight decay: 0.1\n",
      "LR Scheduler: Cosine with Warmup (1000 steps) initialized.\n",
      "AMP GradScaler initialized.\n",
      "Early stopping enabled with patience: 5\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10: Setup - Model, Optimizer, Scheduler ===\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "scaler = None # For AMP\n",
    "\n",
    "print(\"\\nInitializing ViSigLIP model components...\")\n",
    "try:\n",
    "    # Instantiate the encoders and main model\n",
    "    image_encoder = ImageEncoder(config).to(config.device)\n",
    "    text_encoder = TextEncoder(config).to(config.device)\n",
    "    model = ViSigLIPModel(image_encoder, text_encoder, config).to(config.device)\n",
    "\n",
    "    print(f\"\\nViSigLIP Model initialized successfully on {config.device}.\")\n",
    "    num_params_total = sum(p.numel() for p in model.parameters())\n",
    "    num_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {num_params_total / 1e6:.2f} M\")\n",
    "    print(f\"Trainable parameters: {num_params_trainable / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing model components: {e}\")\n",
    "    traceback.print_exc()\n",
    "    model = None\n",
    "\n",
    "if model and train_loader:\n",
    "    print(\"\\nSetting up optimizer...\")\n",
    "    # --- Optimizer ---\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and p.requires_grad], 'weight_decay': config.weight_decay},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) and p.requires_grad], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=config.learning_rate)\n",
    "    print(f\"Optimizer AdamW initialized with base LR: {config.learning_rate}, weight decay: {config.weight_decay}\")\n",
    "\n",
    "    # --- LR Scheduler ---\n",
    "    if config.scheduler_type == \"cosine\":\n",
    "        if hasattr(config, 'total_training_steps'):\n",
    "            lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=config.warmup_steps,\n",
    "                num_training_steps=config.total_training_steps\n",
    "            )\n",
    "            print(f\"LR Scheduler: Cosine with Warmup ({config.warmup_steps} steps) initialized.\")\n",
    "        else:\n",
    "            print(\"ERROR: total_training_steps not calculated. Cannot init Cosine scheduler.\")\n",
    "            lr_scheduler = None\n",
    "    elif config.scheduler_type == \"reduce_on_plateau\":\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode=config.mode, factor=config.rop_factor, patience=config.rop_patience\n",
    "        )\n",
    "        print(f\"LR Scheduler: ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.rop_factor}, patience={config.rop_patience})\")\n",
    "    else:\n",
    "        print(\"No LR Scheduler specified.\")\n",
    "        lr_scheduler = None\n",
    "\n",
    "    # --- Automatic Mixed Precision (AMP) Scaler ---\n",
    "    if config.use_amp:\n",
    "        scaler = torch.amp.GradScaler('cuda')\n",
    "        print(\"AMP GradScaler initialized.\")\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    # Early stopping setup\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = config.early_stopping_patience if hasattr(config, 'early_stopping_patience') else 5\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    print(f\"Early stopping enabled with patience: {early_stopping_patience}\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not initialized or train_loader not available. Skipping optimizer/scheduler setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step and validation epoch functions defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 11: Training and Validation Functions (Adapted for SigLIP & Steps) ===\n",
    "import traceback\n",
    "\n",
    "def train_step(model, batch, optimizer, scaler, device, use_amp):\n",
    "    \"\"\" Performs a single training step with SigLIP loss and optional AMP \"\"\"\n",
    "    model.train() # Set model to training mode\n",
    "\n",
    "    pixel_values = batch['pixel_values']#.to(device) # Moved device transfer to model forward\n",
    "    input_ids = batch['input_ids']#.to(device)\n",
    "    attention_mask = batch['attention_mask']#.to(device)\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        # Get features, temp, bias from model\n",
    "        image_features, text_features, temp, bias = model(pixel_values, input_ids, attention_mask)\n",
    "        # Calculate SigLIP loss\n",
    "        loss = siglip_loss(image_features, text_features, temp, bias)\n",
    "\n",
    "    if use_amp:\n",
    "        scaler.scale(loss).backward()\n",
    "    else:\n",
    "        loss.backward()\n",
    "\n",
    "    return loss.item() # Return scalar loss\n",
    "\n",
    "def validate_epoch(model, dataloader, device):\n",
    "    \"\"\" Performs validation, returning metrics \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Validation\", leave=False, unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'] #.to(device) # Moved to model forward\n",
    "            input_ids = batch['input_ids'] #.to(device)\n",
    "            attention_mask = batch['attention_mask'] #.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=config.use_amp): # Use AMP for validation inference too\n",
    "                # Get features BUT ignore temp/bias for metric calculation\n",
    "                # Metrics are calculated on raw (normalized) similarity\n",
    "                image_embeds_norm, text_embeds_norm, _, _ = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "            # Collect normalized embeddings on CPU to save GPU memory\n",
    "            all_image_embeddings.append(image_embeds_norm.cpu())\n",
    "            all_text_embeddings.append(text_embeds_norm.cpu())\n",
    "\n",
    "    if not all_image_embeddings or not all_text_embeddings:\n",
    "         print(\"Warning: No embeddings collected during validation.\")\n",
    "         # Return default zero metrics, maybe fetch loss if calculated differently\n",
    "         return { \"loss\": float('inf'), \"avg_acc\": 0.0, \"avg_cosine_sim\": 0.0,\n",
    "                  \"i2t recall R@1\": 0.0, \"i2t recall R@5\": 0.0, \"i2t recall R@10\": 0.0,\n",
    "                  \"t2i recall R@1\": 0.0, \"t2i recall R@5\": 0.0, \"t2i recall R@10\": 0.0 }\n",
    "\n",
    "    # Concatenate all embeddings\n",
    "    try:\n",
    "        all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "        all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error concatenating embeddings: {e}\")\n",
    "        # Handle cases where validation might have yielded inconsistent batch sizes if drop_last=False\n",
    "        return { \"loss\": float('inf'), \"avg_acc\": 0.0, \"avg_cosine_sim\": 0.0,\n",
    "                  \"i2t recall R@1\": 0.0, \"i2t recall R@5\": 0.0, \"i2t recall R@10\": 0.0,\n",
    "                  \"t2i recall R@1\": 0.0, \"t2i recall R@5\": 0.0, \"t2i recall R@10\": 0.0 }\n",
    "\n",
    "\n",
    "    print(f\"\\\\nComputing metrics over {all_image_embeddings.shape[0]} validation samples...\")\n",
    "    validation_metrics = compute_metrics(all_image_embeddings.to(device), all_text_embeddings.to(device))\n",
    "\n",
    "    # Format results\n",
    "    final_results = {}\n",
    "    for k, v in validation_metrics.items():\n",
    "        if isinstance(v, dict):\n",
    "            for recall_k, recall_v in v.items(): final_results[f\"{k.replace('_', ' ')} {recall_k}\"] = recall_v\n",
    "        else: final_results[k.replace('_', ' ')] = v\n",
    "\n",
    "    # Clean up memory\n",
    "    del all_image_embeddings\n",
    "    del all_text_embeddings\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "    return final_results\n",
    "\n",
    "print(\"Training step and validation epoch functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting ViSigLIP pretraining for 50 epochs...\n",
      "Target metric for saving best model: 'avg_acc' (mode: max)\n",
      "\n",
      "--- Epoch 1/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229412949c0d4a2fa678cede3ede48bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E1:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1562477/955622713.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 1000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26038a4ca97e4828979af05108a3c4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1562477/955622713.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=config.use_amp): # Use AMP for validation inference too\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.91s\n",
      "Validation Step 1000: avg acc: 0.0001 | avg cosine sim: 0.0703 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' improved from -inf to 0.0001. Saving best model.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_1000.pt\n",
      "--- Epoch 1 Time: 0:06:27.937303 ---\n",
      "\n",
      "--- Epoch 2/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d8c6cdc49f448c9dd1a5dcf9b9035d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E2:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 2000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f9a8e6030f48d4affe773925820439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.18s\n",
      "Validation Step 2000: avg acc: 0.0001 | avg cosine sim: 0.4176 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_2000.pt\n",
      "--- Epoch 2 Time: 0:06:23.407286 ---\n",
      "\n",
      "--- Epoch 3/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b7182a47c44d96bc57a774393eac8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E3:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 3000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248cfaf8b3504d728fa5c323e1ef471c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.26s\n",
      "Validation Step 3000: avg acc: 0.0001 | avg cosine sim: 0.7140 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_3000.pt\n",
      "--- Epoch 3 Time: 0:06:23.708715 ---\n",
      "\n",
      "--- Epoch 4/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f7f78ee37e4488b7c6901346c5f40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E4:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 4000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4802cbdaf549e4a4419ae1aa608237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.30s\n",
      "Validation Step 4000: avg acc: 0.0001 | avg cosine sim: 0.7147 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_4000.pt\n",
      "\n",
      "Running validation at step 5000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a4f77d640242a3a5c24eb473de3314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.07s\n",
      "Validation Step 5000: avg acc: 0.0001 | avg cosine sim: 0.6975 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_5000.pt\n",
      "--- Epoch 4 Time: 0:06:55.078366 ---\n",
      "\n",
      "--- Epoch 5/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c21cbb2ec8a48d490ee5b9ed62ddd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E5:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 6000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd04899dc694b068317a39d9ab9d106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.38s\n",
      "Validation Step 6000: avg acc: 0.0001 | avg cosine sim: 0.6999 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_6000.pt\n",
      "--- Epoch 5 Time: 0:06:23.837935 ---\n",
      "\n",
      "--- Epoch 6/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94dd4d727b0c4e20be29ead807f24270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E6:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 7000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e4fb48cdfd48588e8d3a7656eb2680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.15s\n",
      "Validation Step 7000: avg acc: 0.0001 | avg cosine sim: 0.6958 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_7000.pt\n",
      "--- Epoch 6 Time: 0:06:23.539023 ---\n",
      "\n",
      "--- Epoch 7/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79a493b66a440339d628d917de569ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E7:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 8000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d794bda27b4b14b63a22ec280a476e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.23s\n",
      "Validation Step 8000: avg acc: 0.0001 | avg cosine sim: 0.6935 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_8000.pt\n",
      "\n",
      "Running validation at step 9000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401ecd0f196c45c6b4b544f1209fa15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.16s\n",
      "Validation Step 9000: avg acc: 0.0001 | avg cosine sim: 0.6918 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_9000.pt\n",
      "--- Epoch 7 Time: 0:06:55.265229 ---\n",
      "\n",
      "--- Epoch 8/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39027b972b594c08a5215358b3468280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E8:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 10000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8fdf6c145a483db5b94393e4efddda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.19s\n",
      "Validation Step 10000: avg acc: 0.0001 | avg cosine sim: 0.6905 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_10000.pt\n",
      "--- Epoch 8 Time: 0:06:23.542316 ---\n",
      "\n",
      "--- Epoch 9/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47a6107cea344af9f3a964558691021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E9:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 11000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62242c2f20d343f8a72ae368722f1ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.37s\n",
      "Validation Step 11000: avg acc: 0.0001 | avg cosine sim: 0.6902 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_11000.pt\n",
      "--- Epoch 9 Time: 0:06:23.984508 ---\n",
      "\n",
      "--- Epoch 10/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275ed73c71d746abbe02728a105abe9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E10:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 12000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f277586cfc454cc8a3fbedfef14016b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.34s\n",
      "Validation Step 12000: avg acc: 0.0001 | avg cosine sim: 0.6889 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_12000.pt\n",
      "--- Epoch 10 Time: 0:06:23.788190 ---\n",
      "\n",
      "--- Epoch 11/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26370c17e234e6fa4c2fca86691af88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E11:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 13000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388b88a3fc3943cda959807fa42a28a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.41s\n",
      "Validation Step 13000: avg acc: 0.0001 | avg cosine sim: 0.6884 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_13000.pt\n",
      "\n",
      "Running validation at step 14000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984a6eb82fbd47909eec934049516b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.57s\n",
      "Validation Step 14000: avg acc: 0.0001 | avg cosine sim: 0.6895 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_14000.pt\n",
      "--- Epoch 11 Time: 0:06:55.968926 ---\n",
      "\n",
      "--- Epoch 12/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7655992eb5d54ddf9496f625d3a494b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E12:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 15000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ea686db8bf42c4b1c0974aea4f21e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.30s\n",
      "Validation Step 15000: avg acc: 0.0001 | avg cosine sim: 0.6867 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_15000.pt\n",
      "--- Epoch 12 Time: 0:06:23.823886 ---\n",
      "\n",
      "--- Epoch 13/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0306a75c38243ee98f5d50081cc7c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E13:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 16000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d73da72fa7645a28ab31e062293d382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.20s\n",
      "Validation Step 16000: avg acc: 0.0001 | avg cosine sim: 0.6856 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_16000.pt\n",
      "--- Epoch 13 Time: 0:06:23.649000 ---\n",
      "\n",
      "--- Epoch 14/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd735214ffb46a3906b725965ed6491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E14:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 17000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719c698fa61d472cb1a1234bb32c8bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.17s\n",
      "Validation Step 17000: avg acc: 0.0001 | avg cosine sim: 0.6863 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_17000.pt\n",
      "\n",
      "Running validation at step 18000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e77dca3a7a0414b81dda78a66fef225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.37s\n",
      "Validation Step 18000: avg acc: 0.0001 | avg cosine sim: 0.6882 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_18000.pt\n",
      "--- Epoch 14 Time: 0:06:55.405091 ---\n",
      "\n",
      "--- Epoch 15/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799aeb9569774077ae344f5d1fea5feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E15:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 19000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d7c677742a4050903ebd7d6b9b398c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 29.99s\n",
      "Validation Step 19000: avg acc: 0.0001 | avg cosine sim: 0.6881 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_19000.pt\n",
      "--- Epoch 15 Time: 0:06:23.496486 ---\n",
      "\n",
      "--- Epoch 16/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0652694e7a4cdf95945a4aa71e1f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E16:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 20000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326c9f75cd734afab10d4a868be8b524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.23s\n",
      "Validation Step 20000: avg acc: 0.0001 | avg cosine sim: 0.6861 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_20000.pt\n",
      "--- Epoch 16 Time: 0:06:23.625420 ---\n",
      "\n",
      "--- Epoch 17/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda9a620e5a2412e8ffe545e7b0de5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E17:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 21000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786e321e418748fd8968356921d0683d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.45s\n",
      "Validation Step 21000: avg acc: 0.0001 | avg cosine sim: 0.6838 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_21000.pt\n",
      "--- Epoch 17 Time: 0:06:24.017399 ---\n",
      "\n",
      "--- Epoch 18/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de64558eb374cda9c911b2ba99878ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E18:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 22000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c85462f58e24836ac75c473aa287993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.36s\n",
      "Validation Step 22000: avg acc: 0.0001 | avg cosine sim: 0.6836 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_22000.pt\n",
      "\n",
      "Running validation at step 23000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5160fed1a62e4fd29ae8258652d74b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.10s\n",
      "Validation Step 23000: avg acc: 0.0001 | avg cosine sim: 0.6819 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_23000.pt\n",
      "--- Epoch 18 Time: 0:06:55.363130 ---\n",
      "\n",
      "--- Epoch 19/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e117b5b7784ddf8fc1b9eb06d66931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E19:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 24000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942491d1389640dcb9a707f7883cff9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.13s\n",
      "Validation Step 24000: avg acc: 0.0001 | avg cosine sim: 0.6855 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_24000.pt\n",
      "--- Epoch 19 Time: 0:06:23.754922 ---\n",
      "\n",
      "--- Epoch 20/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8179021198403cb8837b982eaf367f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E20:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 25000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c3c0d9c3ce4d30a1ab6cd24afc0394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.08s\n",
      "Validation Step 25000: avg acc: 0.0001 | avg cosine sim: 0.6860 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_25000.pt\n",
      "--- Epoch 20 Time: 0:06:23.480805 ---\n",
      "\n",
      "--- Epoch 21/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17141f566fa45e483ae493df34f880b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E21:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 26000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ebbdc51248401e9b93697ed4a5ea0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.38s\n",
      "Validation Step 26000: avg acc: 0.0001 | avg cosine sim: 0.6806 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_26000.pt\n",
      "\n",
      "Running validation at step 27000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a30385ece49461e870e9e0df2d0c191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.36s\n",
      "Validation Step 27000: avg acc: 0.0001 | avg cosine sim: 0.6847 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_27000.pt\n",
      "--- Epoch 21 Time: 0:06:55.574708 ---\n",
      "\n",
      "--- Epoch 22/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de204ffeed524465a2875ae52e02aa8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E22:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 28000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761e78e0024d415c98d245031beea82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.32s\n",
      "Validation Step 28000: avg acc: 0.0001 | avg cosine sim: 0.6822 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_28000.pt\n",
      "--- Epoch 22 Time: 0:06:23.896525 ---\n",
      "\n",
      "--- Epoch 23/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492c3e67ac614327be0369e53365f7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E23:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 29000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b97260cb57843a096863cd1e8d5d8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.36s\n",
      "Validation Step 29000: avg acc: 0.0001 | avg cosine sim: 0.6806 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_29000.pt\n",
      "--- Epoch 23 Time: 0:06:23.876587 ---\n",
      "\n",
      "--- Epoch 24/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8b538df54a40898c9f4abca3d4d983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E24:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 30000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562898fa18e84daca61a047a5d9ad7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.28s\n",
      "Validation Step 30000: avg acc: 0.0001 | avg cosine sim: 0.6815 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_30000.pt\n",
      "--- Epoch 24 Time: 0:06:23.633214 ---\n",
      "\n",
      "--- Epoch 25/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab92239a9edd4c378ce0210df727284c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E25:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 31000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16adb8914a4f4b3faf93df82d9423756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.39s\n",
      "Validation Step 31000: avg acc: 0.0001 | avg cosine sim: 0.6834 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_31000.pt\n",
      "\n",
      "Running validation at step 32000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4640253d65724d01bf62774e7358b13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.37s\n",
      "Validation Step 32000: avg acc: 0.0001 | avg cosine sim: 0.6810 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_32000.pt\n",
      "--- Epoch 25 Time: 0:06:55.470729 ---\n",
      "\n",
      "--- Epoch 26/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45211af0ba945b99e55417691562db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E26:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 33000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8313a50089364ddab390a1a80faf462c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.43s\n",
      "Validation Step 33000: avg acc: 0.0001 | avg cosine sim: 0.6807 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_33000.pt\n",
      "--- Epoch 26 Time: 0:06:24.102100 ---\n",
      "\n",
      "--- Epoch 27/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952971e24dc34c1bb9a5d2e4a55bb3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E27:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 34000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ff4ff4509847ebb5f6a9de2932caae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.31s\n",
      "Validation Step 34000: avg acc: 0.0001 | avg cosine sim: 0.6807 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_34000.pt\n",
      "--- Epoch 27 Time: 0:06:23.859852 ---\n",
      "\n",
      "--- Epoch 28/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89605ef821794e0bb17676f6fd58efaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E28:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 35000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80fc1dc62ea40a7b15b066a7f861266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.25s\n",
      "Validation Step 35000: avg acc: 0.0001 | avg cosine sim: 0.6802 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_35000.pt\n",
      "\n",
      "Running validation at step 36000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32f609d5d1d48fbbfd67fd0a6da83ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.27s\n",
      "Validation Step 36000: avg acc: 0.0001 | avg cosine sim: 0.6795 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_36000.pt\n",
      "--- Epoch 28 Time: 0:06:55.281450 ---\n",
      "\n",
      "--- Epoch 29/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dba4726105041ec823b909ed6d44ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E29:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 37000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126973a61e2240199e947f09206e0ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.35s\n",
      "Validation Step 37000: avg acc: 0.0001 | avg cosine sim: 0.6826 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_37000.pt\n",
      "--- Epoch 29 Time: 0:06:23.893634 ---\n",
      "\n",
      "--- Epoch 30/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0429abaa1446a1952c483e3e75d85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E30:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 38000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c989bd2df2154f139135dd875cfd0050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.36s\n",
      "Validation Step 38000: avg acc: 0.0001 | avg cosine sim: 0.6795 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_38000.pt\n",
      "--- Epoch 30 Time: 0:06:23.889092 ---\n",
      "\n",
      "--- Epoch 31/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052c761bbdb54fd69df993245fb6f8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E31:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 39000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360e27bedfc54a83a40fdc1658540ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.27s\n",
      "Validation Step 39000: avg acc: 0.0001 | avg cosine sim: 0.6794 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_39000.pt\n",
      "--- Epoch 31 Time: 0:06:23.670665 ---\n",
      "\n",
      "--- Epoch 32/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8b314d6b8744f8b67d4e8f629b9b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E32:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 40000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65ac9174ad049688ec58f79e3d3523f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.46s\n",
      "Validation Step 40000: avg acc: 0.0001 | avg cosine sim: 0.6803 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_40000.pt\n",
      "\n",
      "Running validation at step 41000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e075333ae0d04484af7c6616718e21b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.18s\n",
      "Validation Step 41000: avg acc: 0.0001 | avg cosine sim: 0.6806 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_41000.pt\n",
      "--- Epoch 32 Time: 0:06:55.467594 ---\n",
      "\n",
      "--- Epoch 33/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff78cb9868fa43e0920d8b06cfca0c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E33:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 42000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253ed0e220e3464798dc960568915fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.19s\n",
      "Validation Step 42000: avg acc: 0.0001 | avg cosine sim: 0.6766 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_42000.pt\n",
      "--- Epoch 33 Time: 0:06:23.640095 ---\n",
      "\n",
      "--- Epoch 34/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ef96d1b6fd48218939940b56be330a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E34:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 43000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b9880f62a5454c859348646e8c2a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.39s\n",
      "Validation Step 43000: avg acc: 0.0001 | avg cosine sim: 0.6795 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_43000.pt\n",
      "--- Epoch 34 Time: 0:06:23.834829 ---\n",
      "\n",
      "--- Epoch 35/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2b2f09ae0b47a6894fcfd205885f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E35:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 44000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89b66a82e2d4a43a3a571fbbec6313f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.35s\n",
      "Validation Step 44000: avg acc: 0.0001 | avg cosine sim: 0.6797 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_44000.pt\n",
      "\n",
      "Running validation at step 45000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d02c9cefdd4abf9da94ef30ad8bc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.23s\n",
      "Validation Step 45000: avg acc: 0.0001 | avg cosine sim: 0.6785 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_45000.pt\n",
      "--- Epoch 35 Time: 0:06:55.406744 ---\n",
      "\n",
      "--- Epoch 36/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae9acb9d13f4872bd4381706e464684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E36:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 46000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4b3d4a94f24dd3bbd94e3810aacf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.27s\n",
      "Validation Step 46000: avg acc: 0.0001 | avg cosine sim: 0.6787 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_46000.pt\n",
      "--- Epoch 36 Time: 0:06:23.697371 ---\n",
      "\n",
      "--- Epoch 37/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ec6062b09c4a68b7ba80d0525ef930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E37:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 47000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f3fe87010e4b528b99ba7b1516e362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.28s\n",
      "Validation Step 47000: avg acc: 0.0001 | avg cosine sim: 0.6657 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_47000.pt\n",
      "--- Epoch 37 Time: 0:06:23.582286 ---\n",
      "\n",
      "--- Epoch 38/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465dc49770db41cb8fc163892af84b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E38:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 48000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51cd0dd6655e4811bb21b45742c07ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.34s\n",
      "Validation Step 48000: avg acc: 0.0001 | avg cosine sim: 0.6866 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_48000.pt\n",
      "--- Epoch 38 Time: 0:06:23.883169 ---\n",
      "\n",
      "--- Epoch 39/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818e0ed72e87478cbdaff33ba8a071cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E39:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 49000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55633f42a5f644e8b5650a9bf5140916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.20s\n",
      "Validation Step 49000: avg acc: 0.0001 | avg cosine sim: 0.6826 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_49000.pt\n",
      "\n",
      "Running validation at step 50000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddbb96b192c469baff628329f8b0795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.04s\n",
      "Validation Step 50000: avg acc: 0.0001 | avg cosine sim: 0.6709 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_50000.pt\n",
      "--- Epoch 39 Time: 0:06:54.930866 ---\n",
      "\n",
      "--- Epoch 40/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238ca101aa2b49b58cea2b590ccb7113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E40:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 51000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21eef91d4fa94d0084dca32f7fa55836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.24s\n",
      "Validation Step 51000: avg acc: 0.0001 | avg cosine sim: 0.6726 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_51000.pt\n",
      "--- Epoch 40 Time: 0:06:23.778524 ---\n",
      "\n",
      "--- Epoch 41/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6c5585198b467caa8805486b0d9a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E41:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 52000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4656af0b34c64fb5a84a68b79db39d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.24s\n",
      "Validation Step 52000: avg acc: 0.0001 | avg cosine sim: 0.6722 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_52000.pt\n",
      "--- Epoch 41 Time: 0:06:23.720734 ---\n",
      "\n",
      "--- Epoch 42/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b506c0a84a4e738fbc0de882baff15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E42:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 53000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b58543adda04b82974ea1c3e8f26937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.27s\n",
      "Validation Step 53000: avg acc: 0.0001 | avg cosine sim: 0.6723 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_53000.pt\n",
      "\n",
      "Running validation at step 54000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb183ddaace4ea98886ebd285184ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.11s\n",
      "Validation Step 54000: avg acc: 0.0001 | avg cosine sim: 0.6714 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_54000.pt\n",
      "--- Epoch 42 Time: 0:06:55.139631 ---\n",
      "\n",
      "--- Epoch 43/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57325d73bdfb424ba17c6cc86113d75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E43:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 55000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fce654b33f481fb77b3249c3d680c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.29s\n",
      "Validation Step 55000: avg acc: 0.0001 | avg cosine sim: 0.6725 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_55000.pt\n",
      "--- Epoch 43 Time: 0:06:23.758763 ---\n",
      "\n",
      "--- Epoch 44/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf030d9cd7d4e1f8181fd57c8a885fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E44:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 56000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db942d73bc94e70b5c0d75136934d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.11s\n",
      "Validation Step 56000: avg acc: 0.0001 | avg cosine sim: 0.6723 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_56000.pt\n",
      "--- Epoch 44 Time: 0:06:23.439123 ---\n",
      "\n",
      "--- Epoch 45/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53f4c0d78bc4102a87b2fbd03413f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E45:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 57000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5201c386ad12455e91482cb1b3bd27da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.13s\n",
      "Validation Step 57000: avg acc: 0.0001 | avg cosine sim: 0.6718 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_57000.pt\n",
      "--- Epoch 45 Time: 0:06:23.599174 ---\n",
      "\n",
      "--- Epoch 46/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe44a26bf55472b907c19d51e69f99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E46:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 58000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709436c5a8ed46cb97fa52c6a1fc0a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.35s\n",
      "Validation Step 58000: avg acc: 0.0001 | avg cosine sim: 0.6721 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_58000.pt\n",
      "\n",
      "Running validation at step 59000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59fea918a8314f3dbdc9dc0cd4ecb920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.45s\n",
      "Validation Step 59000: avg acc: 0.0001 | avg cosine sim: 0.6714 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_59000.pt\n",
      "--- Epoch 46 Time: 0:06:55.467820 ---\n",
      "\n",
      "--- Epoch 47/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feebd31a4e3c4681a59f520cf0f7cc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E47:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 60000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dd850e1cd94046b67b7d66ebcc1fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.17s\n",
      "Validation Step 60000: avg acc: 0.0001 | avg cosine sim: 0.6716 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_60000.pt\n",
      "--- Epoch 47 Time: 0:06:23.441057 ---\n",
      "\n",
      "--- Epoch 48/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cd2ac89c664071af61074d2c52bc39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E48:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 61000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e96d439d2c0457d83ff8b7ddc87fc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.35s\n",
      "Validation Step 61000: avg acc: 0.0001 | avg cosine sim: 0.6718 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_61000.pt\n",
      "--- Epoch 48 Time: 0:06:23.803608 ---\n",
      "\n",
      "--- Epoch 49/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693e186ce32c4cfa8ca68fa12c6b0e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E49:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 62000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f210fa2c97e493d977de8ddb6fd94ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.38s\n",
      "Validation Step 62000: avg acc: 0.0001 | avg cosine sim: 0.6718 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_62000.pt\n",
      "\n",
      "Running validation at step 63000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c00309ba0343448f4d4517b3476c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.22s\n",
      "Validation Step 63000: avg acc: 0.0001 | avg cosine sim: 0.6721 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_63000.pt\n",
      "--- Epoch 49 Time: 0:06:55.296951 ---\n",
      "\n",
      "--- Epoch 50/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcf7aa3e22d46b3b401eb8a06b449ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E50:   0%|          | 0/1288 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 64000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dbe3d52c5042829c7cd46b70ac8c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10002 validation samples...\n",
      "Validation finished in 30.35s\n",
      "Validation Step 64000: avg acc: 0.0001 | avg cosine sim: 0.6715 | i2t acc: 0.0001 | i2t recall R@1: 0.0001 | i2t recall R@10: 0.0010 | i2t recall R@5: 0.0005 | t2i acc: 0.0001 | t2i recall R@1: 0.0001 | t2i recall R@10: 0.0010 | t2i recall R@5: 0.0005\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0001.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_step_64000.pt\n",
      "--- Epoch 50 Time: 0:06:23.907305 ---\n",
      "=============== Pretraining Finished ================\n",
      "Total Training Time: 5:27:13.619622\n",
      "Final model state saved to ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_final.pt\n",
      "Best model based on 'avg_acc' (0.0001) is saved at: ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_best.pt\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "# === Cell 12: Pretraining Loop ===\n",
    "import datetime\n",
    "\n",
    "if model and train_loader and optimizer:  # Basic check\n",
    "    print(f\"\\nStarting ViSigLIP pretraining for {config.epochs} epochs...\")\n",
    "    print(f\"Target metric for saving best model: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    global_step = 0\n",
    "    total_loss_since_log = 0.0\n",
    "    steps_since_log = 0\n",
    "    start_train_time = time.time()\n",
    "    \n",
    "    # Store minimal history\n",
    "    history = {'steps': [], 'train_loss': [], 'val_metrics': {}}\n",
    "\n",
    "    model.train()  # Ensure model is in training mode initially\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Training E{epoch+1}\", leave=True, unit=\"batch\")\n",
    "\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            # Skip dummy batches\n",
    "            if batch['pixel_values'].shape[0] < config.batch_size and torch.all(batch['pixel_values'] == 0):\n",
    "                continue\n",
    "\n",
    "            loss = train_step(model, batch, optimizer, scaler, config.device, config.use_amp)\n",
    "            loss = loss / config.accumulation_steps  # Normalize loss for logging if accumulating\n",
    "\n",
    "            total_loss_since_log += loss\n",
    "            steps_since_log += 1\n",
    "\n",
    "            # Gradient Accumulation & Optimizer Step\n",
    "            if (global_step + 1) % config.accumulation_steps == 0:\n",
    "                if config.use_amp:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # LR Scheduler Step\n",
    "                if lr_scheduler:\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # Logging\n",
    "            if global_step % config.log_interval_steps == 0:\n",
    "                avg_loss = total_loss_since_log / steps_since_log\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\", lr=f\"{current_lr:.2e}\", step=f\"{global_step}\")\n",
    "                history['steps'].append(global_step)\n",
    "                history['train_loss'].append(avg_loss)\n",
    "                total_loss_since_log = 0.0\n",
    "                steps_since_log = 0\n",
    "\n",
    "            # Validation & Checkpointing\n",
    "            if dev_loader and global_step % config.validation_interval_steps == 0 and global_step > 0:\n",
    "                print(f\"\\nRunning validation at step {global_step}...\")\n",
    "                val_start_time = time.time()\n",
    "                val_results = validate_epoch(model, dev_loader, config.device)\n",
    "                val_end_time = time.time()\n",
    "                print(f\"Validation finished in {val_end_time - val_start_time:.2f}s\")\n",
    "\n",
    "                # Log validation metrics\n",
    "                metric_log_str = f\"  Validation Step {global_step}: \"\n",
    "                history['val_metrics'][global_step] = val_results\n",
    "                sorted_keys = sorted(val_results.keys())\n",
    "                for name in sorted_keys:\n",
    "                    metric_log_str += f\"{name}: {val_results[name]:.4f} | \"\n",
    "                print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "                # Save Checkpoint Logic\n",
    "                current_val_metric = val_results.get(config.metric_to_track.replace('_', ' '), None)\n",
    "                is_best = False\n",
    "                save_path = None\n",
    "                save_path_periodic = None\n",
    "                \n",
    "                if current_val_metric is not None:\n",
    "                    if config.mode == \"max\":\n",
    "                        is_best = current_val_metric > best_val_metric + config.early_stopping_min_delta\n",
    "                    else:  # min mode\n",
    "                        is_best = current_val_metric < best_val_metric - config.early_stopping_min_delta\n",
    "\n",
    "                    if is_best:\n",
    "                        print(f\"  Metric '{config.metric_to_track}' improved from {best_val_metric:.4f} to {current_val_metric:.4f}. Saving best model.\")\n",
    "                        best_val_metric = current_val_metric\n",
    "                        save_path = os.path.join(config.model_path, \"visiglip_pretrain_best.pt\")\n",
    "                    else:\n",
    "                        print(f\"  Metric '{config.metric_to_track}' did not improve. Best: {best_val_metric:.4f}.\")\n",
    "\n",
    "                    # Save checkpoint periodically\n",
    "                    if global_step % config.save_interval_steps == 0:\n",
    "                        periodic_save_path = os.path.join(config.model_path, f\"visiglip_pretrain_step_{global_step}.pt\")\n",
    "                        if save_path != periodic_save_path:  # Avoid saving twice if it's the best step\n",
    "                            print(f\"  Saving periodic checkpoint to {periodic_save_path}\")\n",
    "                            save_path_periodic = periodic_save_path\n",
    "                    \n",
    "                    # Prepare Save Dictionary\n",
    "                    if save_path or save_path_periodic:\n",
    "                        save_dict = {\n",
    "                            'step': global_step,\n",
    "                            'epoch': epoch + 1,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_metric': best_val_metric,\n",
    "                            'metric_tracked': config.metric_to_track,\n",
    "                            'current_val_metrics': val_results,\n",
    "                            'vision_model_name': config.blip_vision_model_name,\n",
    "                            'text_model_name': config.selected_text_model,\n",
    "                            'projection_dim': config.projection_dim,\n",
    "                            'learnable_temperature': config.learnable_temperature,\n",
    "                            'learnable_bias': config.learnable_bias,\n",
    "                            'max_length': config.max_length,\n",
    "                        }\n",
    "                        if lr_scheduler:\n",
    "                            save_dict['scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "                        if scaler:\n",
    "                            save_dict['scaler_state_dict'] = scaler.state_dict()\n",
    "\n",
    "                        # Save best model if condition met\n",
    "                        if save_path:\n",
    "                            torch.save(save_dict, save_path)\n",
    "                        # Save periodic checkpoint if condition met\n",
    "                        if save_path_periodic:\n",
    "                            torch.save(save_dict, save_path_periodic)\n",
    "                else:\n",
    "                    print(f\"  Warning: Metric '{config.metric_to_track}' not found in validation results. Cannot determine best model.\")\n",
    "\n",
    "                # Reset model to train mode after validation\n",
    "                model.train()\n",
    "\n",
    "        # End of Epoch\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {datetime.timedelta(seconds=epoch_end_time - epoch_start_time)} ---\")\n",
    "\n",
    "    # End of Training\n",
    "    end_train_time = time.time()\n",
    "    total_duration = datetime.timedelta(seconds=end_train_time - start_train_time)\n",
    "    print(f\"=============== Pretraining Finished ================\")\n",
    "    print(f\"Total Training Time: {total_duration}\")\n",
    "\n",
    "    # Save final model state\n",
    "    final_model_path = os.path.join(config.model_path, 'visiglip_pretrain_final.pt')\n",
    "    final_save_dict = {\n",
    "        'step': global_step,\n",
    "        'epoch': config.epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_metric': best_val_metric,\n",
    "        'metric_tracked': config.metric_to_track,\n",
    "        'vision_model_name': config.blip_vision_model_name,\n",
    "        'text_model_name': config.selected_text_model,\n",
    "        'projection_dim': config.projection_dim,\n",
    "        'learnable_temperature': config.learnable_temperature,\n",
    "        'learnable_bias': config.learnable_bias,\n",
    "        'max_length': config.max_length,\n",
    "    }\n",
    "    if lr_scheduler:\n",
    "        final_save_dict['scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "    if scaler:\n",
    "        final_save_dict['scaler_state_dict'] = scaler.state_dict()\n",
    "    torch.save(final_save_dict, final_model_path)\n",
    "    print(f\"Final model state saved to {final_model_path}\")\n",
    "\n",
    "    best_model_file = os.path.join(config.model_path, \"visiglip_pretrain_best.pt\")\n",
    "    if dev_loader and os.path.exists(best_model_file):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) is saved at: {best_model_file}\")\n",
    "    elif dev_loader:\n",
    "        print(\"Best model checkpoint file not found (or validation was skipped).\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training (model, dataloader, optimizer) not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=============== Starting Test Set Evaluation ===============\n",
      "Loading test data from: ./json_data/test.json\n",
      "Loading JSON metadata (this might take time for large datasets)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622ca019ac8d4a869b15cb71d80665cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading JSONs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10001 samples total from 1 file(s).\n",
      "Dataset size after potential cleaning: 10001\n",
      "Using image target size: 384x384\n",
      "Test loader created with 157 batches.\n",
      "\\nLoading best model: ./trained_models/ViSigLIP_uitopenviic/visiglip_pretrain_best.pt\n",
      "Re-creating model structure for testing...\n",
      "  Using Vision Source: Salesforce/blip-image-captioning-base\n",
      "  Using Text Model: vinai/phobert-base\n",
      "Initializing BLIP Vision Encoder from: Salesforce/blip-image-captioning-base by loading BlipForImageTextRetrieval first.\n",
      "  Loading base BlipForImageTextRetrieval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipForImageTextRetrieval were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['itm_head.bias', 'itm_head.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.word_embeddings.weight', 'text_proj.bias', 'text_proj.weight', 'vision_proj.bias', 'vision_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracting vision_model from BlipForImageTextRetrieval.\n",
      "  Vision model extracted successfully.\n",
      "  Confirmed/Using vision model hidden size: 768\n",
      "  Added projection head: 768 -> 768\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Confirmed text model hidden size: 768\n",
      "  Added projection head: 768 -> 768\n",
      "Using learnable temperature, initialized to 10.0000\n",
      "Using learnable bias, initialized to -10.0000\n",
      "  State dict loading result: <All keys matched successfully>\n",
      "Model weights loaded successfully.\n",
      "\\nRunning evaluation on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8be41ef30764254b211373dedfa3ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1562477/955622713.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=config.use_amp): # Use AMP for validation inference too\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComputing metrics over 10001 validation samples...\n",
      "\\n--- Test Set Results ---\n",
      "avg acc: 0.0001\\n  avg cosine sim: 0.0703\\n  i2t acc: 0.0001\\n  i2t recall R@1: 0.0001\\n  i2t recall R@10: 0.0010\\n  i2t recall R@5: 0.0005\\n  t2i acc: 0.0001\\n  t2i recall R@1: 0.0001\\n  t2i recall R@10: 0.0010\\n  t2i recall R@5: 0.0005\\n\n",
      "------------------------\n",
      "\\n================= Evaluation Finished ==================\n"
     ]
    }
   ],
   "source": [
    "# === Cell 13: Final Evaluation on Test Set (Optional) ===\n",
    "# Needs a separate test set definition (similar to validation setup in Cell 9)\n",
    "# Uses the validation function for evaluation logic\n",
    "\n",
    "# --- Define Test Set Paths ---\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "test_image_path = config.image_base_path\n",
    "\n",
    "print(\"\\\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_loader = None\n",
    "model_to_test = None\n",
    "\n",
    "# 1. Check prerequisites & Create Test Loader\n",
    "if os.path.exists(test_json_path) and 'tokenizer' in globals() and tokenizer and 'image_processor' in globals() and image_processor:\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    try:\n",
    "        test_dataset = CustomImageCaptionDataset(\n",
    "            json_path_or_list=test_json_path, image_base_path=test_image_path,\n",
    "            tokenizer=tokenizer, image_processor=image_processor,\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "        if test_dataset.data:\n",
    "            num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            persist_workers_test = (num_workers > 0) # Check support again\n",
    "            try: _ = DataLoader(test_dataset, num_workers=num_workers, persistent_workers=persist_workers_test)\n",
    "            except TypeError: persist_workers_test = False\n",
    "\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=config.batch_size * 2, shuffle=False, # Larger batch, no shuffle\n",
    "                num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "                drop_last=False, persistent_workers=persist_workers_test\n",
    "            )\n",
    "            print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "        else: print(\"Test dataset loaded but is empty.\")\n",
    "    except Exception as e: print(f\"Error creating test dataset/loader: {e}\")\n",
    "else: print(\"Skipping test evaluation: Test JSON, Tokenizer or Image Processor not found/loaded.\")\n",
    "\n",
    "\n",
    "# 2. Load Best Model for Testing\n",
    "if test_loader:\n",
    "    try:\n",
    "        best_model_path = os.path.join(config.model_path, \"visiglip_pretrain_best.pt\")\n",
    "        # Fallback to final if best doesn't exist\n",
    "        final_model_path = os.path.join(config.model_path, \"visiglip_pretrain_final.pt\")\n",
    "        load_path = None\n",
    "\n",
    "        if os.path.exists(best_model_path):\n",
    "             load_path = best_model_path\n",
    "             print(f\"\\\\nLoading best model: {load_path}\")\n",
    "        elif os.path.exists(final_model_path):\n",
    "             load_path = final_model_path\n",
    "             print(f\"\\\\nLoading final model (best not found): {load_path}\")\n",
    "        else:\n",
    "            print(f\"\\\\nWARNING: No checkpoints found in {config.model_path} to evaluate.\")\n",
    "\n",
    "        if load_path:\n",
    "            checkpoint = torch.load(load_path, map_location=config.device)\n",
    "            print(\"Re-creating model structure for testing...\")\n",
    "\n",
    "            # --- Create a temporary config based on saved checkpoint ---\n",
    "            # Use attribute names expected by the Encoder/Model __init__ methods\n",
    "            from types import SimpleNamespace\n",
    "            temp_config_dict = {\n",
    "                'device': config.device, # Use current device\n",
    "                'blip_vision_model_name': checkpoint.get('vision_model_name', config.selected_vision_source),\n",
    "                'vision_embedding': config.vision_embedding, # Match the current CFG base size\n",
    "                'selected_text_model': checkpoint.get('text_model_name', config.selected_text_model),\n",
    "                'text_embedding': config.text_embedding, # Match the current CFG base size\n",
    "                'projection_dim': checkpoint.get('projection_dim', config.projection_dim),\n",
    "                'learnable_temperature': checkpoint.get('learnable_temperature', config.learnable_temperature),\n",
    "                'temperature_init': config.temperature_init, # Use CFG init value\n",
    "                'learnable_bias': checkpoint.get('learnable_bias', config.learnable_bias),\n",
    "                'bias_init': config.bias_init, # Use CFG init value\n",
    "            }\n",
    "            temp_config = SimpleNamespace(**temp_config_dict)\n",
    "\n",
    "            print(f\"  Using Vision Source: {temp_config.blip_vision_model_name}\")\n",
    "            print(f\"  Using Text Model: {temp_config.selected_text_model}\")\n",
    "\n",
    "            test_image_encoder = ImageEncoder(temp_config).to(config.device)\n",
    "            test_text_encoder = TextEncoder(temp_config).to(config.device)\n",
    "            model_to_test = ViSigLIPModel(test_image_encoder, test_text_encoder, temp_config).to(config.device)\n",
    "\n",
    "            # Load state dict carefully\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            # Handle potential 'module.' prefix if saved from DataParallel/DistributedDataParallel\n",
    "            if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "                print(\"Detected 'module.' prefix, removing.\")\n",
    "                from collections import OrderedDict\n",
    "                state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "\n",
    "            # Load with strict=False initially to see mismatched keys\n",
    "            load_result = model_to_test.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"  State dict loading result: {load_result}\")\n",
    "            if load_result.missing_keys: print(f\"  Warning: Missing keys: {load_result.missing_keys}\")\n",
    "            if load_result.unexpected_keys: print(f\"  Warning: Unexpected keys: {load_result.unexpected_keys}\")\n",
    "            print(f\"Model weights loaded successfully.\")\n",
    "\n",
    "            print(\"\\\\nRunning evaluation on test set...\")\n",
    "            # Use the validation function to compute metrics\n",
    "            test_results = validate_epoch(model_to_test, test_loader, config.device)\n",
    "\n",
    "            print(\"\\\\n--- Test Set Results ---\")\n",
    "            metric_log_str = \"\"\n",
    "            sorted_keys = sorted(test_results.keys())\n",
    "            for name in sorted_keys: metric_log_str += f\"  {name}: {test_results[name]:.4f}\\\\n\"\n",
    "            print(metric_log_str.strip())\n",
    "            print(\"------------------------\")\n",
    "        else:\n",
    "            print(\"Evaluation skipped (no weights found).\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\nERROR during test setup/evaluation: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\\\n================= Evaluation Finished ==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-based plot directory ensured at: /home/researcher/huypq69/TuningModels/train_plot/ViSigLIP_uitopenviic\n",
      "Saved training loss plot to: ./train_plot/ViSigLIP_uitopenviic/training_loss_steps.png\n",
      "Saved validation metrics plot to: ./train_plot/ViSigLIP_uitopenviic/validation_metrics_steps.png\n"
     ]
    }
   ],
   "source": [
    "# === Cell 14: Training Visualization (Adapted for Steps) ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd # For easier plotting from history\n",
    "\n",
    "def plot_training_metrics_steps(history, plot_dir):\n",
    "    if not history or not history.get('steps') or not history.get('train_loss'):\n",
    "        print(\"No/incomplete training history available for step-based plotting.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    print(f\"Step-based plot directory ensured at: {os.path.abspath(plot_dir)}\")\n",
    "\n",
    "    # --- Training Loss Plot ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['steps'], history['train_loss'], 'b-', label='Training Loss (Avg per Log Interval)')\n",
    "    plt.xlabel('Global Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Steps')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    save_path_loss = os.path.join(plot_dir, 'training_loss_steps.png')\n",
    "    plt.savefig(save_path_loss, dpi=300)\n",
    "    print(f\"Saved training loss plot to: {save_path_loss}\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- Validation Metrics Plot ---\n",
    "    if history.get('val_metrics'):\n",
    "        val_steps = sorted(history['val_metrics'].keys())\n",
    "        if val_steps:\n",
    "            metrics_to_plot = list(history['val_metrics'][val_steps[0]].keys()) # Get metric names from first entry\n",
    "\n",
    "            num_plots = len(metrics_to_plot)\n",
    "            if num_plots == 0: return\n",
    "            # Adjust subplot layout based on number of metrics\n",
    "            ncols = 2\n",
    "            nrows = math.ceil(num_plots / ncols)\n",
    "            fig, axes = plt.subplots(nrows, ncols, figsize=(8 * ncols, 6 * nrows), squeeze=False)\n",
    "            axes = axes.flatten() # Flatten for easy indexing\n",
    "\n",
    "            for i, metric_name in enumerate(metrics_to_plot):\n",
    "                 metric_values = [history['val_metrics'][step].get(metric_name, float('nan')) for step in val_steps]\n",
    "                 axes[i].plot(val_steps, metric_values, 'r-o', label=f'Validation {metric_name}')\n",
    "                 axes[i].set_xlabel('Global Steps')\n",
    "                 axes[i].set_ylabel(metric_name.capitalize())\n",
    "                 axes[i].set_title(f'Validation {metric_name} over Steps')\n",
    "                 axes[i].legend()\n",
    "                 axes[i].grid(True)\n",
    "\n",
    "            # Hide unused subplots\n",
    "            for j in range(i + 1, len(axes)):\n",
    "                 fig.delaxes(axes[j])\n",
    "\n",
    "            fig.suptitle('Validation Metrics over Training Steps', fontsize=16, y=1.02)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "            save_path_val = os.path.join(plot_dir, 'validation_metrics_steps.png')\n",
    "            plt.savefig(save_path_val, dpi=300)\n",
    "            print(f\"Saved validation metrics plot to: {save_path_val}\")\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(\"No validation steps found in history.\")\n",
    "    else:\n",
    "        print(\"No validation metrics found in history to plot.\")\n",
    "\n",
    "\n",
    "# --- Plotting ---\n",
    "plot_directory = \"./train_plot/ViSigLIP_uitopenviic\"\n",
    "if 'history' in locals() and isinstance(history, dict):\n",
    "    plot_training_metrics_steps(history, plot_directory)\n",
    "else:\n",
    "    print(\"No training history found. Run training first.\")\n",
    "\n",
    "# --- END OF SCRIPT ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
