{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n",
      "CUDA Capability: (8, 9)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Installs and Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, Blip2Processor, Blip2Model, Blip2Config, Blip2VisionModel, Blip2QFormerModel, BlipImageProcessor\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "# print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Capability: {torch.cuda.get_device_capability(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ViBLIP Q-Former Training Configuration ---\n",
      "Device: cuda\n",
      "Base BLIP-2 Model: Salesforce/blip2-opt-2.7b\n",
      "Text Tokenizer: Salesforce/blip2-opt-2.7b\n",
      "Batch Size: 8\n",
      "Use AMP: True\n",
      "Epochs: 5\n",
      "Q-Former LR: 0.0001\n",
      "Early Stop Patience: 3\n",
      "Output Path: ./ViBLIP_landmark\n",
      "Data Path (JSONs): /home/researcher/huypq69/TuningModels/data/LANDMARK-IN-VIETNAM\n",
      "Image Base Path: /home/researcher/huypq69/TuningModels/data/LANDMARK-IN-VIETNAM\n",
      "---------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration Class (CFG) - Updated with Early Stopping\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    data_path = \"/home/researcher/huypq69/TuningModels/data/LANDMARK-IN-VIETNAM/\"\n",
    "    image_path = \"/home/researcher/huypq69/TuningModels/data/LANDMARK-IN-VIETNAM/\"\n",
    "    model_path = \"./ViBLIP_landmark\"  # Output directory for saved models\n",
    "\n",
    "    # --- Model Selection ---\n",
    "    blip2_model_name = \"Salesforce/blip2-opt-2.7b\"  # Uses ViT-B by default\n",
    "    text_tokenizer_name = \"Salesforce/blip2-opt-2.7b\"  # Use BLIP-2's tokenizer instead of Vietnamese\n",
    "\n",
    "    # --- Training Parameters ---\n",
    "    seed = 42\n",
    "    batch_size = 8  # Reduced for stability\n",
    "    num_workers = 2  # Adjusted for typical CPU\n",
    "    qformer_lr = 1e-4\n",
    "    weight_decay = 0.05\n",
    "    patience = 2  # For LR scheduler\n",
    "    factor = 0.8\n",
    "    epochs = 5\n",
    "    early_stop_patience = 3  # Stop if no improvement for 3 epochs\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_amp = True\n",
    "\n",
    "    # --- Image/Text Parameters ---\n",
    "    image_size = 224\n",
    "    max_length = 77\n",
    "\n",
    "    # --- Loss/Saving Parameters ---\n",
    "    temperature = 0.07\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"val_itc_acc\"\n",
    "    mode = \"max\"\n",
    "\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"--- ViBLIP Q-Former Training Configuration ---\")\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Base BLIP-2 Model: {config.blip2_model_name}\")\n",
    "print(f\"Text Tokenizer: {config.text_tokenizer_name}\")\n",
    "print(f\"Batch Size: {config.batch_size}\")\n",
    "print(f\"Use AMP: {config.use_amp}\")\n",
    "print(f\"Epochs: {config.epochs}\")\n",
    "print(f\"Q-Former LR: {config.qformer_lr}\")\n",
    "print(f\"Early Stop Patience: {config.early_stop_patience}\")\n",
    "print(f\"Output Path: {config.model_path}\")\n",
    "print(f\"Data Path (JSONs): {os.path.abspath(config.data_path)}\")\n",
    "print(f\"Image Base Path: {os.path.abspath(config.image_path)}\")\n",
    "print(f\"---------------------------------------------\\n\")\n",
    "if config.data_path == \".\" and config.image_path == \".\":\n",
    "    print(\"WARNING: Using current directory for data and image paths. Ensure JSON files and images are present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Seeding for Reproducibility\n",
    "\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Metric Calculation Utilities\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        if torch.is_tensor(val): val = val.item()\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.name}: {self.avg:.4f}\"\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    n = similarity_matrix.shape[1-dim]\n",
    "    if n == 0 or k <= 0: return 0.0\n",
    "    effective_k = min(k, n)\n",
    "    correct_count = 0\n",
    "    top_k_indices = torch.topk(similarity_matrix, effective_k, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "\n",
    "    if dim == 0: # I2T\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]: correct_count += 1\n",
    "    elif dim == 1: # T2I\n",
    "        for txt_idx in range(n):\n",
    "            if ground_truth[txt_idx] in top_k_indices[txt_idx, :]: correct_count += 1\n",
    "    else: raise ValueError(\"dim must be 0 or 1\")\n",
    "    return correct_count / n\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    image_embeddings = image_embeddings.float()\n",
    "    text_embeddings = text_embeddings.float()\n",
    "\n",
    "    sim_matrix = text_embeddings @ image_embeddings.T\n",
    "    n = sim_matrix.shape[0]\n",
    "    default_metrics = {\n",
    "        \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "        \"avg_cosine_sim\": 0.0,\n",
    "        \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "        \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "    }\n",
    "    if n == 0: return default_metrics\n",
    "\n",
    "    try:\n",
    "        ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "        i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "        t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "        i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "        t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "        avg_acc = (i2t_acc + t2i_acc) / 2\n",
    "        diag_len = min(sim_matrix.shape[0], sim_matrix.shape[1])\n",
    "        avg_cosine_sim = torch.diagonal(sim_matrix[:diag_len, :diag_len]).mean().item()\n",
    "\n",
    "        i2t_recall = {}\n",
    "        t2i_recall = {}\n",
    "        for k in [1, 5, 10]:\n",
    "            k_str = f\"R@{k}\"\n",
    "            i2t_recall[k_str] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "            t2i_recall[k_str] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "\n",
    "        return {\n",
    "            \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "            \"avg_cosine_sim\": avg_cosine_sim,\n",
    "            \"i2t_recall\": i2t_recall, \"t2i_recall\": t2i_recall\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error during metric calculation: {e}\")\n",
    "        print(f\"Shapes: ImgEmb={image_embeddings.shape}, TxtEmb={text_embeddings.shape}, SimMtx={sim_matrix.shape}\")\n",
    "        return default_metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageCaptionDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Dataset Class Definition\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, tokenizer, image_processor, max_length):\n",
    "        super().__init__()\n",
    "        print(f\"Loading data from: {os.path.abspath(json_path)}\")\n",
    "        self.data = []\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                self.data = json.load(f)\n",
    "            print(f\"Loaded {len(self.data)} samples from {os.path.basename(json_path)}.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: JSON file not found at {json_path}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error loading JSON: {e}\")\n",
    "\n",
    "        self.image_base_path = image_base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "        if not os.path.isdir(self.image_base_path):\n",
    "            print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data): raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "        relative_image_path = item.get('image_path')\n",
    "        caption = item.get('caption', '')\n",
    "        if isinstance(caption, list) and len(caption) > 0:\n",
    "            caption = caption[0]\n",
    "        caption = str(caption)\n",
    "        image = None\n",
    "\n",
    "        # Process image\n",
    "        if relative_image_path:\n",
    "            image_path = os.path.normpath(os.path.join(self.image_base_path, relative_image_path))\n",
    "            try:\n",
    "                img_pil = Image.open(image_path).convert('RGB')\n",
    "                image_processed = self.image_processor(images=img_pil, return_tensors=\"pt\")\n",
    "                image = image_processed['pixel_values'].squeeze(0)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Image not found at {image_path}\")\n",
    "                image = None\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {image_path}: {e}\")\n",
    "                image = None\n",
    "\n",
    "        if image is None:\n",
    "            # Create blank image tensor if image loading failed\n",
    "            c = 3\n",
    "            h = w = config.image_size\n",
    "            image = torch.zeros((c, h, w))\n",
    "\n",
    "        # Process text - following BLIP-2's expected format\n",
    "        try:\n",
    "            import unicodedata\n",
    "            normalized_caption = unicodedata.normalize('NFC', caption)\n",
    "            \n",
    "            # Use the tokenizer in a way compatible with BLIP-2\n",
    "            text_inputs = self.tokenizer(\n",
    "                normalized_caption,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Extract and ensure correct tensor dimensions\n",
    "            input_ids = text_inputs['input_ids'].squeeze(0)\n",
    "            attention_mask = text_inputs['attention_mask'].squeeze(0)\n",
    "            \n",
    "            # Safeguard against dimension issues\n",
    "            if input_ids.dim() == 0:\n",
    "                input_ids = input_ids.unsqueeze(0)\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error tokenizing text: {e}. Using empty text.\")\n",
    "            # Create empty text inputs if tokenization failed\n",
    "            input_ids = torch.zeros(self.max_length, dtype=torch.long)\n",
    "            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n",
    "            input_ids[0] = self.tokenizer.bos_token_id if hasattr(self.tokenizer, 'bos_token_id') else 0\n",
    "            attention_mask[0] = 1\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"raw_caption\": caption\n",
    "        }\n",
    "\n",
    "print(\"ImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP-2 configuration for: Salesforce/blip2-opt-2.7b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP-2 model: Salesforce/blip2-opt-2.7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bf2fe691fb46469d0c259bccd4976c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing Vision Model and Language Model parameters...\n",
      "  Vision model frozen.\n",
      "  Language model frozen.\n",
      "Verifying Q-Former parameters are trainable...\n",
      "  Note: Projection layers (vision_proj, text_proj) not found.\n",
      "\n",
      "Model components loaded successfully.\n",
      "  Total parameters: ~3744.76 M\n",
      "  Frozen parameters: ~3637.63 M\n",
      "  Trainable parameters: ~105.14 M\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Model Loading & Freezing\n",
    "\n",
    "model = None\n",
    "blip_config_loaded = None\n",
    "model_loaded = False\n",
    "\n",
    "try:\n",
    "    print(f\"Loading BLIP-2 configuration for: {config.blip2_model_name}\")\n",
    "    blip_config_loaded = Blip2Config.from_pretrained(config.blip2_model_name)\n",
    "\n",
    "    print(f\"Loading BLIP-2 model: {config.blip2_model_name}\")\n",
    "    model_dtype = torch.float16 if config.use_amp and config.device == torch.device('cuda') else torch.float32\n",
    "    model = Blip2Model.from_pretrained(\n",
    "        config.blip2_model_name,\n",
    "        config=blip_config_loaded,\n",
    "        torch_dtype=model_dtype\n",
    "    )\n",
    "\n",
    "    print(\"Freezing Vision Model and Language Model parameters...\")\n",
    "    frozen_params_count = 0\n",
    "    total_params = 0\n",
    "\n",
    "    if hasattr(model, 'vision_model'):\n",
    "        for param in model.vision_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            frozen_params_count += param.numel()\n",
    "        print(f\"  Vision model frozen.\")\n",
    "    else:\n",
    "        print(\"  Warning: model.vision_model not found.\")\n",
    "\n",
    "    if hasattr(model, 'language_model'):\n",
    "        for param in model.language_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            frozen_params_count += param.numel()\n",
    "        print(f\"  Language model frozen.\")\n",
    "    else:\n",
    "        print(\"  Warning: model.language_model not found.\")\n",
    "\n",
    "    trainable_params_count = 0\n",
    "    if hasattr(model, 'qformer'):\n",
    "        print(\"Verifying Q-Former parameters are trainable...\")\n",
    "        model.qformer.train()\n",
    "        for param in model.qformer.parameters():\n",
    "            param.requires_grad = True\n",
    "            trainable_params_count += param.numel()\n",
    "\n",
    "        proj_layers_found = 0\n",
    "        for proj_name in ['vision_proj', 'text_proj']:\n",
    "            if hasattr(model, proj_name):\n",
    "                layer = getattr(model, proj_name)\n",
    "                if layer is not None and isinstance(layer, nn.Module):\n",
    "                    print(f\"  Verifying {proj_name} parameters are trainable...\")\n",
    "                    layer.train()\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = True\n",
    "                    trainable_params_count += sum(p.numel() for p in layer.parameters())\n",
    "                    proj_layers_found += 1\n",
    "\n",
    "        if proj_layers_found == 0:\n",
    "            print(\"  Note: Projection layers (vision_proj, text_proj) not found.\")\n",
    "\n",
    "        model.to(config.device)\n",
    "        model_loaded = True\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(\"\\nModel components loaded successfully.\")\n",
    "        print(f\"  Total parameters: ~{total_params / 1e6:.2f} M\")\n",
    "        print(f\"  Frozen parameters: ~{frozen_params_count / 1e6:.2f} M\")\n",
    "        print(f\"  Trainable parameters: ~{trainable_params_count / 1e6:.2f} M\")\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: model.qformer not found!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading model '{config.blip2_model_name}': {e}\")\n",
    "    traceback.print_exc()\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP-2 Processor from: Salesforce/blip2-opt-2.7b\n",
      "BLIP-2 Processor loaded successfully\n",
      "  Tokenizer vocabulary size: 50265\n",
      "  Image processor size: {'height': 224, 'width': 224}\n",
      "  Reduced batch size from 8 to 4 for stability\n",
      "\n",
      "Creating datasets...\n",
      "Loading data from: /home/researcher/huypq69/TuningModels/data/LANDMARK-IN-VIETNAM/train.json\n",
      "Loaded 19844 samples from train.json.\n",
      "Loading data from: /home/researcher/huypq69/TuningModels/data/LANDMARK-IN-VIETNAM/dev.json\n",
      "Loaded 5667 samples from dev.json.\n",
      "\n",
      "Creating dataloaders...\n",
      "Using 2 workers with batch size 4.\n",
      "Train loader created (4961 batches).\n",
      "Validation loader created (1417 batches).\n",
      "\n",
      "Data setup complete with BLIP-2 processor.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Data Setup (Tokenizer, Image Processor, Datasets, DataLoaders)\n",
    "\n",
    "processor = None \n",
    "tokenizer = None\n",
    "image_processor = None\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "data_setup_ok = False\n",
    "\n",
    "if model_loaded:\n",
    "    try:\n",
    "        print(f\"Loading BLIP-2 Processor instead of separate tokenizers\")\n",
    "        processor = Blip2Processor.from_pretrained(config.blip2_model_name)\n",
    "        tokenizer = processor.tokenizer\n",
    "        image_processor = processor.image_processor\n",
    "        \n",
    "        print(f\"BLIP-2 Processor loaded successfully\")\n",
    "        print(f\"  Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
    "        print(f\"  Max sequence length: {tokenizer.model_max_length}\")\n",
    "        \n",
    "        if hasattr(image_processor, 'size'):\n",
    "            processor_size = image_processor.size['height'] if isinstance(image_processor.size, dict) else image_processor.size\n",
    "            if processor_size != config.image_size:\n",
    "                print(f\"  Updating config.image_size from {config.image_size} to {processor_size}\")\n",
    "                config.image_size = processor_size\n",
    "\n",
    "        # Reduce batch size to avoid CUDA errors\n",
    "        config.batch_size = 8  \n",
    "        print(f\"  Using reduced batch size: {config.batch_size} to avoid CUDA errors\")\n",
    "        \n",
    "        # Adjust max_length to match tokenizer's expectation\n",
    "        config.max_length = min(config.max_length, tokenizer.model_max_length)\n",
    "        print(f\"  Using max_length: {config.max_length}\")\n",
    "\n",
    "        print(\"\\nCreating datasets...\")\n",
    "        train_json = os.path.join(config.data_path, \"train.json\")\n",
    "        dev_json = os.path.join(config.data_path, \"dev.json\")\n",
    "\n",
    "        train_dataset = ImageCaptionDataset(\n",
    "            json_path=train_json, image_base_path=config.image_path,\n",
    "            tokenizer=tokenizer, image_processor=image_processor, max_length=config.max_length\n",
    "        )\n",
    "        dev_dataset = ImageCaptionDataset(\n",
    "            json_path=dev_json, image_base_path=config.image_path,\n",
    "            tokenizer=tokenizer, image_processor=image_processor, max_length=config.max_length\n",
    "        )\n",
    "\n",
    "        if not train_dataset.data: raise ValueError(\"Training data failed to load.\")\n",
    "        if not dev_dataset.data: print(\"Warning: Validation data not loaded.\")\n",
    "\n",
    "        print(\"\\nCreating dataloaders...\")\n",
    "        num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "        print(f\"Using {num_workers} workers.\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False, drop_last=True\n",
    "        )\n",
    "        print(f\"Train loader created ({len(train_loader)} batches).\")\n",
    "\n",
    "        if dev_dataset.data:\n",
    "            dev_loader = DataLoader(\n",
    "                dev_dataset, batch_size=config.batch_size, shuffle=False, num_workers=num_workers,\n",
    "                pin_memory=True if config.device == torch.device(\"cuda\") else False, drop_last=False\n",
    "            )\n",
    "            print(f\"Validation loader created ({len(dev_loader)} batches).\")\n",
    "\n",
    "        data_setup_ok = True\n",
    "        print(\"\\nData setup complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during data setup: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Skipping data setup because model failed to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up optimizer and scheduler...\n",
      "Found 257 parameter tensors to optimize (~107.13 M).\n",
      "Optimizer AdamW initialized with lr=1.0e-04\n",
      "LR Scheduler ReduceLROnPlateau initialized (mode='max')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/researcher/huypq69/Tuning-CLIP/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Optimizer & Scheduler Setup\n",
    "\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "optimizer_setup_ok = False\n",
    "\n",
    "if model_loaded and data_setup_ok:\n",
    "    print(\"\\nSetting up optimizer and scheduler...\")\n",
    "    try:\n",
    "        trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "        param_count = sum(p.numel() for p in trainable_params)\n",
    "        print(f\"Found {len(trainable_params)} parameter tensors to optimize (~{param_count / 1e6:.2f} M).\")\n",
    "\n",
    "        if not trainable_params:\n",
    "            raise ValueError(\"No trainable parameters found.\")\n",
    "\n",
    "        optimizer = optim.AdamW(trainable_params, lr=config.qformer_lr, weight_decay=config.weight_decay)\n",
    "        print(f\"Optimizer AdamW initialized with lr={config.qformer_lr:.1e}\")\n",
    "\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode=config.mode, factor=config.factor, patience=config.patience, verbose=True\n",
    "        )\n",
    "        print(f\"LR Scheduler ReduceLROnPlateau initialized (mode='{config.mode}')\")\n",
    "        optimizer_setup_ok = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR setting up optimizer/scheduler: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Skipping optimizer setup due to previous errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions defined: ITC and ITM implemented, ITG is placeholder.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Loss Function Definitions\n",
    "\n",
    "def calculate_itc_loss(image_feats_norm, text_feats_norm, temperature):\n",
    "    logits = (image_feats_norm @ text_feats_norm.T) / temperature\n",
    "    logits = logits.float()\n",
    "    batch_size = image_feats_norm.shape[0]\n",
    "    if batch_size == 0: return torch.tensor(0.0, device=logits.device)\n",
    "    labels = torch.arange(batch_size, device=logits.device)\n",
    "    loss_i = F.cross_entropy(logits, labels)\n",
    "    loss_t = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i + loss_t) / 2.0\n",
    "\n",
    "def calculate_itm_loss(model, outputs, batch_size, device):\n",
    "    \"\"\"Image-Text Matching Loss with Hard Negative Mining\"\"\"\n",
    "    if batch_size == 0 or not hasattr(model, 'qformer') or not hasattr(outputs, 'qformer_outputs'):\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    try:\n",
    "        # Extract Q-Former multimodal features (CLS token)\n",
    "        multimodal_feats = outputs.qformer_outputs.last_hidden_state[:, 0]  # [batch_size, hidden_size]\n",
    "\n",
    "        # Hard negative mining: Find mismatched pairs with high ITC similarity\n",
    "        image_feats = model.vision_proj(outputs.image_embeds) if hasattr(model, 'vision_proj') else outputs.image_embeds\n",
    "        text_feats = model.text_proj(outputs.text_embeds) if hasattr(model, 'text_proj') else outputs.text_embeds\n",
    "        image_feats_norm = F.normalize(image_feats, dim=-1)\n",
    "        text_feats_norm = F.normalize(text_feats, dim=-1)\n",
    "        sim_matrix = image_feats_norm @ text_feats_norm.T\n",
    "        sim_matrix.fill_diagonal_(-float('inf'))  # Exclude true pairs\n",
    "        hard_neg_indices = torch.argmax(sim_matrix, dim=1)  # [batch_size]\n",
    "\n",
    "        # Create negative pairs by pairing images with hard-negative texts\n",
    "        neg_input_ids = outputs.input_ids[hard_neg_indices]\n",
    "        neg_attention_mask = outputs.attention_mask[hard_neg_indices]\n",
    "        pixel_values = outputs.pixel_values\n",
    "\n",
    "        with torch.no_grad():\n",
    "            neg_outputs = model(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=neg_input_ids,\n",
    "                attention_mask=neg_attention_mask\n",
    "            )\n",
    "        neg_multimodal_feats = neg_outputs.qformer_outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        # Combine positive and negative features\n",
    "        all_feats = torch.cat([multimodal_feats, neg_multimodal_feats], dim=0)  # [2*batch_size, hidden_size]\n",
    "        itm_logits = model.itm_head(all_feats) if hasattr(model, 'itm_head') else nn.Linear(all_feats.size(-1), 2).to(device)(all_feats)\n",
    "\n",
    "        # Labels: 1 for positive pairs, 0 for negative pairs\n",
    "        itm_labels = torch.cat([torch.ones(batch_size, dtype=torch.long), torch.zeros(batch_size, dtype=torch.long)]).to(device)\n",
    "\n",
    "        return F.cross_entropy(itm_logits, itm_labels)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ITM loss calculation: {e}\")\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "def calculate_itg_loss(model_outputs, target_ids, target_mask):\n",
    "    return torch.tensor(0.0, device=target_ids.device)\n",
    "\n",
    "print(\"Loss functions defined: ITC and ITM implemented, ITG is placeholder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Validation Loop Implementation\n",
    "\n",
    "def validate_qformer_epoch(model, dataloader, device, epoch_num):\n",
    "    print(f\"--- Running Validation Epoch {epoch_num} ---\")\n",
    "    model.eval()\n",
    "    val_loss_meter = AvgMeter(f\"Val Total E{epoch_num}\")\n",
    "    val_itc_meter = AvgMeter(f\"Val ITC E{epoch_num}\")\n",
    "    val_itm_meter = AvgMeter(f\"Val ITM E{epoch_num}\")\n",
    "    val_itc_acc_meter = AvgMeter(f\"Val ITC Acc E{epoch_num}\")\n",
    "    val_itm_acc_meter = AvgMeter(f\"Val ITM Acc E{epoch_num}\")\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Validating E{epoch_num}\", leave=True, unit=\"batch\")\n",
    "\n",
    "    all_image_feats = []\n",
    "    all_text_feats = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_size = pixel_values.size(0)\n",
    "            if batch_size == 0: continue\n",
    "\n",
    "            expected_dtype = torch.float16 if config.use_amp else torch.float32\n",
    "            pixel_values = pixel_values.to(dtype=expected_dtype)\n",
    "\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                try:\n",
    "                    outputs = model(\n",
    "                        pixel_values=pixel_values,\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "\n",
    "                    image_embeds = outputs.image_embeds\n",
    "                    text_embeds = outputs.text_embeds\n",
    "\n",
    "                    image_feats = model.vision_proj(image_embeds) if hasattr(model, 'vision_proj') else image_embeds\n",
    "                    text_feats = model.text_proj(text_embeds) if hasattr(model, 'text_proj') else text_embeds\n",
    "\n",
    "                    image_feats_norm = F.normalize(image_feats, dim=-1)\n",
    "                    text_feats_norm = F.normalize(text_feats, dim=-1)\n",
    "\n",
    "                    loss_itc = calculate_itc_loss(image_feats_norm, text_feats_norm, config.temperature)\n",
    "                    loss_itm = calculate_itm_loss(model, outputs, batch_size, device)\n",
    "                    total_loss = loss_itc + loss_itm\n",
    "\n",
    "                    all_image_feats.append(image_feats_norm)\n",
    "                    all_text_feats.append(text_feats_norm)\n",
    "\n",
    "                    metrics = compute_metrics(image_feats_norm, text_feats_norm)\n",
    "                    itc_acc = metrics['avg_acc']\n",
    "\n",
    "                    multimodal_feats = outputs.qformer_outputs.last_hidden_state[:, 0]\n",
    "                    itm_logits = model.itm_head(multimodal_feats) if hasattr(model, 'itm_head') else nn.Linear(multimodal_feats.size(-1), 2).to(device)(multimodal_feats)\n",
    "                    itm_preds = torch.argmax(itm_logits, dim=-1)\n",
    "                    itm_labels = torch.ones(batch_size, dtype=torch.long).to(device)\n",
    "                    itm_acc = (itm_preds == itm_labels).float().mean().item()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in validation batch: {e}\")\n",
    "                    continue\n",
    "\n",
    "                val_loss_meter.update(total_loss.item(), batch_size)\n",
    "                val_itc_meter.update(loss_itc.item(), batch_size)\n",
    "                val_itm_meter.update(loss_itm.item(), batch_size)\n",
    "                val_itc_acc_meter.update(itc_acc, batch_size)\n",
    "                val_itm_acc_meter.update(itm_acc, batch_size)\n",
    "\n",
    "                progress_bar.set_postfix(loss=f\"{val_loss_meter.avg:.4f}\", itc_acc=f\"{val_itc_acc_meter.avg:.4f}\", itm_acc=f\"{val_itm_acc_meter.avg:.4f}\")\n",
    "\n",
    "    all_image_feats = torch.cat(all_image_feats, dim=0)\n",
    "    all_text_feats = torch.cat(all_text_feats, dim=0)\n",
    "    final_metrics = compute_metrics(all_image_feats, all_text_feats)\n",
    "\n",
    "    results = {\n",
    "        'loss': val_loss_meter.avg,\n",
    "        'val_itc_acc': val_itc_acc_meter.avg,\n",
    "        'val_itm_acc': val_itm_acc_meter.avg,\n",
    "        'val_itc_loss': val_itc_meter.avg,\n",
    "        'val_itm_loss': val_itm_meter.avg,\n",
    "        'i2t_recall': final_metrics['i2t_recall'],\n",
    "        't2i_recall': final_metrics['t2i_recall']\n",
    "    }\n",
    "\n",
    "    print(f\"Validation Epoch {epoch_num} Results: Loss={results['loss']:.4f}, ITC Acc={results['val_itc_acc']:.4f}, ITM Acc={results['val_itm_acc']:.4f}\")\n",
    "    print(f\"  I2T Recall: {results['i2t_recall']}\")\n",
    "    print(f\"  T2I Recall: {results['t2i_recall']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"Validation function implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training Loop - Updated with Early Stopping\n",
    "\n",
    "ready_to_train = model_loaded and optimizer_setup_ok and data_setup_ok and train_loader is not None\n",
    "\n",
    "if ready_to_train:\n",
    "    print(f\"\\n=============== Starting Q-Former Training ===============\")\n",
    "    print(f\"Epochs: {config.epochs}, Batch Size: {config.batch_size}, Device: {config.device}, AMP: {config.use_amp}\")\n",
    "    print(f\"Tracking metric: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "    print(f\"Early Stopping Patience: {config.early_stop_patience} epochs\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    early_stop_counter = 0\n",
    "    history = {'train_loss': [], 'train_itc_loss': [], 'train_itm_loss': [], 'validation_results': []}\n",
    "    start_train_time = time.time()\n",
    "    \n",
    "    # Use newer amp format to avoid warnings\n",
    "    from torch.amp import GradScaler, autocast\n",
    "    scaler = GradScaler(enabled=config.use_amp)\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss_meter = AvgMeter(f\"Train Total E{epoch+1}\")\n",
    "        train_itc_meter = AvgMeter(f\"Train ITC E{epoch+1}\")\n",
    "        train_itm_meter = AvgMeter(f\"Train ITM E{epoch+1}\")\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Training E{epoch+1}\", leave=True, unit=\"batch\")\n",
    "\n",
    "        # Debug first batch to catch issues early\n",
    "        first_batch_done = False\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            try:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                # Move to device and check dimensions\n",
    "                pixel_values = batch['pixel_values'].to(config.device)\n",
    "                input_ids = batch['input_ids'].to(config.device)\n",
    "                attention_mask = batch['attention_mask'].to(config.device)\n",
    "                \n",
    "                # Get actual batch size from the current batch\n",
    "                batch_size = pixel_values.size(0)\n",
    "                if batch_size == 0: \n",
    "                    print(\"Warning: Empty batch detected. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Print shapes for the first batch to help debugging\n",
    "                if not first_batch_done:\n",
    "                    print(f\"\\nFirst batch shapes:\")\n",
    "                    print(f\"  pixel_values: {pixel_values.shape}\")\n",
    "                    print(f\"  input_ids: {input_ids.shape}\")\n",
    "                    print(f\"  attention_mask: {attention_mask.shape}\")\n",
    "                    first_batch_done = True\n",
    "                \n",
    "                # Ensure consistent types\n",
    "                expected_dtype = torch.float16 if config.use_amp else torch.float32\n",
    "                pixel_values = pixel_values.to(dtype=expected_dtype)\n",
    "\n",
    "                with autocast(device_type='cuda', enabled=config.use_amp):\n",
    "                    # Pass tensors to model\n",
    "                    outputs = model(\n",
    "                        pixel_values=pixel_values,\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "\n",
    "                    # Extract embeddings\n",
    "                    image_embeds = outputs.image_embeds\n",
    "                    text_embeds = outputs.text_embeds\n",
    "\n",
    "                    # Apply projection if available\n",
    "                    image_feats = model.vision_proj(image_embeds) if hasattr(model, 'vision_proj') else image_embeds\n",
    "                    text_feats = model.text_proj(text_embeds) if hasattr(model, 'text_proj') else text_embeds\n",
    "\n",
    "                    # Normalize for loss calculation\n",
    "                    image_feats_norm = F.normalize(image_feats, dim=-1)\n",
    "                    text_feats_norm = F.normalize(text_feats, dim=-1)\n",
    "\n",
    "                    # Calculate losses\n",
    "                    loss_itc = calculate_itc_loss(image_feats_norm, text_feats_norm, config.temperature)\n",
    "                    loss_itm = calculate_itm_loss(model, outputs, batch_size, config.device)\n",
    "                    loss_itg = torch.tensor(0.0, device=config.device)\n",
    "\n",
    "                    total_loss = loss_itc + loss_itm + loss_itg\n",
    "\n",
    "                # Skip bad loss values\n",
    "                if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "                    print(f\"Warning: NaN/Inf loss at step {step}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Update with gradient scaling for mixed precision\n",
    "                scaler.scale(total_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # Update metrics\n",
    "                train_loss_meter.update(total_loss.item(), batch_size)\n",
    "                train_itc_meter.update(loss_itc.item(), batch_size)\n",
    "                train_itm_meter.update(loss_itm.item(), batch_size)\n",
    "                \n",
    "                # Show progress\n",
    "                progress_bar.set_postfix(loss=f\"{train_loss_meter.avg:.4f}\", \n",
    "                                         itc=f\"{train_itc_meter.avg:.4f}\", \n",
    "                                         itm=f\"{train_itm_meter.avg:.4f}\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                error_msg = str(e)\n",
    "                if \"CUDA\" in error_msg:\n",
    "                    print(f\"\\nCUDA error at step {step}: {error_msg}\")\n",
    "                    print(\"Consider reducing batch size or model size.\")\n",
    "                    if step == 0:  # If error on first batch, training can't proceed\n",
    "                        raise\n",
    "                    continue  # Skip this batch and try the next one\n",
    "                else:\n",
    "                    raise  # Re-raise other runtime errors\n",
    "            except Exception as e:\n",
    "                print(f\"\\nUnexpected error at step {step}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        # Record training history\n",
    "        history['train_loss'].append(train_loss_meter.avg)\n",
    "        history['train_itc_loss'].append(train_itc_meter.avg)\n",
    "        history['train_itm_loss'].append(train_itm_meter.avg)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss_meter.avg:.4f} (ITC={train_itc_meter.avg:.4f}, ITM={train_itm_meter.avg:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Test Set Evaluation\n",
    "\n",
    "print(\"\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "evaluation_performed = False\n",
    "model_loaded_for_test = False\n",
    "\n",
    "if not (model_loaded and data_setup_ok):\n",
    "    print(\"Skipping test evaluation: Model or data setup failed.\")\n",
    "elif not os.path.exists(test_json_path):\n",
    "    print(f\"Skipping test evaluation: Test JSON not found ({test_json_path}).\")\n",
    "else:\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    if 'tokenizer' in globals() and 'image_processor' in globals():\n",
    "        test_dataset = ImageCaptionDataset(\n",
    "            json_path=test_json_path, image_base_path=config.image_path,\n",
    "            tokenizer=tokenizer, image_processor=image_processor, max_length=config.max_length\n",
    "        )\n",
    "\n",
    "        if test_dataset.data:\n",
    "            num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=num_workers,\n",
    "                pin_memory=True if config.device == torch.device(\"cuda\") else False, drop_last=False\n",
    "            )\n",
    "            print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "\n",
    "            model_to_test = None\n",
    "            try:\n",
    "                blip_config_test = Blip2Config.from_pretrained(config.blip2_model_name)\n",
    "                model_to_test = Blip2Model.from_pretrained(\n",
    "                    config.blip2_model_name, config=blip_config_test,\n",
    "                    torch_dtype=torch.float16 if config.use_amp else torch.float32\n",
    "                )\n",
    "                for param in model_to_test.vision_model.parameters(): param.requires_grad = False\n",
    "                if hasattr(model_to_test, 'language_model'):\n",
    "                    for param in model_to_test.language_model.parameters(): param.requires_grad = False\n",
    "                print(\"Model structure for testing created.\")\n",
    "\n",
    "                best_model_path = os.path.join(config.model_path, \"ViBLIP_QFormer_best.pt\")\n",
    "                if os.path.exists(best_model_path):\n",
    "                    print(f\"Loading best model weights from: {best_model_path}\")\n",
    "                    checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "                    state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "                    if next(iter(state_dict)).startswith('module.'):\n",
    "                        from collections import OrderedDict\n",
    "                        state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "\n",
    "                    load_result = model_to_test.load_state_dict(state_dict, strict=False)\n",
    "                    print(f\"Load Result: {load_result}\")\n",
    "                    model_to_test.to(config.device)\n",
    "                    model_loaded_for_test = True\n",
    "                    print(\"Loaded trained weights into model structure.\")\n",
    "\n",
    "                    print(\"\\nRunning evaluation on test set...\")\n",
    "                    test_results = validate_qformer_epoch(model_to_test, test_loader, config.device, \"Test\")\n",
    "                    evaluation_performed = True\n",
    "                    print(\"\\n--- Test Set Results ---\")\n",
    "                    metric_log_str = f\"  Loss: {test_results['loss']:.4f}\\n\"\n",
    "                    metric_log_str += f\"  ITC Acc: {test_results['val_itc_acc']:.4f}\\n\"\n",
    "                    metric_log_str += f\"  ITM Acc: {test_results['val_itm_acc']:.4f}\\n\"\n",
    "                    metric_log_str += f\"  I2T Recall: {test_results['i2t_recall']}\\n\"\n",
    "                    metric_log_str += f\"  T2I Recall: {test_results['t2i_recall']}\\n\"\n",
    "                    print(metric_log_str.strip())\n",
    "                    print(\"------------------------\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"ERROR: Best model checkpoint not found at {best_model_path}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during test setup or evaluation: {e}\")\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(\"Could not load test data. Skipping test evaluation.\")\n",
    "    else:\n",
    "        print(\"Skipping test evaluation: Tokenizer or Image Processor not available.\")\n",
    "\n",
    "if not evaluation_performed:\n",
    "    print(\"Test set evaluation was not performed.\")\n",
    "\n",
    "print(\"\\n================= Evaluation Finished =================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
