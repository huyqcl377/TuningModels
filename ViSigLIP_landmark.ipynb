{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "Transformers Version: 4.50.0\n",
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: Imports ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR # Keep Cosine for option\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True # Handle potential image loading issues\n",
    "\n",
    "# --- Import SigLIP Vision model and its Processor ---\n",
    "from transformers import SiglipVisionModel, SiglipConfig, SiglipImageProcessor\n",
    "# --- Keep PhoBERT parts ---\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "# --- Blip models no longer needed for loading ---\n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import transformers\n",
    "import gc\n",
    "import traceback # Import traceback\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    # Ensure you're setting the correct device index if needed\n",
    "    device_idx = 1 # Or 0\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(device_idx)}\")\n",
    "    torch.cuda.set_device(device_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Per-Device Batch Size: 128\n",
      "Accumulation Steps: 1\n",
      "Effective Batch Size (per optimizer step): 128\n",
      "Model output path: ./trained_models/ViSigLIP_landmark\n",
      "Selected Vision Source: google/siglip-base-patch16-224\n",
      "Selected Text Model: vinai/phobert-base\n",
      "Image base path (for resolving paths in JSON): /home/researcher/huypq69/2ndrun/TuningModels/data/LANDMARK-IN-VIETNAM\n",
      "AMP Enabled: True\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Configuration Class (CFG) - Modified for SigLIP Vision + PhoBERT Text ===\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    data_path = \"./json_data//\"\n",
    "    image_base_path = \"./data/LANDMARK-IN-VIETNAM/\"\n",
    "    model_path = \"./trained_models/ViSigLIP_landmark\"\n",
    "\n",
    "    # --- Model Selection ---\n",
    "    # --- SigLIP Vision Model ---\n",
    "    selected_vision_source = \"google/siglip-base-patch16-224\"\n",
    "    # --- Keep PhoBERT Text Model ---\n",
    "    selected_text_model = \"vinai/phobert-base\"\n",
    "    text_tokenizer_name = selected_text_model\n",
    "\n",
    "    # --- Model parameters ---\n",
    "    vision_model_name = selected_vision_source # For clarity\n",
    "    text_model_name = selected_text_model   # For clarity\n",
    "    # --- Image Processor: Use SigLIP's processor ---\n",
    "    image_processor_name = selected_vision_source\n",
    "\n",
    "    @property\n",
    "    def text_embedding(self): return 768 # PhoBERT-base output\n",
    "    @property\n",
    "    def vision_embedding(self): return 768 # Siglip-base-patch16-224 output\n",
    "\n",
    "    projection_dim = 768 # Common projection dim for CLIP-style models (adjust if needed, e.g., 768)\n",
    "\n",
    "    # --- Fine-tuning parameters ---\n",
    "    seed = 42\n",
    "    # Adjust batch size based on VRAM for SigLIP base + PhoBERT base\n",
    "    batch_size = 128\n",
    "    num_workers = 20\n",
    "    accumulation_steps = 1 # Effective batch size = 128\n",
    "\n",
    "    # --- Learning Rates for Fine-tuning ---\n",
    "    projection_lr = 1e-4\n",
    "    vision_encoder_lr = 1e-5 # Lower LR for SigLIP backbone\n",
    "    text_encoder_lr = 2e-5   # Slightly higher LR for PhoBERT backbone\n",
    "    weight_decay = 1e-4     # Lower weight decay for fine-tuning\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    # --- Use standard contrastive loss temperature (like CLIP) ---\n",
    "    temperature = 0.07\n",
    "    learnable_temperature = True\n",
    "    # --- Bias term is NOT used in standard contrastive loss ---\n",
    "    learnable_bias = False\n",
    "    bias_init = 0.0\n",
    "\n",
    "    # --- Scheduler ---\n",
    "    scheduler_type = \"reduce_on_plateau\" # RoP often used for fine-tuning\n",
    "    rop_patience = 2\n",
    "    rop_factor = 0.8\n",
    "\n",
    "    epochs = 200 \n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_amp = True # Keep AMP enabled\n",
    "\n",
    "    # --- Image/Text parameters ---\n",
    "    max_length = 77 \n",
    "\n",
    "    # --- Loss/Saving parameters ---\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"avg_acc\" # Track validation accuracy\n",
    "    mode = \"max\"\n",
    "    # Adjust intervals if needed\n",
    "    save_interval_steps = 1000 # Save periodically during fine-tuning (optional)\n",
    "    validation_interval_steps = 1000 # Validate more often during fine-tuning\n",
    "    log_interval_steps = 50\n",
    "\n",
    "    early_stopping_patience = 5 # Patience in terms of validation checks\n",
    "    early_stopping_min_delta = 0.001 # Min change to be considered improvement\n",
    "\n",
    "# --- Instantiate Config and Create Output Dir ---\n",
    "config = CFG()\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Per-Device Batch Size: {config.batch_size}\")\n",
    "print(f\"Accumulation Steps: {config.accumulation_steps}\")\n",
    "print(f\"Effective Batch Size (per optimizer step): {config.batch_size * config.accumulation_steps}\")\n",
    "print(f\"Model output path: {config.model_path}\")\n",
    "print(f\"Selected Vision Source: {config.selected_vision_source}\")\n",
    "print(f\"Selected Text Model: {config.selected_text_model}\")\n",
    "print(f\"Image base path (for resolving paths in JSON): {os.path.abspath(config.image_base_path)}\")\n",
    "print(f\"AMP Enabled: {config.use_amp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 42\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: Seeding ===\n",
    "def set_seed(seed=config.seed):\n",
    "    print(f\"Setting seed: {seed}\")\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Metric & AvgMeter Utilities ===\n",
    "# (Keep AvgMeter, compute_recall_at_k, compute_metrics as before)\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0.0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        if torch.is_tensor(val):\n",
    "            val = val.item()\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = float(self.sum) / self.count if self.count != 0 else 0.0\n",
    "        else:\n",
    "            print(f\"Warning: Cannot update AvgMeter '{self.name}' with value type {type(val)}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.name}: {self.avg:.4f}\"\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    n = similarity_matrix.shape[1-dim]\n",
    "    if n == 0: return 0.0\n",
    "    correct_count = 0\n",
    "    actual_k = min(k, similarity_matrix.shape[dim])\n",
    "    if actual_k == 0: return 0.0\n",
    "    top_k_indices = torch.topk(similarity_matrix, actual_k, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "    if dim == 0: # I2T\n",
    "        for img_idx in range(n):\n",
    "            if ground_truth[img_idx] in top_k_indices[:, img_idx]:\n",
    "                correct_count += 1\n",
    "    elif dim == 1: # T2I\n",
    "        for txt_idx in range(n):\n",
    "             if ground_truth[txt_idx] in top_k_indices[txt_idx, :]:\n",
    "                correct_count += 1\n",
    "    else: raise ValueError(\"dim must be 0 or 1\")\n",
    "    return float(correct_count) / n if n > 0 else 0.0\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings):\n",
    "    sim_matrix = text_embeddings.float() @ image_embeddings.float().T\n",
    "    n = sim_matrix.shape[0]\n",
    "    if n == 0:\n",
    "        return {\n",
    "            \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "            \"avg_cosine_sim\": 0.0,\n",
    "            \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "            \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "        }\n",
    "    ground_truth = torch.arange(n, device=sim_matrix.device)\n",
    "    i2t_preds = torch.argmax(sim_matrix, dim=0)\n",
    "    t2i_preds = torch.argmax(sim_matrix, dim=1)\n",
    "    i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "    t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "    avg_acc = (i2t_acc + t2i_acc) / 2.0\n",
    "    avg_cosine_sim = torch.diag(sim_matrix).mean().item()\n",
    "    i2t_recall = {}\n",
    "    t2i_recall = {}\n",
    "    recall_k_values = [k for k in [1, 5, 10] if k <= n]\n",
    "    for k in recall_k_values:\n",
    "        i2t_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=0)\n",
    "        t2i_recall[f\"R@{k}\"] = compute_recall_at_k(sim_matrix, k, dim=1)\n",
    "    for k in [1, 5, 10]:\n",
    "        k_str = f\"R@{k}\"\n",
    "        if k_str not in i2t_recall: i2t_recall[k_str] = 0.0\n",
    "        if k_str not in t2i_recall: t2i_recall[k_str] = 0.0\n",
    "    metrics = {\n",
    "        \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "        \"avg_cosine_sim\": avg_cosine_sim,\n",
    "        \"i2t_recall\": i2t_recall, \"t2i_recall\": t2i_recall\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomImageCaptionDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Dataset Class Definition (Corrected JSON Loading & Processor Update) ===\n",
    "\n",
    "import traceback\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True # Keep allowing truncated images\n",
    "\n",
    "class CustomImageCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads image-caption pairs from JSON metadata.\n",
    "    Handles both single JSON list format and JSON-per-line format.\n",
    "    Uses specified image_processor (SigLIP or other).\n",
    "    \"\"\"\n",
    "    def __init__(self, json_path_or_list, image_base_path, tokenizer, image_processor, max_length):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        if isinstance(json_path_or_list, str) and os.path.isdir(json_path_or_list):\n",
    "             json_files = [os.path.join(json_path_or_list, f) for f in os.listdir(json_path_or_list) if f.endswith('.json')]\n",
    "             print(f\"Found {len(json_files)} JSON files in {json_path_or_list}\")\n",
    "        elif isinstance(json_path_or_list, str) and os.path.isfile(json_path_or_list):\n",
    "            json_files = [json_path_or_list]\n",
    "        elif isinstance(json_path_or_list, list):\n",
    "            json_files = json_path_or_list\n",
    "        else:\n",
    "            raise ValueError(\"json_path_or_list must be a directory, a single JSON file, or a list of JSON files.\")\n",
    "\n",
    "        print(\"Loading JSON metadata...\")\n",
    "        total_loaded_count = 0\n",
    "        for json_path in tqdm(json_files, desc=\"Loading JSONs\"):\n",
    "            try:\n",
    "                with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        file_data = json.load(f)\n",
    "                        if isinstance(file_data, list):\n",
    "                             self.data.extend(file_data)\n",
    "                             total_loaded_count += len(file_data)\n",
    "                        else:\n",
    "                             self.data.append(file_data)\n",
    "                             total_loaded_count += 1\n",
    "                             print(f\"  Warning: Loaded single JSON object from {json_path}, expected a list.\")\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"  Info: Failed to load {json_path} as single JSON. Attempting JSON-per-line format...\")\n",
    "                        f.seek(0)\n",
    "                        count_line_by_line = 0\n",
    "                        for line in f:\n",
    "                            line = line.strip()\n",
    "                            if line:\n",
    "                                try:\n",
    "                                    line_data = json.loads(line)\n",
    "                                    self.data.append(line_data)\n",
    "                                    count_line_by_line += 1\n",
    "                                except json.JSONDecodeError as line_err:\n",
    "                                     print(f\"  ERROR parsing line in {json_path}: {line_err}. Line content (partial): {line[:100]}...\")\n",
    "                        total_loaded_count += count_line_by_line\n",
    "                        if count_line_by_line > 0:\n",
    "                            print(f\"  Successfully loaded {count_line_by_line} items using JSON-per-line format from {json_path}.\")\n",
    "                        else:\n",
    "                             print(f\"  Failed to load any data using JSON-per-line format from {json_path} either.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR opening or processing file {json_path}: {e}\")\n",
    "\n",
    "        print(f\"Loaded {total_loaded_count} samples total from {len(json_files)} file(s).\")\n",
    "        self.data = [item for item in self.data if item]\n",
    "        print(f\"Dataset size after potential cleaning: {len(self.data)}\")\n",
    "\n",
    "        if not self.data:\n",
    "             print(\"WARNING: No data loaded!\")\n",
    "\n",
    "        self.image_base_path = image_base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor # Store the passed processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # --- Get image size from the loaded processor ---\n",
    "        try:\n",
    "            # SiglipImageProcessor uses config.image_size\n",
    "            self.img_size = image_processor.config.image_size\n",
    "        except AttributeError:\n",
    "            # Fallback for BlipImageProcessor or older versions\n",
    "            try:\n",
    "                 if isinstance(image_processor.size, dict):\n",
    "                     proc_size = image_processor.size\n",
    "                     self.img_size = proc_size.get('height', proc_size.get('shortest_edge', 224))\n",
    "                 else:\n",
    "                     self.img_size = image_processor.size\n",
    "                     if isinstance(self.img_size, (tuple, list)): self.img_size = self.img_size[0]\n",
    "            except AttributeError:\n",
    "                 print(\"Warning: Could not determine image size from processor, defaulting to 224.\")\n",
    "                 self.img_size = 224\n",
    "        print(f\"Using image target size: {self.img_size}x{self.img_size}\")\n",
    "        # ----------------------------------------------\n",
    "\n",
    "        if not os.path.isdir(self.image_base_path):\n",
    "            print(f\"WARNING: Image base path does not exist: {os.path.abspath(self.image_base_path)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data): raise IndexError(\"Index out of bounds\")\n",
    "        item = self.data[idx]\n",
    "\n",
    "        relative_image_path = item.get('image_path', item.get('url', item.get('filename')))\n",
    "        caption_data = item.get('caption', item.get('text', item.get('title', '')))\n",
    "        if isinstance(caption_data, list):\n",
    "            caption = caption_data[0] if caption_data else \"\"\n",
    "        elif isinstance(caption_data, str):\n",
    "            caption = caption_data\n",
    "        else:\n",
    "            caption = \"\"\n",
    "\n",
    "        if not relative_image_path or not caption:\n",
    "            return self._get_dummy_item()\n",
    "\n",
    "        # Load Image\n",
    "        try:\n",
    "            image_path = os.path.join(self.image_base_path, relative_image_path)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            # --- Use the stored image processor ---\n",
    "            image_inputs = self.image_processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = image_inputs['pixel_values'].squeeze(0)\n",
    "            # ------------------------------------\n",
    "        except Exception: # Catch broad exceptions during loading/processing\n",
    "            return self._get_dummy_item()\n",
    "\n",
    "        # Process Text\n",
    "        try:\n",
    "            text_inputs = self.tokenizer(\n",
    "                caption, padding='max_length', truncation=True,\n",
    "                max_length=self.max_length, return_tensors='pt'\n",
    "            )\n",
    "            input_ids = text_inputs['input_ids'].squeeze(0)\n",
    "            attention_mask = text_inputs['attention_mask'].squeeze(0)\n",
    "        except Exception:\n",
    "            return self._get_dummy_item()\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "    def _get_dummy_item(self):\n",
    "        # Use self.img_size determined in __init__\n",
    "        return {\n",
    "            \"pixel_values\": torch.zeros((3, self.img_size, self.img_size), dtype=torch.float),\n",
    "            \"input_ids\": torch.zeros(self.max_length, dtype=torch.long),\n",
    "            \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"CustomImageCaptionDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViPhobertSiglip Model components defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Model Definition (SigLIP Vision + PhoBERT Text) ===\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Encodes images using SigLIP's Vision Model.\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        print(f\"Initializing SigLIP Vision Encoder from: {config_train.vision_model_name}\")\n",
    "\n",
    "        if pretrained:\n",
    "            try:\n",
    "                self.vision_model = SiglipVisionModel.from_pretrained(config_train.vision_model_name)\n",
    "                print(\"  SigLIP Vision model loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR loading pretrained SiglipVisionModel: {e}\")\n",
    "                raise # Stop if vision model fails to load\n",
    "        else:\n",
    "            print(\"  Initializing SiglipVisionModel from scratch.\")\n",
    "            siglip_vision_config = SiglipConfig.from_pretrained(config_train.vision_model_name).vision_config\n",
    "            self.vision_model = SiglipVisionModel(siglip_vision_config)\n",
    "\n",
    "        try:\n",
    "            self.input_features = self.vision_model.config.hidden_size\n",
    "        except AttributeError as e:\n",
    "             print(f\"  ERROR accessing vision_model.config.hidden_size: {e}. Attempting config_train value.\")\n",
    "             self.input_features = config_train.vision_embedding # Fallback\n",
    "\n",
    "        if hasattr(config_train, 'vision_embedding') and self.input_features != config_train.vision_embedding:\n",
    "             print(f\"  WARNING: Configured vision_embedding ({config_train.vision_embedding}) doesn't match loaded model hidden size ({self.input_features}). Using actual size.\")\n",
    "        else:\n",
    "             print(f\"  Confirmed/Using vision model hidden size: {self.input_features}\")\n",
    "\n",
    "        self.projection = nn.Linear(self.input_features, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        vision_outputs = self.vision_model(pixel_values=pixel_values, return_dict=True)\n",
    "        image_embed = vision_outputs.pooler_output\n",
    "        projected_features = self.projection(image_embed)\n",
    "        return projected_features\n",
    "\n",
    "# --- TextEncoder remains the same (loading PhoBERT) ---\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Encodes text using PhoBERT-Base.\"\"\"\n",
    "    def __init__(self, config_train, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.config_train = config_train\n",
    "        print(f\"Initializing Text Encoder: {config_train.text_model_name}\")\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(config_train.text_model_name)\n",
    "        else:\n",
    "            model_config = AutoConfig.from_pretrained(config_train.text_model_name)\n",
    "            self.model = AutoModel.from_config(model_config)\n",
    "        try:\n",
    "            self.input_features = self.model.config.hidden_size\n",
    "        except AttributeError as e:\n",
    "            print(f\"  ERROR accessing model.config.hidden_size: {e}. Attempting config_train value.\")\n",
    "            self.input_features = config_train.text_embedding # Fallback\n",
    "        if hasattr(config_train, 'text_embedding') and self.input_features != config_train.text_embedding:\n",
    "             print(f\"  WARNING: Configured text_embedding ({config_train.text_embedding}) doesn't match loaded PhoBERT hidden size ({self.input_features}). Using actual size.\")\n",
    "        else:\n",
    "            print(f\"  Confirmed text model hidden size: {self.input_features}\")\n",
    "        self.projection = nn.Linear(self.input_features, config_train.projection_dim, bias=False)\n",
    "        print(f\"  Added projection head: {self.input_features} -> {config_train.projection_dim}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        text_features = outputs.last_hidden_state[:, 0, :]\n",
    "        projected_features = self.projection(text_features)\n",
    "        return projected_features\n",
    "\n",
    "# --- Combined Model (CLIP-Style) ---\n",
    "class ViPhobertSiglipModel(nn.Module): # Renamed for clarity\n",
    "    \"\"\"Combines SigLIP Vision encoder and PhoBERT Text encoder for contrastive retrieval.\"\"\"\n",
    "    def __init__(self, image_encoder, text_encoder, config_train):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.config_train = config_train\n",
    "\n",
    "        if config_train.learnable_temperature:\n",
    "            init_val_t = torch.tensor(np.log(1 / config_train.temperature), dtype=torch.float)\n",
    "            self.logit_scale = nn.Parameter(init_val_t)\n",
    "            print(f\"Using learnable temperature (logit_scale), initialized to {self.logit_scale.exp().item():.4f}\")\n",
    "        else:\n",
    "            temp_tensor = torch.tensor(np.log(1 / config_train.temperature), dtype=torch.float)\n",
    "            self.register_buffer('logit_scale', temp_tensor)\n",
    "            print(f\"Using fixed temperature: {config_train.temperature}\")\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        pixel_values = pixel_values.to(self.config_train.device)\n",
    "        input_ids = input_ids.to(self.config_train.device)\n",
    "        attention_mask = attention_mask.to(self.config_train.device)\n",
    "\n",
    "        image_embed = self.image_encoder(pixel_values)\n",
    "        text_embed = self.text_encoder(input_ids, attention_mask)\n",
    "\n",
    "        image_features = F.normalize(image_embed, p=2, dim=-1)\n",
    "        text_features = F.normalize(text_embed, p=2, dim=-1)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp().clamp(max=100)\n",
    "\n",
    "        logits_per_image = logit_scale.float() * image_features.float() @ text_features.float().t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text, image_features, text_features\n",
    "\n",
    "print(\"ViPhobertSiglip Model components defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Contrastive loss function defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 7: Loss Function (Standard Contrastive Loss) ===\n",
    "def contrastive_loss(logits_per_image, logits_per_text):\n",
    "    \"\"\" Standard InfoNCE-based contrastive loss \"\"\"\n",
    "    logits_per_image = logits_per_image.float()\n",
    "    logits_per_text = logits_per_text.float()\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    if batch_size == 0:\n",
    "        return torch.tensor(0.0, device=logits_per_image.device, requires_grad=True)\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "    loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "    total_loss = (loss_img + loss_txt) / 2.0\n",
    "    return total_loss\n",
    "\n",
    "print(\"Standard Contrastive loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer: vinai/phobert-base\n",
      "PhoBERT Tokenizer loaded successfully.\n",
      "Loading Image Processor from: google/siglip-base-patch16-224\n",
      "SigLIP Image Processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8: Setup - Tokenizer and Image Processor (Using SigLIP Processor) ===\n",
    "# --- Use SiglipImageProcessor ---\n",
    "from transformers import AutoTokenizer, SiglipImageProcessor\n",
    "\n",
    "tokenizer = None\n",
    "image_processor = None\n",
    "\n",
    "print(f\"Loading Tokenizer: {config.text_tokenizer_name}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.text_tokenizer_name)\n",
    "    print(\"PhoBERT Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading tokenizer '{config.text_tokenizer_name}': {e}\")\n",
    "\n",
    "print(f\"Loading Image Processor from: {config.image_processor_name}\") # Use SigLIP name from CFG\n",
    "try:\n",
    "    # --- Load SiglipImageProcessor ---\n",
    "    image_processor = SiglipImageProcessor.from_pretrained(config.image_processor_name)\n",
    "    print(\"SigLIP Image Processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading image processor '{config.image_processor_name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nCreating datasets...\n",
      "Attempting to load training data from: ./json_data//train.json\n",
      "Loading JSON metadata...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f85c56304048b49302f8c03a4d76c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading JSONs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19844 samples total from 1 file(s).\n",
      "Dataset size after potential cleaning: 19844\n",
      "Using image target size: 224x224\n",
      "Attempting to load validation data from: ./json_data//dev.json\n",
      "Loading JSON metadata...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593cc6bb6062407e9bf855c61f54c7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading JSONs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5667 samples total from 1 file(s).\n",
      "Dataset size after potential cleaning: 5667\n",
      "Using image target size: 224x224\n",
      "\\nCreating dataloaders...\n",
      "Using 20 workers for DataLoaders.\n",
      "Train loader created with 155 batches.\n",
      "Validation loader created with 23 batches.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 9: Setup - Datasets and DataLoaders (FIXED Validation Path) ===\n",
    "# Uses the dataset class defined above.\n",
    "\n",
    "train_loader = None\n",
    "dev_loader = None\n",
    "\n",
    "# Define paths\n",
    "validation_json_path = os.path.join(config.data_path, \"dev.json\") # <<< CHANGED FILENAME\n",
    "train_json_path = os.path.join(config.data_path, \"train.json\")\n",
    "\n",
    "if tokenizer and image_processor:\n",
    "    print(\"\\\\nCreating datasets...\")\n",
    "    # --- Training Dataset ---\n",
    "    try:\n",
    "        print(f\"Attempting to load training data from: {train_json_path}\")\n",
    "        train_dataset = CustomImageCaptionDataset(\n",
    "            json_path_or_list=train_json_path,\n",
    "            image_base_path=config.image_base_path,\n",
    "            tokenizer=tokenizer,\n",
    "            image_processor=image_processor, # Pass the loaded processor\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "        if not train_dataset.data: print(\"\\\\nERROR: Failed to load training data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating training dataset: {e}\")\n",
    "        train_dataset = None\n",
    "\n",
    "    # --- Validation Dataset ---\n",
    "    if os.path.exists(validation_json_path):\n",
    "         try:\n",
    "             print(f\"Attempting to load validation data from: {validation_json_path}\")\n",
    "             dev_dataset = CustomImageCaptionDataset(\n",
    "                 json_path_or_list=validation_json_path,\n",
    "                 image_base_path=config.image_base_path, # Assumes same base path\n",
    "                 tokenizer=tokenizer,\n",
    "                 image_processor=image_processor, # Use same processor\n",
    "                 max_length=config.max_length\n",
    "             )\n",
    "             if not dev_dataset.data: print(\"\\\\nWARNING: Failed to load validation data.\")\n",
    "         except Exception as e:\n",
    "             print(f\"ERROR creating validation dataset: {e}\")\n",
    "             dev_dataset = None\n",
    "    else:\n",
    "        print(f\"Validation JSON file not found at {validation_json_path}, skipping validation set creation.\")\n",
    "        dev_dataset = None\n",
    "\n",
    "    print(\"\\\\nCreating dataloaders...\")\n",
    "    num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "    print(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "    if train_dataset and train_dataset.data:\n",
    "        persist_workers = (num_workers > 0)\n",
    "        try: # Check if persistent_workers is supported\n",
    "             _ = DataLoader(train_dataset, num_workers=num_workers, persistent_workers=persist_workers)\n",
    "        except TypeError:\n",
    "             persist_workers = False\n",
    "             print(\"Note: `persistent_workers=True` not supported by this PyTorch version/DataLoader setup.\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=True, # Keep drop_last=True for more stable training steps\n",
    "            persistent_workers=persist_workers\n",
    "        )\n",
    "        print(f\"Train loader created with {len(train_loader)} batches.\")\n",
    "        # Calculate total training steps (only if using Cosine scheduler)\n",
    "        if config.scheduler_type == \"cosine\":\n",
    "            config.total_training_steps = len(train_loader) * config.epochs // config.accumulation_steps\n",
    "            print(f\"Total estimated training steps for Cosine Scheduler: {config.total_training_steps}\")\n",
    "    else:\n",
    "        print(\"Skipping train loader creation (no data).\")\n",
    "        config.total_training_steps = 0 # Set default if no loader\n",
    "\n",
    "    if dev_dataset and dev_dataset.data:\n",
    "        persist_workers_dev = (num_workers > 0)\n",
    "        try: # Check support for dev loader too\n",
    "             _ = DataLoader(dev_dataset, num_workers=num_workers, persistent_workers=persist_workers_dev)\n",
    "        except TypeError:\n",
    "             persist_workers_dev = False\n",
    "\n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset, batch_size=config.batch_size * 2, shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "            drop_last=False,\n",
    "            persistent_workers=persist_workers_dev\n",
    "        )\n",
    "        print(f\"Validation loader created with {len(dev_loader)} batches.\")\n",
    "    else: print(\"Skipping validation loader creation.\")\n",
    "\n",
    "    if not train_loader: print(\"\\\\nERROR: Train loader could not be created.\")\n",
    "else:\n",
    "     print(\"ERROR: Tokenizer or Image Processor not loaded. Skipping dataset/loader creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nInitializing ViPhobertSiglip model components...\n",
      "Initializing SigLIP Vision Encoder from: google/siglip-base-patch16-224\n",
      "  SigLIP Vision model loaded successfully.\n",
      "  Confirmed/Using vision model hidden size: 768\n",
      "  Added projection head: 768 -> 768\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Confirmed text model hidden size: 768\n",
      "  Added projection head: 768 -> 768\n",
      "Using learnable temperature (logit_scale), initialized to 14.2857\n",
      "\\nViPhobertSiglip Model initialized successfully on cuda.\n",
      "Total parameters: 229.06 M\n",
      "Trainable parameters: 229.06 M\n",
      "\\nSetting up optimizer...\n",
      "Optimizer AdamW initialized with grouped LRs (Vision: 1e-05, Text: 2e-05, Proj/Temp: 0.0001), WD: 0.0001\n",
      "LR Scheduler: ReduceLROnPlateau initialized (mode='max', factor=0.8, patience=2)\n",
      "AMP GradScaler initialized.\n",
      "Early stopping enabled with patience: 5\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10: Setup - Model, Optimizer, Scheduler (Fine-tuning LRs & Corrected AMP) ===\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "scaler = None # For AMP\n",
    "\n",
    "print(\"\\\\nInitializing ViPhobertSiglip model components...\")\n",
    "try:\n",
    "    # Instantiate the encoders and main model\n",
    "    image_encoder = ImageEncoder(config).to(config.device)\n",
    "    text_encoder = TextEncoder(config).to(config.device)\n",
    "    # --- Instantiate the correct model ---\n",
    "    model = ViPhobertSiglipModel(image_encoder, text_encoder, config).to(config.device)\n",
    "\n",
    "    print(f\"\\\\nViPhobertSiglip Model initialized successfully on {config.device}.\")\n",
    "    num_params_total = sum(p.numel() for p in model.parameters())\n",
    "    num_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {num_params_total / 1e6:.2f} M\")\n",
    "    print(f\"Trainable parameters: {num_params_trainable / 1e6:.2f} M\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing model components: {e}\")\n",
    "    traceback.print_exc()\n",
    "    model = None\n",
    "\n",
    "if model and train_loader: # Check train_loader exists\n",
    "    print(\"\\\\nSetting up optimizer...\")\n",
    "    # --- Optimizer with Fine-tuning LRs ---\n",
    "    vision_encoder_params = list(model.image_encoder.vision_model.parameters())\n",
    "    image_head_params = list(model.image_encoder.projection.parameters())\n",
    "    text_encoder_params = list(model.text_encoder.model.parameters())\n",
    "    text_head_params = list(model.text_encoder.projection.parameters())\n",
    "    logit_scale_param = [model.logit_scale] if isinstance(model.logit_scale, nn.Parameter) else []\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for p in vision_encoder_params if p.requires_grad], 'lr': config.vision_encoder_lr, 'weight_decay': config.weight_decay},\n",
    "        {'params': [p for p in image_head_params if p.requires_grad], 'lr': config.projection_lr, 'weight_decay': config.weight_decay},\n",
    "        {'params': [p for p in text_encoder_params if p.requires_grad], 'lr': config.text_encoder_lr, 'weight_decay': config.weight_decay},\n",
    "        {'params': [p for p in text_head_params if p.requires_grad], 'lr': config.projection_lr, 'weight_decay': config.weight_decay},\n",
    "        {'params': [p for p in logit_scale_param if p.requires_grad], 'lr': config.projection_lr, 'weight_decay': 0.0 }\n",
    "    ]\n",
    "\n",
    "    optimizer_grouped_parameters = [g for g in optimizer_grouped_parameters if g['params']]\n",
    "\n",
    "    if not optimizer_grouped_parameters:\n",
    "        print(\"ERROR: No trainable parameters found for the optimizer.\")\n",
    "    else:\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters, lr=config.learning_rate) # Base LR used if param not in group\n",
    "        print(f\"Optimizer AdamW initialized with grouped LRs (Vision: {config.vision_encoder_lr}, Text: {config.text_encoder_lr}, Proj/Temp: {config.projection_lr}), WD: {config.weight_decay}\")\n",
    "\n",
    "        # --- LR Scheduler ---\n",
    "        if config.scheduler_type == \"cosine\":\n",
    "            if hasattr(config, 'total_training_steps') and config.total_training_steps > 0:\n",
    "                 lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "                     optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=config.total_training_steps\n",
    "                 )\n",
    "                 print(f\"LR Scheduler: Cosine with Warmup ({config.warmup_steps} steps) initialized.\")\n",
    "            else:\n",
    "                 print(\"ERROR: total_training_steps not calculated or zero. Cannot init Cosine scheduler.\")\n",
    "                 lr_scheduler = None\n",
    "        elif config.scheduler_type == \"reduce_on_plateau\":\n",
    "            lr_scheduler = ReduceLROnPlateau(\n",
    "                optimizer, mode=config.mode, factor=config.rop_factor, patience=config.rop_patience\n",
    "            )\n",
    "            print(f\"LR Scheduler: ReduceLROnPlateau initialized (mode='{config.mode}', factor={config.rop_factor}, patience={config.rop_patience})\")\n",
    "        else:\n",
    "            print(\"No LR Scheduler specified.\")\n",
    "            lr_scheduler = None\n",
    "\n",
    "        # --- Automatic Mixed Precision (AMP) Scaler ---\n",
    "        if config.use_amp:\n",
    "            scaler = torch.amp.GradScaler('cuda') # <<< CORRECTED\n",
    "            print(\"AMP GradScaler initialized.\")\n",
    "        else:\n",
    "            scaler = None\n",
    "\n",
    "        # Early stopping setup\n",
    "        early_stopping_counter = 0\n",
    "        best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "        print(f\"Early stopping enabled with patience: {config.early_stopping_patience}\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Model not initialized or train_loader not available. Skipping optimizer/scheduler setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step (contrastive) and validation epoch functions defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 11: Training and Validation Functions (Using Contrastive Loss) ===\n",
    "import traceback\n",
    "\n",
    "def train_step(model, batch, optimizer, scaler, device, use_amp):\n",
    "    \"\"\" Performs a single training step with CONTRASTIVE loss and optional AMP \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    pixel_values = batch['pixel_values']\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        logits_per_image, logits_per_text, _, _ = model(pixel_values, input_ids, attention_mask)\n",
    "        loss = contrastive_loss(logits_per_image, logits_per_text) # <<< USE CONTRASTIVE LOSS\n",
    "\n",
    "    if use_amp:\n",
    "        scaler.scale(loss).backward()\n",
    "    else:\n",
    "        loss.backward()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def validate_epoch(model, dataloader, device):\n",
    "    \"\"\" Performs validation, returning metrics \"\"\"\n",
    "    model.eval()\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Validation\", leave=False, unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values']\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=config.use_amp):\n",
    "                logits_per_image, logits_per_text, image_embeds_norm, text_embeds_norm = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "            all_image_embeddings.append(image_embeds_norm.cpu())\n",
    "            all_text_embeddings.append(text_embeds_norm.cpu())\n",
    "\n",
    "    if not all_image_embeddings or not all_text_embeddings:\n",
    "         print(\"Warning: No embeddings collected during validation.\")\n",
    "         return { \"loss\": float('inf'), \"avg_acc\": 0.0, \"avg_cosine_sim\": 0.0,\n",
    "                  \"i2t recall R@1\": 0.0, \"i2t recall R@5\": 0.0, \"i2t recall R@10\": 0.0,\n",
    "                  \"t2i recall R@1\": 0.0, \"t2i recall R@5\": 0.0, \"t2i recall R@10\": 0.0 }\n",
    "\n",
    "    try:\n",
    "        all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "        all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error concatenating embeddings: {e}\")\n",
    "        return { \"loss\": float('inf'), \"avg_acc\": 0.0, \"avg_cosine_sim\": 0.0,\n",
    "                 \"i2t recall R@1\": 0.0, \"i2t recall R@5\": 0.0, \"i2t recall R@10\": 0.0,\n",
    "                 \"t2i recall R@1\": 0.0, \"t2i recall R@5\": 0.0, \"t2i recall R@10\": 0.0 }\n",
    "\n",
    "    # --- Log Temp/Bias (if learnable) ---\n",
    "    current_temp_val = model.logit_scale.exp().item() if isinstance(model.logit_scale, nn.Parameter) else (1 / config.temperature)\n",
    "    print(f\"DEBUG: Validation - Current Temp (exp(logit_scale)): {current_temp_val:.4f}\")\n",
    "    # No bias term in this model version\n",
    "\n",
    "    print(f\"\\\\nComputing metrics over {all_image_embeddings.shape[0]} validation samples...\")\n",
    "    validation_metrics = compute_metrics(all_image_embeddings.to(device), all_text_embeddings.to(device))\n",
    "\n",
    "    # Format results\n",
    "    final_results = {}\n",
    "    for k, v in validation_metrics.items():\n",
    "        if isinstance(v, dict):\n",
    "            for recall_k, recall_v in v.items(): final_results[f\"{k.replace('_', ' ')} {recall_k}\"] = recall_v\n",
    "        else: final_results[k.replace('_', ' ')] = v\n",
    "\n",
    "    del all_image_embeddings, all_text_embeddings\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "    return final_results\n",
    "\n",
    "print(\"Training step (contrastive) and validation epoch functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nStarting ViPhobertSiglip fine-tuning for 200 epochs...\n",
      "Target metric for saving best model: 'avg_acc' (mode: max)\n",
      "\\n--- Epoch 1/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f195fcd73ba4dc69906e504315c351b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E1:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6991/3834100572.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1 Time: 0:03:44.375009 ---\n",
      "--- Average Train Loss for Epoch 1: 2.8569 ---\n",
      "\\n--- Epoch 2/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93c3ddb861d443db86ea12a2ef721d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E2:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 2 Time: 0:03:43.587412 ---\n",
      "--- Average Train Loss for Epoch 2: 1.8134 ---\n",
      "\\n--- Epoch 3/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a277a6515d84ced8883cbf14cb22bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E3:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 3 Time: 0:03:43.739574 ---\n",
      "--- Average Train Loss for Epoch 3: 1.4585 ---\n",
      "\\n--- Epoch 4/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784e542f79ad40e481e555a3521fa7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E4:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 4 Time: 0:03:39.405906 ---\n",
      "--- Average Train Loss for Epoch 4: 1.2300 ---\n",
      "\\n--- Epoch 5/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc6abb4c908465184457ed3bfe95bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E5:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 5 Time: 0:03:42.365247 ---\n",
      "--- Average Train Loss for Epoch 5: 1.0501 ---\n",
      "\\n--- Epoch 6/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6ce813bf18496e9ec5c21f38279cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E6:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 6 Time: 0:03:39.431805 ---\n",
      "--- Average Train Loss for Epoch 6: 0.9034 ---\n",
      "\\n--- Epoch 7/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf363d0e2314bb285f4ac147b69af91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E7:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 1000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924b6459b74944c1b5adaaf2db56f23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/23 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6991/3834100572.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=config.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 15.9135\n",
      "\\nComputing metrics over 5667 validation samples...\n",
      "Validation finished in 89.73s\n",
      "Validation Step 1000: avg acc: 0.0509 | avg cosine sim: 0.4928 | i2t acc: 0.0625 | i2t recall R@1: 0.0625 | i2t recall R@10: 0.3206 | i2t recall R@5: 0.2130 | t2i acc: 0.0394 | t2i recall R@1: 0.0394 | t2i recall R@10: 0.3153 | t2i recall R@5: 0.2008\n",
      "  RoP Scheduler step called with avg_acc=0.0509\n",
      "  Metric 'avg_acc' improved from -inf to 0.0509. Saving best model.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_landmark/phobert_siglip_step_1000.pt\n",
      "--- Epoch 7 Time: 0:04:52.647457 ---\n",
      "--- Average Train Loss for Epoch 7: 0.7878 ---\n",
      "\\n--- Epoch 8/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272d17d02e064b6baa65440fd00abc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E8:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 8 Time: 0:03:41.597004 ---\n",
      "--- Average Train Loss for Epoch 8: 0.6789 ---\n",
      "\\n--- Epoch 9/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a8bbe384e848988678aa04dad986af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E9:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 9 Time: 0:03:41.867911 ---\n",
      "--- Average Train Loss for Epoch 9: 0.5956 ---\n",
      "\\n--- Epoch 10/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938b9d7d011c4b728c4d79945635a733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E10:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 10 Time: 0:03:39.124748 ---\n",
      "--- Average Train Loss for Epoch 10: 0.5238 ---\n",
      "\\n--- Epoch 11/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c90f89a1cf4a92bc5c8c4e88068ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E11:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 11 Time: 0:03:39.536333 ---\n",
      "--- Average Train Loss for Epoch 11: 0.4522 ---\n",
      "\\n--- Epoch 12/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e556a9d399741d18fc51edc44ae3782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E12:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 12 Time: 0:03:36.602843 ---\n",
      "--- Average Train Loss for Epoch 12: 0.4141 ---\n",
      "\\n--- Epoch 13/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c4ab97d5c7495e909e907cd81592e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E13:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 2000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731172b9f2a749f8ad6ba9f74b4d2b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/23 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 17.5294\n",
      "\\nComputing metrics over 5667 validation samples...\n",
      "Validation finished in 76.08s\n",
      "Validation Step 2000: avg acc: 0.0528 | avg cosine sim: 0.4816 | i2t acc: 0.0649 | i2t recall R@1: 0.0649 | i2t recall R@10: 0.3295 | i2t recall R@5: 0.2220 | t2i acc: 0.0408 | t2i recall R@1: 0.0408 | t2i recall R@10: 0.3222 | t2i recall R@5: 0.2080\n",
      "  RoP Scheduler step called with avg_acc=0.0528\n",
      "  Metric 'avg_acc' improved from 0.0509 to 0.0528. Saving best model.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_landmark/phobert_siglip_step_2000.pt\n",
      "--- Epoch 13 Time: 0:04:58.834033 ---\n",
      "--- Average Train Loss for Epoch 13: 0.3616 ---\n",
      "\\n--- Epoch 14/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff0d372ab664f46a2c60be5819626e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E14:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 14 Time: 0:03:38.039351 ---\n",
      "--- Average Train Loss for Epoch 14: 0.3314 ---\n",
      "\\n--- Epoch 15/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5a4ccde57d4b3bb1ded649323a69df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E15:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 15 Time: 0:03:37.763190 ---\n",
      "--- Average Train Loss for Epoch 15: 0.2960 ---\n",
      "\\n--- Epoch 16/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e58a73ba904415c9f3e342272810904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E16:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 16 Time: 0:03:38.471972 ---\n",
      "--- Average Train Loss for Epoch 16: 0.2727 ---\n",
      "\\n--- Epoch 17/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44aa47511ead4a4ea0cbdbe731a7cb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E17:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 17 Time: 0:03:38.386952 ---\n",
      "--- Average Train Loss for Epoch 17: 0.2503 ---\n",
      "\\n--- Epoch 18/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd38029d4cf2400b81de1ebb962add98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E18:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 18 Time: 0:03:39.378812 ---\n",
      "--- Average Train Loss for Epoch 18: 0.2354 ---\n",
      "\\n--- Epoch 19/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979046be122648ec9204d6613c576ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E19:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 19 Time: 0:03:40.236961 ---\n",
      "--- Average Train Loss for Epoch 19: 0.2072 ---\n",
      "\\n--- Epoch 20/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cac449d64d48bc81e980fb118842c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E20:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 3000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02c97b612d04b728c44b66710c356c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/23 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 18.9503\n",
      "\\nComputing metrics over 5667 validation samples...\n",
      "Validation finished in 98.91s\n",
      "Validation Step 3000: avg acc: 0.0541 | avg cosine sim: 0.4613 | i2t acc: 0.0667 | i2t recall R@1: 0.0667 | i2t recall R@10: 0.3319 | i2t recall R@5: 0.2238 | t2i acc: 0.0415 | t2i recall R@1: 0.0415 | t2i recall R@10: 0.3189 | t2i recall R@5: 0.2123\n",
      "  RoP Scheduler step called with avg_acc=0.0541\n",
      "  Metric 'avg_acc' improved from 0.0528 to 0.0541. Saving best model.\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_landmark/phobert_siglip_step_3000.pt\n",
      "--- Epoch 20 Time: 0:05:02.139290 ---\n",
      "--- Average Train Loss for Epoch 20: 0.1937 ---\n",
      "\\n--- Epoch 21/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c9baa0904345d995224ab4be6c6a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E21:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 21 Time: 0:03:38.388800 ---\n",
      "--- Average Train Loss for Epoch 21: 0.1832 ---\n",
      "\\n--- Epoch 22/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c4aec6665e43d28dfc183748bbb6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E22:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 22 Time: 0:03:37.853652 ---\n",
      "--- Average Train Loss for Epoch 22: 0.1702 ---\n",
      "\\n--- Epoch 23/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69764a9ef4364b95812a2d6b53a41d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E23:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 23 Time: 0:03:36.871123 ---\n",
      "--- Average Train Loss for Epoch 23: 0.1564 ---\n",
      "\\n--- Epoch 24/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e2eeb0199e49cfbfc729b2d3abc474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E24:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 24 Time: 0:03:38.521454 ---\n",
      "--- Average Train Loss for Epoch 24: 0.1532 ---\n",
      "\\n--- Epoch 25/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69b17012f4645f98334df743d7d19cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E25:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 25 Time: 0:03:41.461113 ---\n",
      "--- Average Train Loss for Epoch 25: 0.1419 ---\n",
      "\\n--- Epoch 26/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5600f246444e1e9b911239f7b22a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E26:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 4000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a161184729284b9f9e08fca1b3ad6642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/23 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 20.2548\n",
      "\\nComputing metrics over 5667 validation samples...\n",
      "Validation finished in 76.61s\n",
      "Validation Step 4000: avg acc: 0.0522 | avg cosine sim: 0.4527 | i2t acc: 0.0625 | i2t recall R@1: 0.0625 | i2t recall R@10: 0.3400 | i2t recall R@5: 0.2259 | t2i acc: 0.0420 | t2i recall R@1: 0.0420 | t2i recall R@10: 0.3243 | t2i recall R@5: 0.2172\n",
      "  RoP Scheduler step called with avg_acc=0.0522\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0541. Counter: 1/5\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_landmark/phobert_siglip_step_4000.pt\n",
      "--- Epoch 26 Time: 0:04:54.266825 ---\n",
      "--- Average Train Loss for Epoch 26: 0.1314 ---\n",
      "\\n--- Epoch 27/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b086fca180541d8b37a922d03aa45fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E27:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 27 Time: 0:03:36.124282 ---\n",
      "--- Average Train Loss for Epoch 27: 0.1256 ---\n",
      "\\n--- Epoch 28/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf62692debc647388ea7f6e567280296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E28:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 28 Time: 0:03:38.773381 ---\n",
      "--- Average Train Loss for Epoch 28: 0.1199 ---\n",
      "\\n--- Epoch 29/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5360c34b299747199d72ce0878ea8129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E29:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 29 Time: 0:03:39.494912 ---\n",
      "--- Average Train Loss for Epoch 29: 0.1171 ---\n",
      "\\n--- Epoch 30/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe0ce632a754e34864e1e3e6d05f2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E30:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 30 Time: 0:03:40.792321 ---\n",
      "--- Average Train Loss for Epoch 30: 0.1130 ---\n",
      "\\n--- Epoch 31/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7ecbb34b2048c693b58379b6feb0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E31:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 31 Time: 0:03:38.893088 ---\n",
      "--- Average Train Loss for Epoch 31: 0.1052 ---\n",
      "\\n--- Epoch 32/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11f01614872450a9207ec7139e37761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E32:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 32 Time: 0:03:37.917491 ---\n",
      "--- Average Train Loss for Epoch 32: 0.1046 ---\n",
      "\\n--- Epoch 33/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154aec98d34145bda5d3216c94cad54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E33:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 5000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc09755fdf084eb9845de32c117d22f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/23 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 21.5442\n",
      "\\nComputing metrics over 5667 validation samples...\n",
      "Validation finished in 96.23s\n",
      "Validation Step 5000: avg acc: 0.0540 | avg cosine sim: 0.4354 | i2t acc: 0.0663 | i2t recall R@1: 0.0663 | i2t recall R@10: 0.3229 | i2t recall R@5: 0.2188 | t2i acc: 0.0416 | t2i recall R@1: 0.0416 | t2i recall R@10: 0.3196 | t2i recall R@5: 0.2110\n",
      "  RoP Scheduler step called with avg_acc=0.0540\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0541. Counter: 2/5\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_landmark/phobert_siglip_step_5000.pt\n",
      "--- Epoch 33 Time: 0:04:53.425101 ---\n",
      "--- Average Train Loss for Epoch 33: 0.0980 ---\n",
      "\\n--- Epoch 34/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cae415b2cf41dc90e87315e1fb4cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E34:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 34 Time: 0:03:37.846463 ---\n",
      "--- Average Train Loss for Epoch 34: 0.1015 ---\n",
      "\\n--- Epoch 35/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2222b506550245f1b5d306aab2d0b0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E35:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 35 Time: 0:03:37.936707 ---\n",
      "--- Average Train Loss for Epoch 35: 0.0975 ---\n",
      "\\n--- Epoch 36/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37f11d5dd344ac79446d7bc34952b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E36:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 36 Time: 0:03:36.124224 ---\n",
      "--- Average Train Loss for Epoch 36: 0.0906 ---\n",
      "\\n--- Epoch 37/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7feb25ce14344363af49fa2f47448ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E37:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 37 Time: 0:03:37.061576 ---\n",
      "--- Average Train Loss for Epoch 37: 0.0902 ---\n",
      "\\n--- Epoch 38/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb06eefb2d045f5831e958556232c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E38:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 38 Time: 0:03:41.274982 ---\n",
      "--- Average Train Loss for Epoch 38: 0.0854 ---\n",
      "\\n--- Epoch 39/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347c367851c64d549274f27b133875ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E39:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 6000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6386a22f3a6a4164819d095e7a57c9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/23 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 22.9077\n",
      "\\nComputing metrics over 5667 validation samples...\n",
      "Validation finished in 88.02s\n",
      "Validation Step 6000: avg acc: 0.0503 | avg cosine sim: 0.4298 | i2t acc: 0.0581 | i2t recall R@1: 0.0581 | i2t recall R@10: 0.3220 | i2t recall R@5: 0.2165 | t2i acc: 0.0425 | t2i recall R@1: 0.0425 | t2i recall R@10: 0.3152 | t2i recall R@5: 0.2052\n",
      "  RoP Scheduler step called with avg_acc=0.0503\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0541. Counter: 3/5\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_landmark/phobert_siglip_step_6000.pt\n",
      "--- Epoch 39 Time: 0:04:53.290843 ---\n",
      "--- Average Train Loss for Epoch 39: 0.0786 ---\n",
      "\\n--- Epoch 40/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7db12be0294559b388636ef106e740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E40:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 40 Time: 0:03:36.920813 ---\n",
      "--- Average Train Loss for Epoch 40: 0.0740 ---\n",
      "\\n--- Epoch 41/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fe959f8c014180bddb5d0f952098b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E41:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 41 Time: 0:03:38.853138 ---\n",
      "--- Average Train Loss for Epoch 41: 0.0701 ---\n",
      "\\n--- Epoch 42/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7d7fe35b734089965a27f289383301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E42:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 42 Time: 0:03:36.627609 ---\n",
      "--- Average Train Loss for Epoch 42: 0.0658 ---\n",
      "\\n--- Epoch 43/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3379b8083b346418d2b4f827d74e55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E43:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 43 Time: 0:03:39.386015 ---\n",
      "--- Average Train Loss for Epoch 43: 0.0690 ---\n",
      "\\n--- Epoch 44/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc996bdf7d34a69a737e57d6616beab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E44:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 44 Time: 0:03:39.298916 ---\n",
      "--- Average Train Loss for Epoch 44: 0.0653 ---\n",
      "\\n--- Epoch 45/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de166e869fce465e95f72d000f91aec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E45:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 45 Time: 0:03:38.964830 ---\n",
      "--- Average Train Loss for Epoch 45: 0.0614 ---\n",
      "\\n--- Epoch 46/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adaf1416ac7474b834773772cd92245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E46:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 7000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baccb045fb904915a147a9efb3bbe59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/23 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 23.9653\n",
      "\\nComputing metrics over 5667 validation samples...\n",
      "Validation finished in 110.40s\n",
      "Validation Step 7000: avg acc: 0.0510 | avg cosine sim: 0.4164 | i2t acc: 0.0600 | i2t recall R@1: 0.0600 | i2t recall R@10: 0.3219 | i2t recall R@5: 0.2156 | t2i acc: 0.0420 | t2i recall R@1: 0.0420 | t2i recall R@10: 0.3187 | t2i recall R@5: 0.2073\n",
      "  RoP Scheduler step called with avg_acc=0.0510\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0541. Counter: 4/5\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_landmark/phobert_siglip_step_7000.pt\n",
      "--- Epoch 46 Time: 0:04:57.995865 ---\n",
      "--- Average Train Loss for Epoch 46: 0.0646 ---\n",
      "\\n--- Epoch 47/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f4df80effe4399bc89227e02f61860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E47:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 47 Time: 0:03:41.476538 ---\n",
      "--- Average Train Loss for Epoch 47: 0.0640 ---\n",
      "\\n--- Epoch 48/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6f477b6df748f9948695b7da4ad7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E48:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 48 Time: 0:03:39.364316 ---\n",
      "--- Average Train Loss for Epoch 48: 0.0609 ---\n",
      "\\n--- Epoch 49/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58562c20050f44388da014cba4ad404c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E49:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 49 Time: 0:03:39.100635 ---\n",
      "--- Average Train Loss for Epoch 49: 0.0615 ---\n",
      "\\n--- Epoch 50/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116172c9b3f2445e979ae57570fb7404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E50:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 50 Time: 0:03:39.093310 ---\n",
      "--- Average Train Loss for Epoch 50: 0.0635 ---\n",
      "\\n--- Epoch 51/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c301ea7c789949ad9c453ae09c8bb8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E51:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 51 Time: 0:03:38.175345 ---\n",
      "--- Average Train Loss for Epoch 51: 0.0593 ---\n",
      "\\n--- Epoch 52/200 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e564f2a5194bbb96244cd9c99157e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training E52:   0%|          | 0/155 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning validation at step 8000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8d5ec525cc4ad4a7cb63317d4fdd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/23 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 25.1713\n",
      "\\nComputing metrics over 5667 validation samples...\n",
      "Validation finished in 99.54s\n",
      "Validation Step 8000: avg acc: 0.0501 | avg cosine sim: 0.4177 | i2t acc: 0.0600 | i2t recall R@1: 0.0600 | i2t recall R@10: 0.3175 | i2t recall R@5: 0.2135 | t2i acc: 0.0402 | t2i recall R@1: 0.0402 | t2i recall R@10: 0.3120 | t2i recall R@5: 0.2036\n",
      "  RoP Scheduler step called with avg_acc=0.0501\n",
      "  Metric 'avg_acc' did not improve. Best: 0.0541. Counter: 5/5\n",
      "  Saving periodic checkpoint to ./trained_models/ViSigLIP_landmark/phobert_siglip_step_8000.pt\n",
      "\\nEarly stopping triggered after 5 validation checks without improvement.\n",
      "--- Epoch 52 Time: 0:04:20.665420 ---\n",
      "--- Average Train Loss for Epoch 52: 0.0582 ---\n",
      "=============== Fine-tuning Finished ================\n",
      "Total Training Time: 3:19:39.774545\n",
      "Final model state saved to ./trained_models/ViSigLIP_landmark/phobert_siglip_final.pt\n",
      "Best model based on 'avg_acc' (0.0541) is saved at: ./trained_models/ViSigLIP_landmark/phobert_siglip_best.pt\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "# === Cell 12: Fine-tuning Loop ===\n",
    "import datetime\n",
    "\n",
    "if model and train_loader and optimizer:  # Basic check\n",
    "    print(f\"\\\\nStarting ViPhobertSiglip fine-tuning for {config.epochs} epochs...\") # Updated print\n",
    "    print(f\"Target metric for saving best model: '{config.metric_to_track}' (mode: {config.mode})\")\n",
    "\n",
    "    best_val_metric = -float('inf') if config.mode == \"max\" else float('inf')\n",
    "    global_step = 0\n",
    "    total_loss_since_log = 0.0\n",
    "    steps_since_log = 0\n",
    "    start_train_time = time.time()\n",
    "    early_stopping_counter = 0 # Initialize here\n",
    "\n",
    "    history = {'steps': [], 'train_loss': [], 'val_metrics': {}} # Use steps for logging x-axis\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Training E{epoch+1}\", leave=True, unit=\"batch\")\n",
    "        epoch_loss_meter = AvgMeter(f\"Train Loss E{epoch+1}\") # Track epoch average loss\n",
    "\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            # Skip dummy batches if any errors occurred during data loading\n",
    "            if batch['pixel_values'].shape[0] < config.batch_size and torch.all(batch['pixel_values'] == 0):\n",
    "                continue\n",
    "\n",
    "            # --- Training Step ---\n",
    "            loss = train_step(model, batch, optimizer, scaler, config.device, config.use_amp)\n",
    "            epoch_loss_meter.update(loss, batch['pixel_values'].shape[0]) # Update epoch meter\n",
    "\n",
    "            # Accumulate loss for logging interval\n",
    "            loss_normalized_for_log = loss / config.accumulation_steps\n",
    "            total_loss_since_log += loss_normalized_for_log\n",
    "            steps_since_log += 1\n",
    "\n",
    "            # --- Gradient Accumulation & Optimizer Step ---\n",
    "            is_update_step = (global_step + 1) % config.accumulation_steps == 0\n",
    "            if is_update_step:\n",
    "                # Unscale gradients before clipping (if needed) and optimizer step\n",
    "                if config.use_amp:\n",
    "                    scaler.unscale_(optimizer) # Unscales the gradients of optimizer's assigned params in-place\n",
    "                    # Optional: Gradient Clipping\n",
    "                    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    # Optional: Gradient Clipping\n",
    "                    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # --- LR Scheduler Step (per optimizer step for Cosine, skipped for RoP here) ---\n",
    "                if lr_scheduler and config.scheduler_type == \"cosine\":\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "            global_step += 1 # Increment global step after processing a batch\n",
    "\n",
    "            # --- Logging ---\n",
    "            if global_step % config.log_interval_steps == 0 and steps_since_log > 0:\n",
    "                avg_loss = total_loss_since_log / steps_since_log\n",
    "                current_lr = optimizer.param_groups[0]['lr'] # Get first group's LR for logging\n",
    "                progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\", lr=f\"{current_lr:.2e}\", step=f\"{global_step}\")\n",
    "                history['steps'].append(global_step)\n",
    "                history['train_loss'].append(avg_loss)\n",
    "                total_loss_since_log = 0.0\n",
    "                steps_since_log = 0\n",
    "\n",
    "            # --- Validation & Checkpointing (Based on Steps) ---\n",
    "            if dev_loader and global_step % config.validation_interval_steps == 0 and global_step > 0:\n",
    "                print(f\"\\\\nRunning validation at step {global_step}...\")\n",
    "                val_start_time = time.time()\n",
    "                val_results = validate_epoch(model, dev_loader, config.device)\n",
    "                val_end_time = time.time()\n",
    "                print(f\"Validation finished in {val_end_time - val_start_time:.2f}s\")\n",
    "\n",
    "                metric_log_str = f\"  Validation Step {global_step}: \"\n",
    "                history['val_metrics'][global_step] = val_results\n",
    "                sorted_keys = sorted(val_results.keys())\n",
    "                for name in sorted_keys:\n",
    "                    metric_log_str += f\"{name}: {val_results[name]:.4f} | \"\n",
    "                print(metric_log_str.strip(\" | \"))\n",
    "\n",
    "                # --- Scheduler Step (ReduceLROnPlateau) ---\n",
    "                current_val_metric_for_scheduler = val_results.get(config.metric_to_track.replace('_', ' '), None)\n",
    "                if lr_scheduler and config.scheduler_type == \"reduce_on_plateau\":\n",
    "                     if current_val_metric_for_scheduler is not None:\n",
    "                         lr_scheduler.step(current_val_metric_for_scheduler)\n",
    "                         print(f\"  RoP Scheduler step called with {config.metric_to_track}={current_val_metric_for_scheduler:.4f}\")\n",
    "                     else:\n",
    "                         print(f\"  Warning: Metric '{config.metric_to_track}' not found. RoP Scheduler not stepped.\")\n",
    "\n",
    "                # --- Save Checkpoint Logic ---\n",
    "                current_val_metric = val_results.get(config.metric_to_track.replace('_', ' '), None)\n",
    "                is_best = False\n",
    "                save_path = None\n",
    "                save_path_periodic = None\n",
    "\n",
    "                if current_val_metric is not None:\n",
    "                    improvement_threshold = best_val_metric + config.early_stopping_min_delta if config.mode == \"max\" else best_val_metric - config.early_stopping_min_delta\n",
    "                    if config.mode == \"max\": is_best = current_val_metric > improvement_threshold\n",
    "                    else: is_best = current_val_metric < improvement_threshold\n",
    "\n",
    "                    if is_best:\n",
    "                        print(f\"  Metric '{config.metric_to_track}' improved from {best_val_metric:.4f} to {current_val_metric:.4f}. Saving best model.\")\n",
    "                        best_val_metric = current_val_metric\n",
    "                        early_stopping_counter = 0 # Reset counter\n",
    "                        # --- Use new checkpoint name ---\n",
    "                        save_path = os.path.join(config.model_path, \"phobert_siglip_best.pt\")\n",
    "                    else:\n",
    "                        early_stopping_counter += 1\n",
    "                        print(f\"  Metric '{config.metric_to_track}' did not improve. Best: {best_val_metric:.4f}. Counter: {early_stopping_counter}/{config.early_stopping_patience}\")\n",
    "\n",
    "                    # Save periodic checkpoint\n",
    "                    if global_step % config.save_interval_steps == 0:\n",
    "                        # --- Use new checkpoint name ---\n",
    "                        periodic_save_path = os.path.join(config.model_path, f\"phobert_siglip_step_{global_step}.pt\")\n",
    "                        if save_path != periodic_save_path:\n",
    "                            print(f\"  Saving periodic checkpoint to {periodic_save_path}\")\n",
    "                            save_path_periodic = periodic_save_path\n",
    "\n",
    "                    # Prepare Save Dictionary & Save\n",
    "                    if save_path or save_path_periodic:\n",
    "                        save_dict = {\n",
    "                            'step': global_step, 'epoch': epoch + 1,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_metric': best_val_metric,\n",
    "                            'metric_tracked': config.metric_to_track,\n",
    "                            'current_val_metrics': val_results,\n",
    "                            # Save relevant configs\n",
    "                            'vision_model_name': config.vision_model_name, # Use specific names\n",
    "                            'text_model_name': config.text_model_name,\n",
    "                            'projection_dim': config.projection_dim,\n",
    "                            'learnable_temperature': config.learnable_temperature,\n",
    "                            'temperature': config.temperature, # Save base temp\n",
    "                            'max_length': config.max_length,\n",
    "                        }\n",
    "                        if lr_scheduler: save_dict['scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "                        if scaler: save_dict['scaler_state_dict'] = scaler.state_dict()\n",
    "                        if save_path: torch.save(save_dict, save_path)\n",
    "                        if save_path_periodic: torch.save(save_dict, save_path_periodic)\n",
    "                else:\n",
    "                    print(f\"  Warning: Metric '{config.metric_to_track}' not found. Cannot save best or check early stopping.\")\n",
    "                    early_stopping_counter += 1 # Still count as no improvement\n",
    "\n",
    "                # --- Early Stopping Check ---\n",
    "                if early_stopping_counter >= config.early_stopping_patience:\n",
    "                    print(f\"\\\\nEarly stopping triggered after {early_stopping_counter} validation checks without improvement.\")\n",
    "                    break # Break INNER loop\n",
    "\n",
    "                model.train() # Reset model to train mode\n",
    "\n",
    "        # --- End of Epoch ---\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"--- Epoch {epoch+1} Time: {datetime.timedelta(seconds=epoch_end_time - epoch_start_time)} ---\")\n",
    "        print(f\"--- Average Train Loss for Epoch {epoch+1}: {epoch_loss_meter.avg:.4f} ---\") # Log epoch loss\n",
    "\n",
    "        # Break OUTER loop if early stopping was triggered\n",
    "        if early_stopping_counter >= config.early_stopping_patience:\n",
    "           break\n",
    "\n",
    "    # --- End of Training ---\n",
    "    end_train_time = time.time()\n",
    "    total_duration = datetime.timedelta(seconds=end_train_time - start_train_time)\n",
    "    print(f\"=============== Fine-tuning Finished ================\") # Updated print\n",
    "    print(f\"Total Training Time: {total_duration}\")\n",
    "\n",
    "    # Save final model state\n",
    "    # --- Use new checkpoint name ---\n",
    "    final_model_path = os.path.join(config.model_path, 'phobert_siglip_final.pt')\n",
    "    final_save_dict = {\n",
    "        'step': global_step, 'epoch': epoch + 1, # Save last completed epoch\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_metric': best_val_metric,\n",
    "        'metric_tracked': config.metric_to_track,\n",
    "        'vision_model_name': config.vision_model_name,\n",
    "        'text_model_name': config.text_model_name,\n",
    "        'projection_dim': config.projection_dim,\n",
    "        'learnable_temperature': config.learnable_temperature,\n",
    "        'temperature': config.temperature,\n",
    "        'max_length': config.max_length,\n",
    "    }\n",
    "    if lr_scheduler: final_save_dict['scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "    if scaler: final_save_dict['scaler_state_dict'] = scaler.state_dict()\n",
    "    torch.save(final_save_dict, final_model_path)\n",
    "    print(f\"Final model state saved to {final_model_path}\")\n",
    "\n",
    "    # --- Use new checkpoint name ---\n",
    "    best_model_file = os.path.join(config.model_path, \"phobert_siglip_best.pt\")\n",
    "    if dev_loader and os.path.exists(best_model_file):\n",
    "        print(f\"Best model based on '{config.metric_to_track}' ({best_val_metric:.4f}) is saved at: {best_model_file}\")\n",
    "    elif dev_loader: print(\"Best model checkpoint file not found (or validation was skipped/no improvement).\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Prerequisites for training (model, dataloader, optimizer) not met. Training loop skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=============== Starting Test Set Evaluation ===============\n",
      "Loading test data from: ./json_data//test.json\n",
      "Loading JSON metadata...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d487c435c26465995fa751d5e66be7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading JSONs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2845 samples total from 1 file(s).\n",
      "Dataset size after potential cleaning: 2845\n",
      "Using image target size: 224x224\n",
      "Test loader created with 12 batches.\n",
      "\\nLoading best model: ./trained_models/ViSigLIP_landmark/phobert_siglip_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6991/3621274867.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(load_path, map_location=config.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-creating model structure for testing...\n",
      "  Using Vision Source: google/siglip-base-patch16-224\n",
      "  Using Text Model: vinai/phobert-base\n",
      "Initializing SigLIP Vision Encoder from: google/siglip-base-patch16-224\n",
      "  SigLIP Vision model loaded successfully.\n",
      "  Confirmed/Using vision model hidden size: 768\n",
      "  Added projection head: 768 -> 768\n",
      "Initializing Text Encoder: vinai/phobert-base\n",
      "  Confirmed text model hidden size: 768\n",
      "  Added projection head: 768 -> 768\n",
      "Using learnable temperature (logit_scale), initialized to 14.2857\n",
      "  State dict loading result: <All keys matched successfully>\n",
      "Model weights loaded successfully.\n",
      "\\nRunning evaluation on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08733922c0b498dacfaea3a531c6add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/12 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6991/3834100572.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=config.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation - Current Temp (exp(logit_scale)): 18.9503\n",
      "\\nComputing metrics over 2845 validation samples...\n",
      "\\n--- Test Set Results ---\n",
      "avg acc: 0.0738\\n  avg cosine sim: 0.4615\\n  i2t acc: 0.0896\\n  i2t recall R@1: 0.0896\\n  i2t recall R@10: 0.4541\\n  i2t recall R@5: 0.2956\\n  t2i acc: 0.0580\\n  t2i recall R@1: 0.0580\\n  t2i recall R@10: 0.4457\\n  t2i recall R@5: 0.2970\\n\n",
      "------------------------\n",
      "\\n================= Evaluation Finished ==================\n"
     ]
    }
   ],
   "source": [
    "# === Cell 13: Final Evaluation on Test Set (Updated for Phobert+Siglip) ===\n",
    "import traceback\n",
    "from types import SimpleNamespace\n",
    "from collections import OrderedDict\n",
    "\n",
    "print(\"\\\\n=============== Starting Test Set Evaluation ===============\")\n",
    "\n",
    "test_loader = None\n",
    "model_to_test = None\n",
    "\n",
    "test_json_path = os.path.join(config.data_path, \"test.json\")\n",
    "test_image_path = config.image_base_path\n",
    "\n",
    "# 1. Check prerequisites & Create Test Loader\n",
    "if os.path.exists(test_json_path) and 'tokenizer' in globals() and tokenizer and 'image_processor' in globals() and image_processor:\n",
    "    print(f\"Loading test data from: {test_json_path}\")\n",
    "    try:\n",
    "        test_dataset = CustomImageCaptionDataset(\n",
    "            json_path_or_list=test_json_path, image_base_path=test_image_path,\n",
    "            tokenizer=tokenizer, image_processor=image_processor,\n",
    "            max_length=config.max_length\n",
    "        )\n",
    "        if test_dataset.data:\n",
    "            num_workers = min(config.num_workers, os.cpu_count() if os.cpu_count() else 1)\n",
    "            persist_workers_test = (num_workers > 0)\n",
    "            try: _ = DataLoader(test_dataset, num_workers=num_workers, persistent_workers=persist_workers_test)\n",
    "            except TypeError: persist_workers_test = False\n",
    "\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=config.batch_size * 2, shuffle=False,\n",
    "                num_workers=num_workers, pin_memory=True if config.device == torch.device(\"cuda\") else False,\n",
    "                drop_last=False, persistent_workers=persist_workers_test\n",
    "            )\n",
    "            print(f\"Test loader created with {len(test_loader)} batches.\")\n",
    "        else: print(\"Test dataset loaded but is empty.\")\n",
    "    except Exception as e: print(f\"Error creating test dataset/loader: {e}\")\n",
    "else: print(\"Skipping test evaluation: Test JSON, Tokenizer or Image Processor not found/loaded.\")\n",
    "\n",
    "\n",
    "# 2. Load Model for Testing\n",
    "if test_loader:\n",
    "    try:\n",
    "        # --- Use updated checkpoint names ---\n",
    "        best_model_path = os.path.join(config.model_path, \"phobert_siglip_best.pt\")\n",
    "        final_model_path = os.path.join(config.model_path, \"phobert_siglip_final.pt\")\n",
    "        load_path = None\n",
    "\n",
    "        if os.path.exists(best_model_path):\n",
    "             load_path = best_model_path\n",
    "             print(f\"\\\\nLoading best model: {load_path}\")\n",
    "        elif os.path.exists(final_model_path):\n",
    "             load_path = final_model_path\n",
    "             print(f\"\\\\nLoading final model (best not found): {load_path}\")\n",
    "        else:\n",
    "            print(f\"\\\\nWARNING: No checkpoints found in {config.model_path} to evaluate.\")\n",
    "\n",
    "        if load_path:\n",
    "            checkpoint = torch.load(load_path, map_location=config.device)\n",
    "            print(\"Re-creating model structure for testing...\")\n",
    "\n",
    "            # --- Create temp config based on saved checkpoint ---\n",
    "            temp_config_dict = {\n",
    "                'device': config.device,\n",
    "                # Use model names saved in checkpoint\n",
    "                'vision_model_name': checkpoint.get('vision_model_name', config.selected_vision_source),\n",
    "                'text_model_name': checkpoint.get('text_model_name', config.selected_text_model),\n",
    "                # Use embedding sizes from current config (should match base models)\n",
    "                'vision_embedding': config.vision_embedding,\n",
    "                'text_embedding': config.text_embedding,\n",
    "                # Get these from checkpoint or current config\n",
    "                'projection_dim': checkpoint.get('projection_dim', config.projection_dim),\n",
    "                'learnable_temperature': checkpoint.get('learnable_temperature', config.learnable_temperature),\n",
    "                'temperature': checkpoint.get('temperature', config.temperature),\n",
    "                # Bias is not used/saved in this setup\n",
    "                'learnable_bias': False,\n",
    "                'bias_init': 0.0,\n",
    "            }\n",
    "            temp_config = SimpleNamespace(**temp_config_dict)\n",
    "\n",
    "            print(f\"  Using Vision Source: {temp_config.vision_model_name}\")\n",
    "            print(f\"  Using Text Model: {temp_config.text_model_name}\")\n",
    "\n",
    "            # --- Instantiate the CORRECT model class ---\n",
    "            test_image_encoder = ImageEncoder(temp_config).to(config.device)\n",
    "            test_text_encoder = TextEncoder(temp_config).to(config.device)\n",
    "            model_to_test = ViPhobertSiglipModel(test_image_encoder, test_text_encoder, temp_config).to(config.device)\n",
    "            # -----------------------------------------\n",
    "\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "                print(\"Detected 'module.' prefix, removing.\")\n",
    "                state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "\n",
    "            load_result = model_to_test.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"  State dict loading result: {load_result}\")\n",
    "            if load_result.missing_keys: print(f\"  Warning: Missing keys: {load_result.missing_keys}\")\n",
    "            if load_result.unexpected_keys: print(f\"  Warning: Unexpected keys: {load_result.unexpected_keys}\")\n",
    "            print(f\"Model weights loaded successfully.\")\n",
    "\n",
    "            print(\"\\\\nRunning evaluation on test set...\")\n",
    "            test_results = validate_epoch(model_to_test, test_loader, config.device)\n",
    "\n",
    "            print(\"\\\\n--- Test Set Results ---\")\n",
    "            metric_log_str = \"\"\n",
    "            sorted_keys = sorted(test_results.keys())\n",
    "            for name in sorted_keys: metric_log_str += f\"  {name}: {test_results[name]:.4f}\\\\n\"\n",
    "            print(metric_log_str.strip())\n",
    "            print(\"------------------------\")\n",
    "        else:\n",
    "            print(\"Evaluation skipped (no weights found).\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\nERROR during test setup/evaluation: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\\\n================= Evaluation Finished ==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot directory ensured at: /home/researcher/huypq69/2ndrun/TuningModels/train_plot/ViSigLIP_landmark\n",
      "Saved training loss plot to: ./train_plot/ViSigLIP_landmark/training_loss_step.png\n",
      "Saved validation metrics plot to: ./train_plot/ViSigLIP_landmark/validation_metrics_step.png\n"
     ]
    }
   ],
   "source": [
    "# === Cell 14: Training Visualization (Adapted for Steps/Epochs) ===\n",
    "# This function plots based on epochs if validation runs per epoch,\n",
    "# or steps if validation runs based on steps.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math # Import math\n",
    "\n",
    "def plot_training_metrics(history, plot_dir, plot_by='epoch'):\n",
    "    \"\"\"Plots training and validation metrics.\"\"\"\n",
    "\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    print(f\"Plot directory ensured at: {os.path.abspath(plot_dir)}\")\n",
    "\n",
    "    if not history:\n",
    "        print(\"No history data provided.\")\n",
    "        return\n",
    "\n",
    "    # Determine x-axis based on available data and preference\n",
    "    if plot_by == 'step' and history.get('steps') and history.get('train_loss'):\n",
    "        x_axis_train = history['steps']\n",
    "        x_label = 'Global Steps'\n",
    "        x_axis_val = sorted(history.get('val_metrics', {}).keys()) if history.get('val_metrics') else []\n",
    "    elif history.get('train_loss') and history.get('validation_results'):\n",
    "         # Use epochs if validation results are stored per epoch\n",
    "         num_epochs = len(history['train_loss'])\n",
    "         x_axis_train = range(1, num_epochs + 1)\n",
    "         x_axis_val = range(1, len(history['validation_results']) + 1)\n",
    "         x_label = 'Epoch'\n",
    "         plot_by = 'epoch' # Force epoch plotting if step data is missing for val\n",
    "    else:\n",
    "        print(\"Insufficient history data (need train_loss and either steps or validation_results per epoch).\")\n",
    "        return\n",
    "\n",
    "    val_metrics_data = history.get('val_metrics', {}) if plot_by == 'step' else history.get('validation_results', [])\n",
    "\n",
    "    # --- Training Loss ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_axis_train, history['train_loss'], 'b-', label=f'Training Loss (Avg per Log Interval if steps)')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss over {x_label.capitalize()}')\n",
    "\n",
    "    # Plot validation loss if available (assuming it's stored in val_results/val_metrics)\n",
    "    if val_metrics_data:\n",
    "        try:\n",
    "            if plot_by == 'step':\n",
    "                val_loss = [val_metrics_data[step].get('loss', float('nan')) for step in x_axis_val]\n",
    "            else: # Plot by epoch\n",
    "                val_loss = [res.get('loss', float('nan')) for res in val_metrics_data if res]\n",
    "            if any(not math.isnan(vl) for vl in val_loss): # Only plot if loss was calculated and stored\n",
    "                 plt.plot(x_axis_val, val_loss, 'r-', label='Validation Loss')\n",
    "        except (KeyError, TypeError):\n",
    "             print(\"Validation loss not found or incorrectly formatted in history.\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    save_path_loss = os.path.join(plot_dir, f'training_loss_{plot_by}.png')\n",
    "    plt.savefig(save_path_loss, dpi=300)\n",
    "    print(f\"Saved training loss plot to: {save_path_loss}\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- Validation Metrics ---\n",
    "    if val_metrics_data and x_axis_val:\n",
    "        # Get metric names from the first valid entry\n",
    "        first_valid_val_result = next((res for res in (val_metrics_data.values() if plot_by == 'step' else val_metrics_data) if res and isinstance(res, dict)), None)\n",
    "        if first_valid_val_result:\n",
    "            metrics_to_plot = [k for k in first_valid_val_result.keys() if k != 'loss'] # Exclude loss\n",
    "\n",
    "            num_plots = len(metrics_to_plot)\n",
    "            if num_plots > 0:\n",
    "                ncols = 2\n",
    "                nrows = math.ceil(num_plots / ncols)\n",
    "                fig, axes = plt.subplots(nrows, ncols, figsize=(8 * ncols, 6 * nrows), squeeze=False)\n",
    "                axes = axes.flatten()\n",
    "\n",
    "                for i, metric_name in enumerate(metrics_to_plot):\n",
    "                    if plot_by == 'step':\n",
    "                        metric_values = [val_metrics_data[step].get(metric_name, float('nan')) for step in x_axis_val]\n",
    "                    else: # Plot by epoch\n",
    "                         metric_values = [res.get(metric_name, float('nan')) for res in val_metrics_data if res]\n",
    "\n",
    "                    if any(not math.isnan(v) for v in metric_values): # Check if metric has valid data\n",
    "                        axes[i].plot(x_axis_val, metric_values, 'r-o', label=f'Validation {metric_name}')\n",
    "                        axes[i].set_xlabel(x_label)\n",
    "                        axes[i].set_ylabel(metric_name.replace('_', ' ').capitalize())\n",
    "                        axes[i].set_title(f'Validation {metric_name} over {x_label.capitalize()}')\n",
    "                        axes[i].legend()\n",
    "                        axes[i].grid(True)\n",
    "                    else:\n",
    "                         axes[i].set_title(f'Validation {metric_name} (No Data)')\n",
    "                         axes[i].text(0.5, 0.5, 'No Data', ha='center', va='center')\n",
    "\n",
    "\n",
    "                for j in range(i + 1, len(axes)): fig.delaxes(axes[j]) # Hide unused subplots\n",
    "\n",
    "                fig.suptitle(f'Validation Metrics over {x_label.capitalize()}', fontsize=16, y=1.02)\n",
    "                plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "                save_path_val = os.path.join(plot_dir, f'validation_metrics_{plot_by}.png')\n",
    "                plt.savefig(save_path_val, dpi=300)\n",
    "                print(f\"Saved validation metrics plot to: {save_path_val}\")\n",
    "                plt.close()\n",
    "            else: print(\"No validation metrics (excluding loss) found to plot.\")\n",
    "        else: print(\"No valid validation results found.\")\n",
    "    else: print(\"No validation metrics found in history to plot.\")\n",
    "\n",
    "\n",
    "# --- Plotting ---\n",
    "# Decide whether to plot by 'step' or 'epoch' based on how validation was run\n",
    "plot_directory = \"./train_plot/ViSigLIP_landmark\"\n",
    "plotting_mode = 'step' if config.validation_interval_steps > 0 else 'epoch'\n",
    "\n",
    "if 'history' in locals() and isinstance(history, dict):\n",
    "    plot_training_metrics(history, plot_directory, plot_by=plotting_mode)\n",
    "else:\n",
    "    print(\"No training history found. Run training first.\")\n",
    "\n",
    "# --- END OF SCRIPT ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
