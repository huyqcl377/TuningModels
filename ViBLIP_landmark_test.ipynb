{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca9efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from transformers import (\n",
    "    Blip2Processor,\n",
    "    Blip2Model,\n",
    "    PhobertTokenizer, # Using PhoBERT tokenizer\n",
    "    AutoImageProcessor,\n",
    "    logging\n",
    ")\n",
    "from datasets import load_dataset # Can be useful for handling data if needed later\n",
    "\n",
    "# Suppress excessive warnings\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # Set to False for reproducibility\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "if cuda_available:\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Capability: {torch.cuda.get_device_capability(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. Configuration\n",
    "\n",
    "# %%\n",
    "class CFG:\n",
    "    # --- Paths ---\n",
    "    # IMPORTANT: Adjust these paths to match your directory structure\n",
    "    data_path = \"./\" # Directory containing train.json, dev.json, test.json\n",
    "    image_base_path = \"./\" # Base directory where image_path in JSON starts from (e.g., contains 'images/' folder)\n",
    "    model_path = \"./ViBLIP_QFormer_Trained_Retrieval\" # Where to save trained models\n",
    "\n",
    "    # --- Model Selection ---\n",
    "    blip2_model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "    text_tokenizer_name = \"vinai/phobert-base\" # Vietnamese PhoBERT\n",
    "\n",
    "    # --- Training Parameters ---\n",
    "    seed = 42\n",
    "    batch_size = 8  # Reduced further for potential 4090 memory limits with large models\n",
    "    num_workers = 4\n",
    "    qformer_lr = 1e-5 # Often requires a smaller LR for fine-tuning\n",
    "    weight_decay = 0.05\n",
    "    patience = 2 # ReduceLROnPlateau patience\n",
    "    factor = 0.8 # ReduceLROnPlateau factor\n",
    "    epochs = 10 # Increased epochs, adjust as needed\n",
    "    early_stop_patience = 3 # Stop training if no improvement after N epochs\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_amp = True # Use Automatic Mixed Precision\n",
    "\n",
    "    # --- Image/Text Parameters ---\n",
    "    image_size = 224 # Standard BLIP-2 image size\n",
    "    max_length = 77 # Standard CLIP/BLIP text length\n",
    "\n",
    "    # --- Loss/Saving Parameters ---\n",
    "    temperature = 0.07 # Temperature for contrastive loss\n",
    "    save_best_only = True\n",
    "    metric_to_track = \"val_avg_acc\" # Track average retrieval accuracy\n",
    "    mode = \"max\" # Maximize accuracy\n",
    "\n",
    "config = CFG()\n",
    "seed_everything(config.seed)\n",
    "os.makedirs(config.model_path, exist_ok=True)\n",
    "\n",
    "print(f\"--- ViBLIP Q-Former Retrieval Training Configuration ---\")\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Base BLIP-2 Model: {config.blip2_model_name}\")\n",
    "print(f\"Text Tokenizer: {config.text_tokenizer_name}\")\n",
    "print(f\"Batch Size: {config.batch_size}\")\n",
    "print(f\"Use AMP: {config.use_amp}\")\n",
    "print(f\"Epochs: {config.epochs}\")\n",
    "print(f\"Q-Former LR: {config.qformer_lr}\")\n",
    "print(f\"Early Stop Patience: {config.early_stop_patience}\")\n",
    "print(f\"Output Path: {config.model_path}\")\n",
    "print(f\"Data Path (JSONs): {os.path.abspath(config.data_path)}\")\n",
    "print(f\"Image Base Path: {os.path.abspath(config.image_base_path)}\")\n",
    "print(f\"Metric to Track for Best Model: {config.metric_to_track} ({config.mode})\")\n",
    "print(f\"------------------------------------------------------\\n\")\n",
    "\n",
    "if not os.path.exists(config.data_path) or not os.path.exists(config.image_base_path):\n",
    "     print(f\"WARNING: Data path ({config.data_path}) or Image base path ({config.image_base_path}) does not exist.\")\n",
    "     print(\"Please ensure the paths are correct and the dataset is present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Metric Calculation Utilities\n",
    "\n",
    "# %%\n",
    "class AvgMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        if torch.is_tensor(val): val = val.item() # Ensure val is a Python number\n",
    "        if isinstance(val, (int, float)):\n",
    "            self.sum += val * count\n",
    "            self.count += count\n",
    "            self.avg = self.sum / self.count if self.count != 0 else 0\n",
    "        elif val is not None: # Handle potential None or unexpected types gracefully\n",
    "             print(f\"Warning: AvgMeter received unexpected type {type(val)}. Skipping update.\")\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.name}: {self.avg:.4f}\"\n",
    "\n",
    "def compute_recall_at_k(similarity_matrix, k, dim):\n",
    "    \"\"\" Computes Recall@k for image-to-text (dim=0) or text-to-image (dim=1) \"\"\"\n",
    "    n = similarity_matrix.shape[1-dim] # Number of items to retrieve from\n",
    "    if n == 0 or k <= 0: return 0.0\n",
    "    effective_k = min(k, n) # Cannot retrieve more items than available\n",
    "    correct_count = 0\n",
    "\n",
    "    # Get the indices of the top-k most similar items\n",
    "    # topk returns (values, indices)\n",
    "    top_k_indices = torch.topk(similarity_matrix, effective_k, dim=dim).indices\n",
    "    ground_truth = torch.arange(n, device=similarity_matrix.device)\n",
    "\n",
    "    if dim == 0: # I2T Recall: For each text, check if its corresponding image is in the top-k images\n",
    "        # Transpose for easier iteration if needed, or adjust indexing\n",
    "        # Check if ground_truth image index is present in the top k retrieved text indices for that image\n",
    "         for txt_idx in range(n): # Iterate through columns (texts)\n",
    "             if ground_truth[txt_idx] in top_k_indices[:, txt_idx]: # Check if correct image index is in the column\n",
    "                correct_count += 1\n",
    "    elif dim == 1: # T2I Recall: For each image, check if its corresponding text is in the top-k texts\n",
    "        # Check if ground_truth text index is present in the top k retrieved image indices for that text\n",
    "        for img_idx in range(n): # Iterate through rows (images)\n",
    "             if ground_truth[img_idx] in top_k_indices[img_idx, :]: # Check if correct text index is in the row\n",
    "                correct_count += 1\n",
    "    else:\n",
    "        raise ValueError(\"dim must be 0 (I2T) or 1 (T2I)\")\n",
    "\n",
    "    return correct_count / n if n > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_metrics(image_embeddings, text_embeddings, temp=1.0):\n",
    "    \"\"\" Computes ITC loss and retrieval metrics \"\"\"\n",
    "    image_embeddings = image_embeddings.float() # Ensure float32 for stability\n",
    "    text_embeddings = text_embeddings.float()\n",
    "\n",
    "    # Calculate cosine similarity matrix (logits)\n",
    "    # Assumes embeddings are already normalized\n",
    "    logits = text_embeddings @ image_embeddings.T * temp # Use temperature from config if needed scaling here, or apply in loss\n",
    "\n",
    "    n = logits.shape[0] # Should be batch size or validation set size\n",
    "    default_metrics = {\n",
    "        \"loss\": None, # Loss needs to be computed where gradients are required\n",
    "        \"i2t_acc\": 0.0, \"t2i_acc\": 0.0, \"avg_acc\": 0.0,\n",
    "        \"avg_cosine_sim\": 0.0,\n",
    "        \"i2t_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0},\n",
    "        \"t2i_recall\": {\"R@1\": 0.0, \"R@5\": 0.0, \"R@10\": 0.0}\n",
    "    }\n",
    "    if n == 0:\n",
    "        print(\"Warning: compute_metrics received empty embeddings.\")\n",
    "        return default_metrics\n",
    "\n",
    "    try:\n",
    "        # --- Accuracy Calculation ---\n",
    "        ground_truth = torch.arange(n, device=logits.device)\n",
    "        # Image-to-Text Accuracy: For each image, is the correct text caption ranked highest?\n",
    "        i2t_preds = torch.argmax(logits, dim=0) # Find max similarity text for each image\n",
    "        i2t_acc = (i2t_preds == ground_truth).float().mean().item()\n",
    "\n",
    "        # Text-to-Image Accuracy: For each text caption, is the correct image ranked highest?\n",
    "        t2i_preds = torch.argmax(logits, dim=1) # Find max similarity image for each text\n",
    "        t2i_acc = (t2i_preds == ground_truth).float().mean().item()\n",
    "\n",
    "        avg_acc = (i2t_acc + t2i_acc) / 2\n",
    "\n",
    "        # Average cosine similarity of the correct pairs (diagonal)\n",
    "        diag_len = min(logits.shape[0], logits.shape[1])\n",
    "        avg_cosine_sim = torch.diag(logits[:diag_len, :diag_len]).mean().item() / temp # Divide by temp if logits included it\n",
    "\n",
    "        # --- Recall Calculation ---\n",
    "        i2t_recall = {}\n",
    "        t2i_recall = {}\n",
    "        for k in [1, 5, 10]:\n",
    "            k_str = f\"R@{k}\"\n",
    "            # Recall@k for I2T: For each image, is the correct text among the top k texts? (dim=0)\n",
    "            i2t_recall[k_str] = compute_recall_at_k(logits, k, dim=0)\n",
    "             # Recall@k for T2I: For each text, is the correct image among the top k images? (dim=1)\n",
    "            t2i_recall[k_str] = compute_recall_at_k(logits, k, dim=1)\n",
    "\n",
    "        # --- Loss Calculation (Symmetric Cross Entropy for ITC) ---\n",
    "        # Calculate loss here if needed (e.g., during validation)\n",
    "        # Note: For training, loss is usually computed within the training step for gradients\n",
    "        labels = torch.arange(n, device=logits.device)\n",
    "        loss_i2t = F.cross_entropy(logits.T, labels) # Predict text given image (logits columns)\n",
    "        loss_t2i = F.cross_entropy(logits, labels)   # Predict image given text (logits rows)\n",
    "        loss = (loss_i2t + loss_t2i) / 2.0\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"i2t_acc\": i2t_acc, \"t2i_acc\": t2i_acc, \"avg_acc\": avg_acc,\n",
    "            \"avg_cosine_sim\": avg_cosine_sim,\n",
    "            \"i2t_recall\": i2t_recall, \"t2i_recall\": t2i_recall\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error during metric calculation: {e}\")\n",
    "        print(f\"Shapes: ImgEmb={image_embeddings.shape}, TxtEmb={text_embeddings.shape}, Logits={logits.shape}\")\n",
    "        # Return default metrics but maybe set loss to NaN or a high value\n",
    "        default_metrics[\"loss\"] = float('nan')\n",
    "        return default_metrics\n",
    "\n",
    "print(\"Metric utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Dataset and DataLoader\n",
    "\n",
    "# %%\n",
    "class VietnameseImageRetrievalDataset(Dataset):\n",
    "    def __init__(self, json_path, image_base_path, image_processor, tokenizer, max_length, is_train=True):\n",
    "        self.json_path = json_path\n",
    "        self.image_base_path = image_base_path\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_train = is_train # Flag might be useful later\n",
    "\n",
    "        print(f\"Loading data from: {self.json_path}\")\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                self.data = json.load(f)\n",
    "            print(f\"Loaded {len(self.data)} samples.\")\n",
    "            # Basic validation of the first item\n",
    "            if self.data:\n",
    "                first_item = self.data[0]\n",
    "                if \"image_path\" not in first_item or \"caption\" not in first_item:\n",
    "                     raise ValueError(\"JSON items must contain 'image_path' and 'caption' keys.\")\n",
    "                if not isinstance(first_item[\"caption\"], list) or not first_item[\"caption\"]:\n",
    "                     raise ValueError(\"'caption' must be a non-empty list of strings.\")\n",
    "            else:\n",
    "                 print(\"Warning: JSON file is empty.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: JSON file not found at {json_path}\")\n",
    "            self.data = []\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"ERROR: Could not decode JSON from {json_path}\")\n",
    "            self.data = []\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred loading JSON: {e}\")\n",
    "            self.data = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_rel_path = item['image_path']\n",
    "        # Ensure correct path joining (handle cases like './images/' or 'images/')\n",
    "        if image_rel_path.startswith('./'):\n",
    "            image_rel_path = image_rel_path[2:]\n",
    "        image_full_path = os.path.join(self.image_base_path, image_rel_path)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_full_path).convert(\"RGB\")\n",
    "            # Apply image transformations (resizing, normalization) using the processor\n",
    "            # The processor expects a PIL image or similar format\n",
    "            processed_image = self.image_processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0) # Remove batch dim\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image not found at {image_full_path}. Returning None for image.\")\n",
    "            # Handle missing images: return None or a placeholder tensor\n",
    "            processed_image = torch.zeros((3, config.image_size, config.image_size)) # Placeholder\n",
    "            # Or skip this item if many are missing? Needs careful consideration.\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Error loading image {image_full_path}: {e}. Returning placeholder.\")\n",
    "             processed_image = torch.zeros((3, config.image_size, config.image_size))\n",
    "\n",
    "\n",
    "        # Get the caption (assuming the first caption if multiple are provided)\n",
    "        caption = item['caption'][0]\n",
    "\n",
    "        # Tokenize the caption using PhoBERT tokenizer\n",
    "        # Important: Ensure tokenizer handles padding and truncation\n",
    "        tokenized_caption = self.tokenizer(\n",
    "            caption,\n",
    "            padding='max_length', # Pad to max_length\n",
    "            truncation=True,      # Truncate if longer\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"   # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # Squeeze unnecessary dimensions added by tokenizer\n",
    "        input_ids = tokenized_caption['input_ids'].squeeze()\n",
    "        attention_mask = tokenized_caption['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": processed_image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Initialize Processors, Tokenizer, Datasets, and DataLoaders\n",
    "\n",
    "# %%\n",
    "# Load the specific image processor associated with the BLIP-2 model\n",
    "# Although we fine-tune Q-Former, image processing should match the pre-training\n",
    "try:\n",
    "    image_processor = AutoImageProcessor.from_pretrained(config.blip2_model_name)\n",
    "    print(f\"Image processor loaded from {config.blip2_model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading image processor: {e}. Using default Blip2Processor.\")\n",
    "    # Fallback if AutoImageProcessor fails for some reason\n",
    "    fallback_processor = Blip2Processor.from_pretrained(config.blip2_model_name)\n",
    "    image_processor = fallback_processor.image_processor\n",
    "\n",
    "\n",
    "# Load the specified Vietnamese PhoBERT tokenizer\n",
    "try:\n",
    "    text_tokenizer = PhobertTokenizer.from_pretrained(config.text_tokenizer_name)\n",
    "    # Check if pad token exists, add if necessary (though PhoBERT usually has it)\n",
    "    if text_tokenizer.pad_token is None:\n",
    "        print(\"PhoBERT tokenizer does not have a pad token. Setting it to eos_token.\")\n",
    "        text_tokenizer.pad_token = text_tokenizer.eos_token # Common practice if pad is missing\n",
    "    print(f\"Text tokenizer loaded: {config.text_tokenizer_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Could not load PhoBERT tokenizer '{config.text_tokenizer_name}'. {e}\")\n",
    "    # Exit or raise error, as tokenizer is critical\n",
    "    raise e\n",
    "\n",
    "\n",
    "# --- Create Datasets ---\n",
    "train_json_path = os.path.join(config.data_path, \"train.json\")\n",
    "dev_json_path = os.path.join(config.data_path, \"dev.json\")\n",
    "# test_json_path = os.path.join(config.data_path, \"test.json\") # Optional for final testing\n",
    "\n",
    "train_dataset = VietnameseImageRetrievalDataset(\n",
    "    json_path=train_json_path,\n",
    "    image_base_path=config.image_base_path,\n",
    "    image_processor=image_processor,\n",
    "    tokenizer=text_tokenizer,\n",
    "    max_length=config.max_length,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "val_dataset = VietnameseImageRetrievalDataset(\n",
    "    json_path=dev_json_path,\n",
    "    image_base_path=config.image_base_path,\n",
    "    image_processor=image_processor,\n",
    "    tokenizer=text_tokenizer,\n",
    "    max_length=config.max_length,\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True, # Helps speed up data transfer to GPU\n",
    "    drop_last=False # Keep last batch even if smaller\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False, # No shuffling for validation\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}, Train loader batches: {len(train_loader)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}, Validation loader batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e735dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Model Loading and Preparation\n",
    "\n",
    "# %%\n",
    "print(f\"Loading BLIP-2 model: {config.blip2_model_name}...\")\n",
    "# Load the base BLIP-2 model. Blip2Model provides access to vision, qformer, and language model parts.\n",
    "# Use load_in_8bit=True if memory is very tight, requires `bitsandbytes`\n",
    "# device_map='auto' might be needed for very large models across multiple GPUs\n",
    "try:\n",
    "     model = Blip2Model.from_pretrained(\n",
    "         config.blip2_model_name,\n",
    "         # load_in_8bit=True, # Uncomment if needed, requires bitsandbytes\n",
    "         # device_map=\"auto\" # Uncomment if using multiple GPUs or offloading\n",
    "         # torch_dtype=torch.float16 # Uncomment if using AMP and want model loaded in fp16\n",
    "     )\n",
    "     print(\"BLIP-2 model loaded successfully.\")\n",
    "except Exception as e:\n",
    "     print(f\"Error loading model {config.blip2_model_name}: {e}\")\n",
    "     raise e\n",
    "\n",
    "\n",
    "# --- Freeze parameters ---\n",
    "print(\"Freezing model parameters initially...\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --- Unfreeze Q-Former parameters ---\n",
    "print(\"Unfreezing Q-Former parameters for fine-tuning...\")\n",
    "qformer_param_count = 0\n",
    "total_param_count = 0\n",
    "trainable_param_count = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    total_param_count += param.numel()\n",
    "    # Target parameters within the qformer module\n",
    "    if \"qformer.\" in name:\n",
    "        param.requires_grad = True\n",
    "        qformer_param_count += param.numel()\n",
    "        trainable_param_count += param.numel()\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "print(f\"Total parameters: {total_param_count:,}\")\n",
    "print(f\"Q-Former parameters: {qformer_param_count:,}\")\n",
    "print(f\"Trainable parameters (should match Q-Former): {trainable_param_count:,}\")\n",
    "\n",
    "if trainable_param_count == 0:\n",
    "    print(\"WARNING: No parameters were unfrozen for training. Check model structure and parameter names.\")\n",
    "\n",
    "# --- Move model to device ---\n",
    "model.to(config.device)\n",
    "print(f\"Model moved to {config.device}\")\n",
    "\n",
    "# If using device_map=\"auto\", the model might already be on GPU(s)\n",
    "# If using load_in_8bit=True, model parts might stay on CPU/GPU depending on device_map\n",
    "\n",
    "# Optional: Compile model (PyTorch 2.0+) - can speed up training but might have issues\n",
    "# try:\n",
    "#     if hasattr(torch, 'compile'):\n",
    "#         print(\"Compiling model with torch.compile...\")\n",
    "#         model = torch.compile(model)\n",
    "# except Exception as e:\n",
    "#      print(f\"torch.compile failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 7. Training and Validation Functions\n",
    "\n",
    "# %%\n",
    "def train_one_epoch(model, dataloader, optimizer, scaler, device, temperature):\n",
    "    model.train()\n",
    "    loss_meter = AvgMeter(name=\"Train Loss\")\n",
    "    start_time = time.time()\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in pbar:\n",
    "        # Move batch to device\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(enabled=config.use_amp):\n",
    "            # Extract features using the model\n",
    "            # We need image features and text features projected to the same multimodal space\n",
    "            # Blip2Model output contains 'image_embeds' (from vision encoder) and 'last_hidden_state' (from LM)\n",
    "            # We need the outputs *after* the Q-Former processing for multimodal alignment.\n",
    "            # Let's use the dedicated methods: get_image_features and get_text_features\n",
    "            # These typically apply the projection layers.\n",
    "\n",
    "            image_outputs = model.get_image_features(pixel_values=pixel_values)\n",
    "            # For text features with Q-Former, we usually pass input_ids to the model directly\n",
    "            # or use get_text_features which might involve the language model.\n",
    "            # Let's confirm Blip2Model's get_text_features usage...\n",
    "            # Yes, get_text_features takes input_ids and attention_mask.\n",
    "            text_outputs = model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # image_features = image_outputs.image_embeds # Check the attribute name, might be different\n",
    "            # text_features = text_outputs.text_embeds   # Check the attribute name\n",
    "\n",
    "            # Blip2Model output doesn't directly give 'image_embeds' and 'text_embeds' in the multimodal space easily.\n",
    "            # A common way for retrieval with BLIP-2 is to use the query outputs from Q-Former.\n",
    "            # Let's try getting QFormer output directly if possible, or use `get_image_features` / `get_text_features`\n",
    "            # which *should* return embeddings suitable for contrastive loss.\n",
    "            # `image_features` (from get_image_features) and `text_features` (from get_text_features)\n",
    "            # are typically pooled representations. Let's assume they are suitable.\n",
    "\n",
    "            image_features = image_outputs # Output might be the embedding tensor directly\n",
    "            text_features = text_outputs  # Output might be the embedding tensor directly\n",
    "\n",
    "            # --- Crucial: Normalize embeddings for cosine similarity ---\n",
    "            image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "            text_features = F.normalize(text_features, p=2, dim=-1)\n",
    "\n",
    "            # --- Calculate Contrastive Loss (InfoNCE) ---\n",
    "            logits = (text_features @ image_features.T) * temperature\n",
    "            targets = torch.arange(logits.shape[0], device=device) # Ground truth: image i matches text i\n",
    "\n",
    "            loss_i2t = F.cross_entropy(logits.T, targets) # Predict text given image\n",
    "            loss_t2i = F.cross_entropy(logits, targets)   # Predict image given text\n",
    "            loss = (loss_i2t + loss_t2i) / 2.0\n",
    "\n",
    "        # Backward pass\n",
    "        if config.use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_meter.update(loss.item(), count=pixel_values.size(0))\n",
    "        pbar.set_postfix({\"Loss\": loss_meter.avg, \"LR\": optimizer.param_groups[0]['lr']})\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Train Epoch Completed - Time: {elapsed_time:.2f}s, Avg Loss: {loss_meter.avg:.4f}\")\n",
    "    return loss_meter.avg\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, dataloader, device, temperature):\n",
    "    model.eval()\n",
    "    all_image_features = []\n",
    "    all_text_features = []\n",
    "    loss_meter = AvgMeter(name=\"Val Loss\")\n",
    "    start_time = time.time()\n",
    "    pbar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                # Extract features (same as in training)\n",
    "                image_outputs = model.get_image_features(pixel_values=pixel_values)\n",
    "                text_outputs = model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "                image_features = image_outputs\n",
    "                text_features = text_outputs\n",
    "\n",
    "                # Normalize\n",
    "                image_features_norm = F.normalize(image_features, p=2, dim=-1)\n",
    "                text_features_norm = F.normalize(text_features, p=2, dim=-1)\n",
    "\n",
    "            all_image_features.append(image_features_norm.cpu())\n",
    "            all_text_features.append(text_features_norm.cpu())\n",
    "\n",
    "            # Optional: Calculate batch-level loss for monitoring (doesn't affect gradients)\n",
    "            logits = (text_features_norm @ image_features_norm.T) * temperature\n",
    "            targets = torch.arange(logits.shape[0], device=device)\n",
    "            loss_i2t = F.cross_entropy(logits.T, targets)\n",
    "            loss_t2i = F.cross_entropy(logits, targets)\n",
    "            loss = (loss_i2t + loss_t2i) / 2.0\n",
    "            loss_meter.update(loss.item(), count=pixel_values.size(0))\n",
    "            pbar.set_postfix({\"Loss\": loss_meter.avg})\n",
    "\n",
    "\n",
    "    # Concatenate all features from the validation set\n",
    "    if not all_image_features or not all_text_features:\n",
    "         print(\"Warning: No features collected during validation.\")\n",
    "         # Return default bad metrics\n",
    "         metrics = compute_metrics(torch.tensor([]), torch.tensor([]), temp=temperature) # Get default structure\n",
    "         metrics[\"loss\"] = loss_meter.avg if loss_meter.count > 0 else float('nan')\n",
    "         return metrics\n",
    "\n",
    "    all_image_features = torch.cat(all_image_features, dim=0).to(device)\n",
    "    all_text_features = torch.cat(all_text_features, dim=0).to(device)\n",
    "\n",
    "    # --- Calculate Metrics on the ENTIRE validation set ---\n",
    "    print(f\"\\nCalculating metrics on {all_image_features.shape[0]} validation samples...\")\n",
    "    # Pass normalized features directly\n",
    "    metrics = compute_metrics(all_image_features, all_text_features, temp=temperature)\n",
    "    # Override the loss calculated within compute_metrics with the averaged batch loss\n",
    "    metrics[\"loss\"] = loss_meter.avg # Use the averaged batch loss for consistency report\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Validation Completed - Time: {elapsed_time:.2f}s\")\n",
    "\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd719a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Main Training Loop\n",
    "\n",
    "# %%\n",
    "print(\"--- Starting Training ---\")\n",
    "\n",
    "# Optimizer (AdamW is recommended for transformers)\n",
    "# Filter parameters that require gradients\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "if not trainable_params:\n",
    "    raise ValueError(\"No trainable parameters found. Check model freezing/unfreezing steps.\")\n",
    "\n",
    "optimizer = AdamW(\n",
    "    trainable_params,\n",
    "    lr=config.qformer_lr,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min', # Reduce LR when validation loss stops decreasing\n",
    "    factor=config.factor,\n",
    "    patience=config.patience,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Gradient Scaler for Automatic Mixed Precision (AMP)\n",
    "scaler = GradScaler(enabled=config.use_amp)\n",
    "\n",
    "# Tracking best model and early stopping\n",
    "best_metric_value = -np.inf if config.mode == \"max\" else np.inf\n",
    "epochs_no_improve = 0\n",
    "history = {'train_loss': [], 'val_loss': [], f'{config.metric_to_track}': []}\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    print(f\"\\n===== Epoch {epoch}/{config.epochs} =====\")\n",
    "\n",
    "    # --- Training ---\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, scaler, config.device, config.temperature)\n",
    "    history['train_loss'].append(train_loss)\n",
    "\n",
    "    # --- Validation ---\n",
    "    val_metrics = validate_one_epoch(model, val_loader, config.device, config.temperature)\n",
    "    val_loss = val_metrics[\"loss\"]\n",
    "    val_tracked_metric = val_metrics.get(config.metric_to_track, None) # Get the specific metric we track\n",
    "\n",
    "    history['val_loss'].append(val_loss)\n",
    "    if val_tracked_metric is not None:\n",
    "         history[config.metric_to_track].append(val_tracked_metric)\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Metrics:\")\n",
    "    print(f\"    Avg Accuracy (I2T+T2I)/2: {val_metrics.get('avg_acc', 'N/A'):.4f}\")\n",
    "    print(f\"    Accuracy (I2T): {val_metrics.get('i2t_acc', 'N/A'):.4f}, Accuracy (T2I): {val_metrics.get('t2i_acc', 'N/A'):.4f}\")\n",
    "    i2t_rec = val_metrics.get('i2t_recall', {})\n",
    "    t2i_rec = val_metrics.get('t2i_recall', {})\n",
    "    print(f\"    I2T Recall: R@1={i2t_rec.get('R@1', 'N/A'):.4f}, R@5={i2t_rec.get('R@5', 'N/A'):.4f}, R@10={i2t_rec.get('R@10', 'N/A'):.4f}\")\n",
    "    print(f\"    T2I Recall: R@1={t2i_rec.get('R@1', 'N/A'):.4f}, R@5={t2i_rec.get('R@5', 'N/A'):.4f}, R@10={t2i_rec.get('R@10', 'N/A'):.4f}\")\n",
    "    print(f\"    Avg Cosine Sim (Pos Pairs): {val_metrics.get('avg_cosine_sim', 'N/A'):.4f}\")\n",
    "\n",
    "\n",
    "    # --- Learning Rate Scheduling ---\n",
    "    # ReduceLROnPlateau steps based on validation loss\n",
    "    lr_scheduler.step(val_loss)\n",
    "\n",
    "    # --- Model Saving & Early Stopping ---\n",
    "    if val_tracked_metric is None:\n",
    "         print(f\"Warning: Metric '{config.metric_to_track}' not found in validation results. Cannot save best model or check early stopping based on it.\")\n",
    "         current_is_best = False # Cannot determine best if metric is missing\n",
    "    else:\n",
    "        if config.mode == \"max\":\n",
    "            current_is_best = val_tracked_metric > best_metric_value\n",
    "        else: # mode == \"min\"\n",
    "            current_is_best = val_tracked_metric < best_metric_value\n",
    "\n",
    "    if current_is_best:\n",
    "        best_metric_value = val_tracked_metric\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"*** New best model found! ({config.metric_to_track}: {best_metric_value:.4f}) Saving model... ***\")\n",
    "        # Save only the Q-Former weights, as that's what we trained\n",
    "        qformer_state_dict = {k: v for k, v in model.state_dict().items() if \"qformer.\" in k}\n",
    "        save_path = os.path.join(config.model_path, \"best_qformer_weights.pth\")\n",
    "        torch.save(qformer_state_dict, save_path)\n",
    "        print(f\"Q-Former weights saved to {save_path}\")\n",
    "\n",
    "        # Optionally save the full model if needed, but it will be large\n",
    "        # if not config.save_best_only: # Example: Save full model checkpoint too\n",
    "        #    full_save_path = os.path.join(config.model_path, f\"epoch_{epoch}_full_model.pth\")\n",
    "        #    torch.save(model.state_dict(), full_save_path)\n",
    "        #    print(f\"Full model saved to {full_save_path}\")\n",
    "\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement in {config.metric_to_track} for {epochs_no_improve} epoch(s). Best: {best_metric_value:.4f}\")\n",
    "        if config.early_stop_patience > 0 and epochs_no_improve >= config.early_stop_patience:\n",
    "            print(f\"\\n--- Early stopping triggered after {epoch} epochs. ---\")\n",
    "            break\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Training Finished\n",
    "#\n",
    "# The training loop has completed. The best Q-Former weights (based on validation `{config.metric_to_track}`) should be saved in `{config.model_path}/best_qformer_weights.pth`.\n",
    "#\n",
    "# You can now load these weights back into the Q-Former part of a `Blip2Model` for inference or further use. Example loading:\n",
    "#\n",
    "# ```python\n",
    "# # Load the base model architecture again\n",
    "# inference_model = Blip2Model.from_pretrained(config.blip2_model_name)\n",
    "#\n",
    "# # Load the fine-tuned Q-Former weights\n",
    "# qformer_weights_path = os.path.join(config.model_path, \"best_qformer_weights.pth\")\n",
    "# if os.path.exists(qformer_weights_path):\n",
    "#     qformer_state_dict = torch.load(qformer_weights_path, map_location='cpu') # Load to CPU first\n",
    "#\n",
    "#     # Load the state dict into the model's Q-Former\n",
    "#     # Need to access the qformer submodule correctly\n",
    "#     # The exact name might vary slightly depending on the transformers version, but 'qformer' is standard\n",
    "#     missing_keys, unexpected_keys = inference_model.qformer.load_state_dict(qformer_state_dict, strict=False)\n",
    "#     print(\"Q-Former weights loaded.\")\n",
    "#     if missing_keys: print(f\"Missing keys: {missing_keys}\")\n",
    "#     if unexpected_keys: print(f\"Unexpected keys: {unexpected_keys}\") # Should ideally be empty if saved correctly\n",
    "#\n",
    "#     inference_model.to(config.device) # Move to GPU/CPU for inference\n",
    "#     inference_model.eval()\n",
    "# else:\n",
    "#     print(f\"ERROR: Saved Q-Former weights not found at {qformer_weights_path}\")\n",
    "#\n",
    "# # Now 'inference_model' has the fine-tuned Q-Former and can be used for retrieval tasks.\n",
    "# ```\n",
    "\n",
    "# %%\n",
    "print(\"\\n--- Training Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ef819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Plot training history\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    metric_key = config.metric_to_track\n",
    "    if metric_key in history and history[metric_key]:\n",
    "         plt.plot(history[metric_key], label=f'Validation {metric_key}')\n",
    "         plt.title(f'{metric_key} History')\n",
    "         plt.xlabel('Epoch')\n",
    "         plt.ylabel(metric_key)\n",
    "         plt.legend()\n",
    "         plt.grid(True)\n",
    "    else:\n",
    "         plt.text(0.5, 0.5, f'Metric \"{metric_key}\"\\n not recorded', horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Install matplotlib to plot training history: pip install matplotlib\")\n",
    "except Exception as e:\n",
    "     print(f\"Couldn't plot history: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
